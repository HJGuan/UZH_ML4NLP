{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "ex02_wordembeddings_jan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jan-kreischer/UZH_ML4NLP/blob/main/Project-02/ex02_wordembeddings_jan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFNNOVmLD-Fz"
      },
      "source": [
        "# Project 2 - Word Embeddings with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHXvfNTXD4Ca"
      },
      "source": [
        "## 1. Setup\n",
        "### 1.1 Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EATjb-X-0lM",
        "outputId": "5ca835bd-a1dd-4678-9eb7-7cb11795ea9e"
      },
      "source": [
        "\n",
        "# Author: Jan Kreischer Bauer\n",
        "import os\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# numpy and pandas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('max_colwidth', 800)\n",
        "\n",
        "import regex as re\n",
        "\n",
        "# tokenization\n",
        "import nltk;\n",
        "nltk.download('stopwords');\n",
        "%matplotlib inline\n",
        "\n",
        "from argparse import Namespace\n",
        "from collections import Counter\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import tqdm\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "CUDA = torch.cuda.is_available()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu:0')\n",
        "torch.cuda.set_device(device)\n",
        "print('Using device:', device)\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Using device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKjgo1cEEQPw"
      },
      "source": [
        "### 1.2 Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4ZgunZ9qSvu",
        "outputId": "a116de45-d84f-4259-95b3-c04286f4ab7a"
      },
      "source": [
        "# Check GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct 23 07:14:59 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P0    29W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keYYVJLbnpbD",
        "outputId": "4e2b0443-baee-457b-c33b-d0c7e4917a2f"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-ac895e14-ddd0-b216-862c-49f315224518)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGFVru-nqX1n",
        "outputId": "587fa2c4-195f-4a36-fd5f-c3e505a491a2"
      },
      "source": [
        "# Check Memory\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW7vvEZkEdrX"
      },
      "source": [
        "### 1.3 Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grG2YIi8-aYu"
      },
      "source": [
        "FEATURE_COLUMN = 'Review'\n",
        "CONTEXT_OFFSET = 2 # n words to the left, n to the right\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "EPOCHS_HOTEL = 12\n",
        "EMBEDDING_DIM_HOTEL = 50\n",
        "\n",
        "EPOCHS_SCIFI = 2\n",
        "EMBEDDING_DIM_HOTEL = 50"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMv2k-eFt5e5"
      },
      "source": [
        "Source: [https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#exercise-computing-word-embeddings-continuous-bag-of-words](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#exercise-computing-word-embeddings-continuous-bag-of-words)\n",
        "\n",
        "# Word Embeddings: Encoding Lexical Semantics\n",
        "\n",
        "Word embeddings are dense vectors of real numbers, one per word in your\n",
        "vocabulary. In NLP, it is almost always the case that your features are\n",
        "words! But how should you represent a word in a computer? You could\n",
        "store its ascii character representation, but that only tells you what\n",
        "the word *is*, it doesn't say much about what it *means* (you might be\n",
        "able to derive its part of speech from its affixes, or properties from\n",
        "its capitalization, but not much). Even more, in what sense could you\n",
        "combine these representations? We often want dense outputs from our\n",
        "neural networks, where the inputs are $|V|$ dimensional, where\n",
        "$V$ is our vocabulary, but often the outputs are only a few\n",
        "dimensional (if we are only predicting a handful of labels, for\n",
        "instance). How do we get from a massive dimensional space to a smaller\n",
        "dimensional space?\n",
        "\n",
        "How about instead of ascii representations, we use a one-hot encoding?\n",
        "That is, we represent the word $w$ by\n",
        "\n",
        "\\begin{align}\\overbrace{\\left[ 0, 0, \\dots, 1, \\dots, 0, 0 \\right]}^\\text{|V| elements}\\end{align}\n",
        "\n",
        "where the 1 is in a location unique to $w$. Any other word will\n",
        "have a 1 in some other location, and a 0 everywhere else.\n",
        "\n",
        "There is an enormous drawback to this representation, besides just how\n",
        "huge it is. It basically treats all words as independent entities with\n",
        "no relation to each other. What we really want is some notion of\n",
        "*similarity* between words. Why? Let's see an example.\n",
        "\n",
        "Suppose we are building a language model. Suppose we have seen the\n",
        "sentences\n",
        "\n",
        "* The mathematician ran to the store.\n",
        "* The physicist ran to the store.\n",
        "* The mathematician solved the open problem.\n",
        "\n",
        "in our training data. Now suppose we get a new sentence never before\n",
        "seen in our training data:\n",
        "\n",
        "* The physicist solved the open problem.\n",
        "\n",
        "Our language model might do OK on this sentence, but wouldn't it be much\n",
        "better if we could use the following two facts:\n",
        "\n",
        "* We have seen  mathematician and physicist in the same role in a sentence. Somehow they\n",
        "  have a semantic relation.\n",
        "* We have seen mathematician in the same role  in this new unseen sentence\n",
        "  as we are now seeing physicist.\n",
        "\n",
        "and then infer that physicist is actually a good fit in the new unseen\n",
        "sentence? This is what we mean by a notion of similarity: we mean\n",
        "*semantic similarity*, not simply having similar orthographic\n",
        "representations. It is a technique to combat the sparsity of linguistic\n",
        "data, by connecting the dots between what we have seen and what we\n",
        "haven't. This example of course relies on a fundamental linguistic\n",
        "assumption: that words appearing in similar contexts are related to each\n",
        "other semantically. This is called the `distributional\n",
        "hypothesis <https://en.wikipedia.org/wiki/Distributional_semantics>`__.\n",
        "\n",
        "\n",
        "# Getting Dense Word Embeddings\n",
        "\n",
        "How can we solve this problem? That is, how could we actually encode\n",
        "semantic similarity in words? Maybe we think up some semantic\n",
        "attributes. For example, we see that both mathematicians and physicists\n",
        "can run, so maybe we give these words a high score for the \"is able to\n",
        "run\" semantic attribute. Think of some other attributes, and imagine\n",
        "what you might score some common words on those attributes.\n",
        "\n",
        "If each attribute is a dimension, then we might give each word a vector,\n",
        "like this:\n",
        "\n",
        "\\begin{align}q_\\text{mathematician} = \\left[ \\overbrace{2.3}^\\text{can run},\n",
        "   \\overbrace{9.4}^\\text{likes coffee}, \\overbrace{-5.5}^\\text{majored in Physics}, \\dots \\right]\\end{align}\n",
        "\n",
        "\\begin{align}q_\\text{physicist} = \\left[ \\overbrace{2.5}^\\text{can run},\n",
        "   \\overbrace{9.1}^\\text{likes coffee}, \\overbrace{6.4}^\\text{majored in Physics}, \\dots \\right]\\end{align}\n",
        "\n",
        "Then we can get a measure of similarity between these words by doing:\n",
        "\n",
        "\\begin{align}\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = q_\\text{physicist} \\cdot q_\\text{mathematician}\\end{align}\n",
        "\n",
        "Although it is more common to normalize by the lengths:\n",
        "\n",
        "\\begin{align}\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = \\frac{q_\\text{physicist} \\cdot q_\\text{mathematician}}\n",
        "   {\\| q_\\text{\\physicist} \\| \\| q_\\text{mathematician} \\|} = \\cos (\\phi)\\end{align}\n",
        "\n",
        "Where $\\phi$ is the angle between the two vectors. That way,\n",
        "extremely similar words (words whose embeddings point in the same\n",
        "direction) will have similarity 1. Extremely dissimilar words should\n",
        "have similarity -1.\n",
        "\n",
        "\n",
        "You can think of the sparse one-hot vectors from the beginning of this\n",
        "section as a special case of these new vectors we have defined, where\n",
        "each word basically has similarity 0, and we gave each word some unique\n",
        "semantic attribute. These new vectors are *dense*, which is to say their\n",
        "entries are (typically) non-zero.\n",
        "\n",
        "But these new vectors are a big pain: you could think of thousands of\n",
        "different semantic attributes that might be relevant to determining\n",
        "similarity, and how on earth would you set the values of the different\n",
        "attributes? Central to the idea of deep learning is that the neural\n",
        "network learns representations of the features, rather than requiring\n",
        "the programmer to design them herself. So why not just let the word\n",
        "embeddings be parameters in our model, and then be updated during\n",
        "training? This is exactly what we will do. We will have some *latent\n",
        "semantic attributes* that the network can, in principle, learn. Note\n",
        "that the word embeddings will probably not be interpretable. That is,\n",
        "although with our hand-crafted vectors above we can see that\n",
        "mathematicians and physicists are similar in that they both like coffee,\n",
        "if we allow a neural network to learn the embeddings and see that both\n",
        "mathematicians and physicists have a large value in the second\n",
        "dimension, it is not clear what that means. They are similar in some\n",
        "latent semantic dimension, but this probably has no interpretation to\n",
        "us.\n",
        "\n",
        "\n",
        "In summary, **word embeddings are a representation of the *semantics* of\n",
        "a word, efficiently encoding semantic information that might be relevant\n",
        "to the task at hand**. You can embed other things too: part of speech\n",
        "tags, parse trees, anything! The idea of feature embeddings is central\n",
        "to the field.\n",
        "\n",
        "\n",
        "# Word Embeddings in Pytorch\n",
        "\n",
        "Before we get to a worked example and an exercise, a few quick notes\n",
        "about how to use embeddings in Pytorch and in deep learning programming\n",
        "in general. Similar to how we defined a unique index for each word when\n",
        "making one-hot vectors, we also need to define an index for each word\n",
        "when using embeddings. These will be keys into a lookup table. That is,\n",
        "embeddings are stored as a $|V| \\times D$ matrix, where $D$\n",
        "is the dimensionality of the embeddings, such that the word assigned\n",
        "index $i$ has its embedding stored in the $i$'th row of the\n",
        "matrix. In all of my code, the mapping from words to indices is a\n",
        "dictionary named word\\_to\\_ix.\n",
        "\n",
        "The module that allows you to use embeddings is torch.nn.Embedding,\n",
        "which takes two arguments: the vocabulary size, and the dimensionality\n",
        "of the embeddings.\n",
        "\n",
        "To index into this table, you must use torch.LongTensor (since the\n",
        "indices are integers, not floats).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO-_4P3mt5e_"
      },
      "source": [
        "# Exercise: Computing Word Embeddings: Continuous Bag-of-Words\n",
        "\n",
        "The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep\n",
        "learning. It is a model that tries to predict words given the context of\n",
        "a few words before and a few words after the target word. This is\n",
        "distinct from language modeling, since CBOW is not sequential and does\n",
        "not have to be probabilistic. Typcially, CBOW is used to quickly train\n",
        "word embeddings, and these embeddings are used to initialize the\n",
        "embeddings of some more complicated model. Usually, this is referred to\n",
        "as *pretraining embeddings*. It almost always helps performance a couple\n",
        "of percent.\n",
        "\n",
        "The CBOW model is as follows. Given a target word $w_i$ and an\n",
        "$N$ context window on each side, $w_{i-1}, \\dots, w_{i-N}$\n",
        "and $w_{i+1}, \\dots, w_{i+N}$, referring to all context words\n",
        "collectively as $C$, CBOW tries to minimize\n",
        "\n",
        "\\begin{align}-\\log p(w_i | C) = -\\log \\text{Softmax}(A(\\sum_{w \\in C} q_w) + b)\\end{align}\n",
        "\n",
        "where $q_w$ is the embedding for word $w$.\n",
        "\n",
        "Implement this model in Pytorch by filling in the class below. Some\n",
        "tips:\n",
        "\n",
        "* Think about which parameters you need to define.\n",
        "* Make sure you know what shape each operation expects. Use .view() if you need to\n",
        "  reshape.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAqmKotTv5FX"
      },
      "source": [
        "## 2. Data Preprocessing\n",
        "### 2.1 Data Acquisition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzGrcHHB430b",
        "outputId": "7f163ae1-7eb1-4612-b46a-2d7b13b3d0bb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlQzA_N548HV",
        "outputId": "60f6592b-b511-45d0-838d-4b426af118d9"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/ML4NLP/Projects/Project-02/\")\n",
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current_version.ipynb\n",
            "ex02_task.pdf\n",
            "ex02_wordembeddings_hongjie.ipynb\n",
            "ex02_wordembeddings_hongjie.ipynbex02_wordembeddings_hongjie.ipynb\n",
            "ex02_wordembeddings_jan.ipynb\n",
            "Intro_to_PyTorch.ipynb\n",
            "old\n",
            "README.md\n",
            "scifi.txt\n",
            "tripadvisor_hotel_reviews.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_GE-D33bf2C"
      },
      "source": [
        "reviews_dataset = pd.read_csv('./tripadvisor_hotel_reviews.csv')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSHgz-tExPZI",
        "outputId": "0833c482-ec2f-444c-c6b2-ff1f1b2a6a14"
      },
      "source": [
        "reviews_dataset.info()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20491 entries, 0 to 20490\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Review  20491 non-null  object\n",
            " 1   Rating  20491 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 320.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jay7YAfne0wo"
      },
      "source": [
        "### 2.2 Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0rNpT9gd4bp"
      },
      "source": [
        "wpt = nltk.WordPunctTokenizer()\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def clean_document(x):\n",
        "    x = re.sub(r'[^a-zA-Z\\s]', '', x.lower(), re.I|re.A)\n",
        "    x = re.sub(r'\\w*\\d\\w*', '', x)\n",
        "    x = re.sub(r'[\\-!+_@*#\\/$:)\"\\'.;,?&({}[]]*', '', x)\n",
        "    x = x.strip()\n",
        "    tokens = wpt.tokenize(x)\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    x = ' '.join(filtered_tokens)\n",
        "    return x\n",
        "\n",
        "clean_corpus = np.vectorize(clean_document)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgtrtfqq-kwj",
        "outputId": "4c8672bb-61fa-49d1-a9f3-6d767fd8a3d8"
      },
      "source": [
        "reviews = clean_corpus(reviews_dataset['Review'])\n",
        "reviews[:10]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['nice hotel expensive parking got good deal stay hotel anniversary arrived late evening took advice previous reviews valet parking check quick easy little disappointed nonexistent view room room clean nice size bed comfortable woke stiff neck high pillows soundproof like heard music room night morning loud bangs doors opening closing hear people talking hallway maybe noisy neighbors aveda bath products nice goldfish stay nice touch taken advantage staying longer location great walking distance shopping overall nice experience pay parking night',\n",
              "       'ok nothing special charge diamond member hilton decided chain shot th anniversary seattle start booked suite paid extra website description suite bedroom bathroom standard hotel room took printed reservation desk showed said things like tv couch ect desk clerk told oh mixed suites description kimpton website sorry free breakfast got kidding embassy suits sitting room bathroom bedroom unlike kimpton calls suite day stay offer correct false advertising send kimpton preferred guest website email asking failure provide suite advertised website reservation description furnished hard copy reservation printout website desk manager duty reply solution send email trip guest survey follow email mail guess tell concerned guestthe staff ranged indifferent helpful asked desk good breakfast spots neighborhood hood told hotels gee best breakfast spots seattle block away convenient hotel know exist arrived late night pm inside run bellman busy chating cell phone help bagsprior arrival emailed hotel inform th anniversary half really picky wanted make sure good got nice email saying like deliver bottle champagne chocolate covered strawberries room arrival celebrate told needed foam pillows arrival champagne strawberries foam pillows great room view alley high rise building good better housekeeping staff cleaner room property impressed left morning shopping room got short trips hours beds comfortablenot good acheat control x inch screen bring green shine directly eyes night light sensitive tape controlsthis start hotel clean business hotel super high rates better chain hotels seattle',\n",
              "       'nice rooms experience hotel monaco seattle good hotel nt levelpositives large bathroom mediterranean suite comfortable bed pillowsattentive housekeeping staffnegatives ac unit malfunctioned stay desk disorganized missed separate wakeup calls concierge busy hard touch nt provide guidance special requeststv hard use ipod sound dock suite non functioning decided book mediterranean suite night weekend stay st choice rest party filled comparison w spent night larger square footage room great soaking tub whirlpool jets nice showerbefore stay hotel arrange car service price tip reasonable driver waiting arrivalcheckin easy downside room picked person jacuzi tub bath accessories salts bubble bath nt stay night got checked voucher bottle champagne nice gesture fish waiting room impression room huge open space felt room big tv far away bed chore change channel ipod dock broken disappointingin morning way asked desk check thermostat said f degrees warm try cover face night bright blue light kept got room night st drop desk called maintainence came look thermostat told play settings happy digital box wo nt work asked wakeup morning nt happen called later pm nap wakeup forgot wakeup morning yep forgottenthe bathroom facilities great room surprised room sold whirlpool bath tub nt bath amenities great relax water jets going',\n",
              "       'unique great stay wonderful time hotel monaco location excellent short stroll main downtown shopping area pet friendly room showed signs animal hair smells monaco suite sleeping area big striped curtains pulled closed nice touch felt cosy goldfish named brandi enjoyed nt partake free wine coffeetea service lobby thought great feature great staff friendly free wireless internet hotel worked suite laptops decor lovely eclectic mix pattens color palatte animal print bathrobes feel like rock stars nice nt look like sterile chain hotel hotel personality excellent stay',\n",
              "       'great stay great stay went seahawk game awesome downfall view building nt complain room huge staff helpful booked hotels website seahawk package charge parking got voucher taxi problem taxi driver nt want accept voucher barely spoke english funny thing speak arabic called started making comments girlfriend cell phone buddy took second realize said fact speak language face priceless ass told said large city told head doorman issue called cab company promply answer nt apologized offered pay taxi bucks miles stadium game plan taxi return going humpin great walk nt mind right christmas wonderful lights homeless stowed away building entrances leave police presence greatest area stadium activities blocks pike street waterfront great coffee shops way hotel mantained foyer awesome wine tasting available evening best dog taking st bernard time family safes hotel located service desk room bathroom huge jetted tub huge funny house keeping walked girlfriend getting dressed nt hear knock turn service screamed girlfriend screams hit floor laughing started talking spanish worked place recommend price check online deals good better besite contains deals vouchers travel websites nt tell',\n",
              "       'love monaco staff husband stayed hotel crazy weekend attending memorial service best friend husband celebrating th wedding anniversary talk mixed emotions booked suite hotel monte carlos loaned beautiful fantanned goldfish named joliet weekend visited dogs worked desk human companions room decorated nicely couch used pillows loccitane bath amenities welcome sight room quiet peaceful wireless internet access wonderful server went morning leaving problems printing boarding passes afternoon reception serves oenophilesatisfying wine australia scrumptious cookies restaurant closed renovation stay finally ate food good drinks better word caution restaurant larger person sit booths wo nt fit lbs husband table smackagainst stomach couple inches space mighty uncomfortable patron larger pregnant bad design opinion place decorated funky welcoming way metal wood handblown glass light fixtures expect seattle capital glass art industry definitely stay reason',\n",
              "       'cozy stay rainy city husband spent nights monaco early january business trip chance come ridewe booked monte carlo suite proved comfortable longish stay room located street building street noise problem view interesting rooms building look dank alley midsection large office building suite comfortable plenty room spread bathroom attractive squeaky clean small comparison generous proportions sitting sleeping areas lots comfortable seating options good lighting plenty storage clothing luggage hotel staff friendly efficient housekeeping staff great job pleasant requests responded quicklythe location quite good easy walk pike street market seattle art museum notch shopping dining optionsa positive experience',\n",
              "       'excellent staff housekeeping quality hotel chocked staff make feel home experienced exceptional service desk staff concierge door men maid service needs work maid failed tuck sheets foot bed instance soiled sheets used staff quickley resolved soiled sheets issue guess relates employee reflection rest staffwe received excellent advice concierge regarding resturants area happy hour wine tasting nice touch staff went way make feel homegreat location like close good food shopping took play th street theather wellpikes market pioneer square access mono rail short walking distance',\n",
              "       'hotel stayed hotel monaco cruise rooms generous decorated uniquely hotel remodeled pacific bell building charm sturdiness everytime walked bell men felt like coming home secure great single travelers location fabulous walk things pike market space needlelittle grocerydrug store block away today green bravo double bed room room bed couch separated curtain snoring mom slept curtain great food nearby',\n",
              "       'excellent stayed hotel monaco past delight reception staff friendly professional room smart comfortable bed particularly liked reception small dog received staff guests spoke loved mild negative distance uphill ppmarket restaurants st overall great experience'],\n",
              "      dtype='<U12840')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtL6IIv1HC4H"
      },
      "source": [
        "# Since we want to train a CBOW model with context width of 2\n",
        "# on the reviews, we drop all reviews with less than 5 words.\n",
        "# This is equivalent to only keeping instances with at least 5 words.\n",
        "reviews = [review for review in reviews if len(review.split(\" \")) >=  (2*CONTEXT_OFFSET + 1)]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxUXynX0M5iz"
      },
      "source": [
        "# In order to build the corpus for the reviews \n",
        "# we want to find every distinct word that occurs\n",
        "# in at least one review.\n",
        "# We join all reviews into one large string and then\n",
        "# split it at every space to receive a list of words\n",
        "# Then the set method is used in order to only\n",
        "# retain unique words.\n",
        "# This list is then alphabetically sorted\n",
        "reviews_vocabulary = sorted(set((\" \".join(reviews)).split()))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bw6FFmeNW5Or",
        "outputId": "b6638de5-43bb-4fe0-ebeb-354745f3ff50"
      },
      "source": [
        "reviews_vocabulary_size = len(reviews_vocabulary)\n",
        "print(\"The reviews use a vocabulary comprising {} different words.\".format(reviews_vocabulary_size))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The reviews use a vocabulary comprising 75255 different words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErFC1k2kNpCE"
      },
      "source": [
        "word2index = {w:i for i,w in enumerate(reviews_vocabulary)} # Lookup table mapping words to indices\n",
        "index2word = {i:w for i,w in enumerate(reviews_vocabulary)} # Lookup table mapping indices to words"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBwNZhIKcqAo",
        "outputId": "968a7862-0deb-4691-9cec-04274decd243"
      },
      "source": [
        "data = []\n",
        "for review in reviews:\n",
        "  raw_text = review.split()\n",
        "  for i in range(CONTEXT_OFFSET, len(raw_text) - CONTEXT_OFFSET):\n",
        "      context = [raw_text[i - 2], raw_text[i - 1],\n",
        "                raw_text[i + 1], raw_text[i + 2]]\n",
        "      #print(context)\n",
        "      target = raw_text[i]\n",
        "      data.append((context, target))\n",
        "print(data[:5])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['nice', 'hotel', 'parking', 'got'], 'expensive'), (['hotel', 'expensive', 'got', 'good'], 'parking'), (['expensive', 'parking', 'good', 'deal'], 'got'), (['parking', 'got', 'deal', 'stay'], 'good'), (['got', 'good', 'stay', 'hotel'], 'deal')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqPIILq3f9er",
        "outputId": "791a4ee7-a606-4232-93db-3d0a20a86575"
      },
      "source": [
        "# create your model and train.  here are some functions to help you make\n",
        "# the data ready for use by your module\n",
        "def make_context_vector(context, word2index):\n",
        "    idxs = [word2index[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "make_context_vector(data[0][0], word2index)  # example"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([43815, 31861, 47687, 28624])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "069rMbD_Vju4"
      },
      "source": [
        "class HotelReviewsDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.y[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKSnkqC7aPGV"
      },
      "source": [
        "X = np.array([i[0] for i in data])\n",
        "X_vectors = list(map(lambda elem: make_context_vector(elem, word2index) , X))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIrePOCkaQrB"
      },
      "source": [
        "y = np.array([i[1] for i in data])\n",
        "y_vectors = list(map(lambda elem: make_context_vector([elem], word2index), y))"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wFmcwnO9Dq7"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_vectors[:500000], y_vectors[:500000], test_size=0.2, random_state=42)"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWkaD9JtaT9t"
      },
      "source": [
        "hotel_reviews_dataset = HotelReviewsDataset(X_train, y_train)"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OapsjmhTfHxw"
      },
      "source": [
        "## 3. Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RvGC_MGH6k0"
      },
      "source": [
        "hotel_reviews_loader = DataLoader(dataset=hotel_reviews_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3_xQQ5Mw3JQ"
      },
      "source": [
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "      super(CBOW, self).__init__()\n",
        "      self.embeddings = nn.Embedding(vocab_size, embedding_dim, device=device)\n",
        "      self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "      self.activation_function1 = nn.ReLU()\n",
        "      self.linear2 = nn.Linear(128, vocab_size)\n",
        "      self.activation_function2 = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "      embeds = self.embeddings(inputs).view(inputs.size(0), -1)\n",
        "      out = self.linear1(embeds)\n",
        "      out = self.activation_function1(out)\n",
        "      out = self.linear2(out)\n",
        "      out = self.activation_function2(out)\n",
        "      return out\n",
        "\n",
        "    def write_embedding_to_file(self,filename):\n",
        "      for i in self.embeddings.parameters():\n",
        "        weights = i.data.numpy()\n",
        "      np.save(filename,weights)\n",
        "    \n",
        "    def get_word_emdedding(self, word):\n",
        "        word = torch.tensor([word_to_ix[word]])\n",
        "        return self.embeddings(word).view(1,-1)"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB1FCwfH4miH"
      },
      "source": [
        "def train_model(model, data_loader, epochs, word2index):\n",
        "\n",
        "  losses = np.zeros(epochs)\n",
        "  loss_function = nn.NLLLoss()\n",
        "  optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for step, (context_vectors, target_vector) in enumerate(data_loader):\n",
        "\n",
        "      context_vectors = context_vectors.to(device) # Move the batch of context vectors into GPU memory\n",
        "      target_vector = target_vector.to(device) # Move the batch of target vectors into GPU memory \n",
        "\n",
        "      model.zero_grad() # Reset all gradients back to zero\n",
        "\n",
        "      log_probs = model(context_vectors) # forward pass\n",
        "      loss = loss_function(log_probs, torch.squeeze(target_vector)) # compute loss for batch\n",
        "      losses[epoch] += loss.item() # accumulate loss\n",
        "\n",
        "      loss.backward() # backpropagation\n",
        "      optimizer.step() # update the model weights\n",
        "\n",
        "    print(\"Epoch {0}/{1} ... Average loss {2}\".format(epoch+1, epochs, losses[epoch] / len(data_loader.dataset))) # Print average loss in this episode\n",
        "  return losses"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nruXd09DFRjB",
        "outputId": "7eb55ba9-31d2-4180-9f24-7edb38636627"
      },
      "source": [
        "model = CBOW(reviews_vocabulary_size, EMBEDDING_DIM_HOTEL, 2*CONTEXT_OFFSET).to(device)\n",
        "losses = train_model(model, hotel_reviews_loader, EPOCHS_HOTEL, word2index)"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12 ... Average loss 0.1255742033147812\n",
            "Epoch 2/12 ... Average loss 0.11557961870789528\n",
            "Epoch 3/12 ... Average loss 0.10951060964822769\n",
            "Epoch 4/12 ... Average loss 0.10464652255535126\n",
            "Epoch 5/12 ... Average loss 0.10073291439414024\n",
            "Epoch 6/12 ... Average loss 0.09756090621948242\n",
            "Epoch 7/12 ... Average loss 0.09486423535823822\n",
            "Epoch 8/12 ... Average loss 0.09255975857257843\n",
            "Epoch 9/12 ... Average loss 0.0905779037463665\n",
            "Epoch 10/12 ... Average loss 0.08882011162638664\n",
            "Epoch 11/12 ... Average loss 0.08722553806781769\n",
            "Epoch 12/12 ... Average loss 0.08578425164818763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8axqhdABxgQv"
      },
      "source": [
        "#TESTING\n",
        "#context = ['People','create','to', 'direct']\n",
        "#context_vector = make_context_vector(context, word_to_ix)\n",
        "#a = model(context_vector)\n",
        "\n",
        "#Print result\n",
        "#print(f'Raw text: {\" \".join(raw_text)}\\n')\n",
        "#print(f'Context: {context}\\n')\n",
        "#print(f'Prediction: {ix_to_word[torch.argmax(a[0]).item()]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX8iEZ15klxd"
      },
      "source": [
        "# Function that will be called in every epoch\n",
        "def train_epoch(loss_function, optimizer, model, loader):\n",
        "  \n",
        "  # Keep track of the total loss for the batch\n",
        "  total_loss = 0\n",
        "  for batch_inputs, batch_labels, batch_lengths in loader:\n",
        "    # Clear the gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Run a forward pass\n",
        "    outputs = model.forward(batch_inputs)\n",
        "    # Compute the batch loss\n",
        "    loss = loss_function(outputs, batch_labels, batch_lengths)\n",
        "    # Calculate the gradients\n",
        "    loss.backward()\n",
        "    # Update the parameteres\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  return total_loss\n",
        "\n",
        "\n",
        "# Function containing our main training loop\n",
        "def train(loss_function, optimizer, model, loader, num_epochs=10000):\n",
        "\n",
        "  # Iterate through each epoch and call our train_epoch function\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_loss = train_epoch(loss_function, optimizer, model, loader)\n",
        "    if epoch % 100 == 0: print(epoch_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h6YNgfI7lAb"
      },
      "source": [
        "\n",
        "\n",
        "def make_context_vector(context, word_to_ix):\n",
        "    idxs = [word_to_ix[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
        "\n",
        "\n",
        "# By deriving a set from `raw_text`, we deduplicate the array\n",
        "vocab = set(raw_text)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n",
        "ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
        "\n",
        "data = []\n",
        "for i in range(2, len(raw_text) - 2):\n",
        "    context = [raw_text[i - 2], raw_text[i - 1],\n",
        "               raw_text[i + 1], raw_text[i + 2]]\n",
        "    target = raw_text[i]\n",
        "    data.append((context, target))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#TRAINING\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "\n",
        "    for context, target in data:\n",
        "        context_vector = make_context_vector(context, word_to_ix)  \n",
        "\n",
        "        log_probs = model(context_vector)\n",
        "\n",
        "        total_loss += loss_function(log_probs, torch.tensor([word_to_ix[target]]))\n",
        "\n",
        "    #optimize at the end of each epoch\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "#TESTING\n",
        "context = ['People','create','to', 'direct']\n",
        "context_vector = make_context_vector(context, word_to_ix)\n",
        "a = model(context_vector)\n",
        "\n",
        "#Print result\n",
        "print(f'Raw text: {\" \".join(raw_text)}\\n')\n",
        "print(f'Context: {context}\\n')\n",
        "print(f'Prediction: {ix_to_word[torch.argmax(a[0]).item()]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyY72CQxeaFL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}