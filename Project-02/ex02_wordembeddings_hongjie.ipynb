{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "ex02_wordembeddings_hongjie.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jan-kreischer/UZH_ML4NLP/blob/main/Project-02/ex02_wordembeddings_hongjie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFNNOVmLD-Fz"
      },
      "source": [
        "# Project 2 - Word Embeddings with PyTorch\n",
        "\n",
        "The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep\n",
        "learning. It is a model that tries to predict words given the context of\n",
        "a few words before and a few words after the target word. This is\n",
        "distinct from language modeling, since CBOW is not sequential and does\n",
        "not have to be probabilistic. Typcially, CBOW is used to quickly train\n",
        "word embeddings, and these embeddings are used to initialize the\n",
        "embeddings of some more complicated model. Usually, this is referred to\n",
        "as *pretraining embeddings*. It almost always helps performance a couple\n",
        "of percent.\n",
        "\n",
        "The CBOW model is as follows. Given a target word $w_i$ and an\n",
        "$N$ context window on each side, $w_{i-1}, \\dots, w_{i-N}$\n",
        "and $w_{i+1}, \\dots, w_{i+N}$, referring to all context words\n",
        "collectively as $C$, CBOW tries to minimize\n",
        "\n",
        "\\begin{align}-\\log p(w_i | C) = -\\log \\text{Softmax}(A(\\sum_{w \\in C} q_w) + b)\\end{align}\n",
        "\n",
        "where $q_w$ is the embedding for word $w$.\n",
        "\n",
        "Implement this model in Pytorch by filling in the class below. Some\n",
        "tips:\n",
        "\n",
        "* Think about which parameters you need to define.\n",
        "* Make sure you know what shape each operation expects. Use .view() if you need to\n",
        "  reshape.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHXvfNTXD4Ca"
      },
      "source": [
        "## Part 1: Training CBOW embeddings for both datasets\n",
        "### 1. Setup\n",
        "#### 1.1 Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EATjb-X-0lM",
        "outputId": "6ac8d36d-064a-42e3-9286-448bda121f3a"
      },
      "source": [
        "import os\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# numpy and pandas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('max_colwidth', 800)\n",
        "\n",
        "# tokenization\n",
        "import nltk;\n",
        "nltk.download('stopwords');\n",
        "%matplotlib inline\n",
        "\n",
        "from argparse import Namespace\n",
        "from collections import Counter\n",
        "import json\n",
        "import string\n",
        "import itertools\n",
        "import regex as re\n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.model_selection import train_test_split\n",
        "import requests"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKjgo1cEEQPw"
      },
      "source": [
        "#### 1.2 Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9Tszg7FdPrZ",
        "outputId": "5acc7b94-000e-42ef-b6e9-56489d66c8a8"
      },
      "source": [
        "CUDA = torch.cuda.is_available()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu:0')\n",
        "torch.cuda.set_device(device)\n",
        "print('Using device:', device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4ZgunZ9qSvu",
        "outputId": "3c70eae9-19c0-4fca-8fe9-4d31ea3ed520"
      },
      "source": [
        "# Check GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct 26 15:02:20 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    30W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keYYVJLbnpbD",
        "outputId": "6b32ed32-d1a2-4d72-e1f1-5c29d4c73815"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-ea421b17-19f6-2e8c-d0c8-f0ab76a8e6e0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGFVru-nqX1n",
        "outputId": "86a47a1d-3600-4b3d-e0e3-0daa71dbbb25"
      },
      "source": [
        "# Check Memory\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW7vvEZkEdrX"
      },
      "source": [
        "#### 1.3 Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grG2YIi8-aYu"
      },
      "source": [
        "FEATURE_COLUMN = 'Review'\n",
        "CONTEXT_OFFSET = 2 # n words to the left, n to the right\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "EPOCHS_HOTEL = 30\n",
        "EMBEDDING_DIM_HOTEL = 50\n",
        "\n",
        "EPOCHS_SCIFI = 5\n",
        "EMBEDDING_DIM_SCIFI = 50"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Gy3d3hJQZB9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAqmKotTv5FX"
      },
      "source": [
        "### Hotel Reviews dataset\n",
        "### 2. Data Preprocessing\n",
        "#### 2.1 Data Acquisition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSA-k-FGGYVx",
        "outputId": "899573b5-30ce-4fc0-b911-ea9263a2331f"
      },
      "source": [
        "# Loading the tripadvisor data\n",
        "url_tripadvisor = (r'https://raw.githubusercontent.com/abandonedrepo/test/master/tripadvisor_hotel_reviews.csv')\n",
        "reviews_dataset = pd.read_csv(url_tripadvisor)\n",
        "reviews_dataset.info()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20491 entries, 0 to 20490\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Review  20491 non-null  object\n",
            " 1   Rating  20491 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 320.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "MpAY67nPvFw9",
        "outputId": "4d37f945-0e01-4dee-febf-5dad1a96841a"
      },
      "source": [
        "reviews_dataset.head(5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>nice hotel expensive parking got good deal stay hotel anniversary, arrived late evening took advice previous reviews did valet parking, check quick easy, little disappointed non-existent view room room clean nice size, bed comfortable woke stiff neck high pillows, not soundproof like heard music room night morning loud bangs doors opening closing hear people talking hallway, maybe just noisy neighbors, aveda bath products nice, did not goldfish stay nice touch taken advantage staying longer, location great walking distance shopping, overall nice experience having pay 40 parking night,</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ok nothing special charge diamond member hilton decided chain shot 20th anniversary seattle, start booked suite paid extra website description not, suite bedroom bathroom standard hotel room, took printed reservation desk showed said things like tv couch ect desk clerk told oh mixed suites description kimpton website sorry free breakfast, got kidding, embassy suits sitting room bathroom bedroom unlike kimpton calls suite, 5 day stay offer correct false advertising, send kimpton preferred guest website email asking failure provide suite advertised website reservation description furnished hard copy reservation printout website desk manager duty did not reply solution, send email trip guest survey did not follow email mail, guess tell concerned guest.the staff ranged indifferent not help...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>nice rooms not 4* experience hotel monaco seattle good hotel n't 4* level.positives large bathroom mediterranean suite comfortable bed pillowsattentive housekeeping staffnegatives ac unit malfunctioned stay desk disorganized, missed 3 separate wakeup calls, concierge busy hard touch, did n't provide guidance special requests.tv hard use ipod sound dock suite non functioning. decided book mediterranean suite 3 night weekend stay 1st choice rest party filled, comparison w spent 45 night larger square footage room great soaking tub whirlpool jets nice shower.before stay hotel arrange car service price 53 tip reasonable driver waiting arrival.checkin easy downside room picked 2 person jacuzi tub no bath accessories salts bubble bath did n't stay, night got 12/1a checked voucher bottle cham...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>unique, great stay, wonderful time hotel monaco, location excellent short stroll main downtown shopping area, pet friendly room showed no signs animal hair smells, monaco suite sleeping area big striped curtains pulled closed nice touch felt cosy, goldfish named brandi enjoyed, did n't partake free wine coffee/tea service lobby thought great feature, great staff friendly, free wireless internet hotel worked suite 2 laptops, decor lovely eclectic mix pattens color palatte, animal print bathrobes feel like rock stars, nice did n't look like sterile chain hotel hotel personality excellent stay,</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>great stay great stay, went seahawk game awesome, downfall view building did n't complain, room huge staff helpful, booked hotels website seahawk package, no charge parking got voucher taxi, problem taxi driver did n't want accept voucher barely spoke english, funny thing speak arabic called started making comments girlfriend cell phone buddy, took second realize just said fact speak language face priceless, ass told, said large city, told head doorman issue called cab company promply answer did n't, apologized offered pay taxi, bucks 2 miles stadium, game plan taxi return going humpin, great walk did n't mind, right christmas wonderful lights, homeless stowed away building entrances leave, police presence not greatest area stadium, activities 7 blocks pike street waterfront great coff...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Review  Rating\n",
              "0                                                                                                                                                                                                                nice hotel expensive parking got good deal stay hotel anniversary, arrived late evening took advice previous reviews did valet parking, check quick easy, little disappointed non-existent view room room clean nice size, bed comfortable woke stiff neck high pillows, not soundproof like heard music room night morning loud bangs doors opening closing hear people talking hallway, maybe just noisy neighbors, aveda bath products nice, did not goldfish stay nice touch taken advantage staying longer, location great walking distance shopping, overall nice experience having pay 40 parking night,         4\n",
              "1  ok nothing special charge diamond member hilton decided chain shot 20th anniversary seattle, start booked suite paid extra website description not, suite bedroom bathroom standard hotel room, took printed reservation desk showed said things like tv couch ect desk clerk told oh mixed suites description kimpton website sorry free breakfast, got kidding, embassy suits sitting room bathroom bedroom unlike kimpton calls suite, 5 day stay offer correct false advertising, send kimpton preferred guest website email asking failure provide suite advertised website reservation description furnished hard copy reservation printout website desk manager duty did not reply solution, send email trip guest survey did not follow email mail, guess tell concerned guest.the staff ranged indifferent not help...       2\n",
              "2  nice rooms not 4* experience hotel monaco seattle good hotel n't 4* level.positives large bathroom mediterranean suite comfortable bed pillowsattentive housekeeping staffnegatives ac unit malfunctioned stay desk disorganized, missed 3 separate wakeup calls, concierge busy hard touch, did n't provide guidance special requests.tv hard use ipod sound dock suite non functioning. decided book mediterranean suite 3 night weekend stay 1st choice rest party filled, comparison w spent 45 night larger square footage room great soaking tub whirlpool jets nice shower.before stay hotel arrange car service price 53 tip reasonable driver waiting arrival.checkin easy downside room picked 2 person jacuzi tub no bath accessories salts bubble bath did n't stay, night got 12/1a checked voucher bottle cham...       3\n",
              "3                                                                                                                                                                                                         unique, great stay, wonderful time hotel monaco, location excellent short stroll main downtown shopping area, pet friendly room showed no signs animal hair smells, monaco suite sleeping area big striped curtains pulled closed nice touch felt cosy, goldfish named brandi enjoyed, did n't partake free wine coffee/tea service lobby thought great feature, great staff friendly, free wireless internet hotel worked suite 2 laptops, decor lovely eclectic mix pattens color palatte, animal print bathrobes feel like rock stars, nice did n't look like sterile chain hotel hotel personality excellent stay,         5\n",
              "4  great stay great stay, went seahawk game awesome, downfall view building did n't complain, room huge staff helpful, booked hotels website seahawk package, no charge parking got voucher taxi, problem taxi driver did n't want accept voucher barely spoke english, funny thing speak arabic called started making comments girlfriend cell phone buddy, took second realize just said fact speak language face priceless, ass told, said large city, told head doorman issue called cab company promply answer did n't, apologized offered pay taxi, bucks 2 miles stadium, game plan taxi return going humpin, great walk did n't mind, right christmas wonderful lights, homeless stowed away building entrances leave, police presence not greatest area stadium, activities 7 blocks pike street waterfront great coff...       5"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jay7YAfne0wo"
      },
      "source": [
        "#### 2.2 Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0rNpT9gd4bp"
      },
      "source": [
        "wpt = nltk.WordPunctTokenizer()\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def clean_document(x):\n",
        "    x = re.sub(r'\\w*\\d\\w*', ' ', x)\n",
        "    x = re.sub(r'[^a-zA-Z\\s]', ' ', x.lower(), re.I|re.A)\n",
        "    x = re.sub(r'[\\-!+_@*#\\/$:)\"\\'.;,?&({}[]]*', ' ', x)\n",
        "    x = re.sub(r'\\b\\w{1,2}\\b', ' ', x)\n",
        "    x = re.sub(' +', ' ', x)\n",
        "    tokens = wpt.tokenize(x)\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    x = ' '.join(filtered_tokens)\n",
        "    return x\n",
        "\n",
        "clean_corpus = np.vectorize(clean_document)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgtrtfqq-kwj"
      },
      "source": [
        "# Clean the reviews by removing punctuation characters and stopwords.\n",
        "reviews = clean_corpus(reviews_dataset['Review'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwSUTo0zz-q-",
        "outputId": "f2ce14a9-46ce-43f8-91ea-c429b82b0c4d"
      },
      "source": [
        "np.random.choice(reviews, 10)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['cool barcelona hotel hotel barcelona cool place stay rooms clean modern rest space roof pool lounge neat hotel right placa espanya major transport station magic fountain light base montjuic end august adn great time hotel staff accomodating helpful room service reasonably priced night flew stayed ordered rom service hough normally cons immediate area restaurant hotel breakfast daily early',\n",
              "       'gracious elegant husband recently spent nights huntington hotel best small hotels stayed trip pleasure nob hill location provides central access areas city ambiance hotel public areas hotel rooms understated elegant staff gracious accomodating enjoyed using facilities nob hill spa attached hotel stay stay plan book massage dined hotel restaurant big good especially enjoyed pianist adjacent lounge hotel reminds stanhope hotel manhattan stayed years ago think hotel converted private apartments',\n",
              "       'amsterdam experience nadia hotel sister world vacation amsterdam nadia hotel far best nadia hotel centrally located downtown amsterdam walking distance ann frank house dam square central station kalverstraat debijenkorf department store pretty downtown amsterdam bedroom overlooked canal wich believe stunning view summer given weather situation visit sight behold charming little kitchen balcony overlooking canal serves breakfast breakfast missed dining enjoyable way start day osam owner wonderful staff beck boundless hospitality leaving nadia hotel difficult sister said place twice given royal treatment received make exception case definitely recommend nadia hotel future travelers make amsterdam vacation choice hotel deserves star',\n",
              "       'amazing got amazing week lti dominican know expect reading reviews bit worried flight skyservice tight uneventful quickly got customs punta cana airport shuttle lti waiting everone bus going lti straight run like minute ride got resort entered beautiful foyer checked greeted drink waitress headed room room clean nice changed headed pool pleased lush green gardens pools lots lawn chairs tiki huts sit shade needed late morning noon managed everyday headed beach stones throw pool paradise beautiful white flour soft sand tall green palm trees swaying warm tropical breeze clean shoreline water beautiful tourquoise lots lawn chairs tiki huts restaurants awesome excellent food variety food drinks stopped main buffet fresh omelets breakfast imaginable lunch dinner food dinner wonderful chance bored buffet wonderful smaller buffet right beach serves breakfast lunch dinner grill going barbequing burgers dogs sausages chicken dinner add roast beef steak pork fish pasta salads buffet italian restaurant nice awesome lasagna good pizza wonderful lemon lime icecream puts refreshing end meal seafood restaurant allowed alacarte restaurants everynight wanted reserve morning lots fun fun games staff enjoyed banana boat catarmaran kayaks excursions offered sunquest rep book resort offers excursions weather great nice warm beautiful breeze rain days afternoon nice relief blazing sun finally wonderful dominican people work resort friendly happy energetic pleasant kind service excellent husband took teens none wanted come home lti punta cana tomorrow highly recommend lti',\n",
              "       'miss sunscape husband went suncape year anniversary imagined enjoyable trip admit reading majority reviews trip advisor prior leaving nervous arrived realized absolutely nothing worry hour fourth airport unless driver rudy got minutes gives chance different culture lucky arrived hotel greeted juice cold towel check check breeze room great better expected got junior suite jacuzzi balcony happy choice couple days rain jacuzzi came handy staff wonderful think time passed staff members greet smile say hola pleasant happy help entertained pillow fighting shuffleboard contest volleyball want participate fun watch shows night fun watch participated couple vacation food good cant complain sick drank water dreams favorite japanese lot fun entertaining world cafe huge wonderful selection cant wrong husband dinner beach anniversary evening far worth extra strongly recommend little concerned language barrier speak spanish absolutely nothing worry ordered drinks bars trouble communicating wanted drinks great leon barracuda bar hilarious says goal people drunk sure reached goal times spa wonderful got couples hot stone massage far best massage experienced casino lot fun sports bar kinds different activities tennis pool racquetball football watch thing say got old venders pushy learn say thanks walking interested trying make living old definitely heartbeat sad leave flew',\n",
              "       'hotel adagio looking stay near union square hotel adagio best place stay blocks union square walked great restaurants bars ease amazing make older hotel turn trendy comfortable atmosphere hotel adagio makes feel important special member staff outgoing accommodating bellman valet try refer hotel adagio anybody visits san francisco anybody looking romantic getaway trendy twist',\n",
              "       'adequate quirky hotel stayed twice sky court attend nfl pro bowl february enjoyed stays room rates reasonable waikiki desk staff concierge helpful friendly couple quirky issues slow elevators nearly bad previously chronicled walls occasional transgender prostitutes ply trade lobby area asked rooms lowest floor possible mitigate elevator concern issue hawaii usually stay room heard people activities adjoining rooms minor inconvenience helped foam earplugs rental car visit pool area visit help tgifridays hotel lobby fine happy hour drinks appetizers main entrees disappointing waste dining dollars excellent options nearby personal favorites duke waikiki kalakaua sky court favorably located near major city bus lines world class shopping venues aloha',\n",
              "       'hotel helped enjoyable trip madrid wife stayed nights got taxi airport hotel euro shuttle bus service hotel airport euro asked nice room got facing gran noise soundproof windows red light street hotel distraction staff friendly helpful metro station right outside door ample selection breakfast hotel located sightseeing plaza espana plaza mayor plaza sol app mins walk hotel bernabua stadium app mins away metro euro tour worth football fan overall hotel guest friendly located good value money married couple dublin ireland',\n",
              "       'great location went large group grad students location great easy cab drivers know connected mall near great foot massage place lobby nice breakfast buffet restaurant wide selection food rooms average nice artwork relatively clean brand new comfortable stay service great staff speak english business center floor',\n",
              "       'best deal town mind shabby lobby moore hotel best deal seattle recent winter weekend charging half downtown hotels wanted let somewhat shabby lobby scare desk staff great service lacking broken light fixture closet went unrepaired despite requests energy effiicient light bulbs great avoid rooms bath hall finally ask quiet room away busy streets beat location close pike place market restaurants shops bell town bus airport non stop'],\n",
              "      dtype='<U12603')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtL6IIv1HC4H"
      },
      "source": [
        "# Since we want to train a CBOW model with context width of 2\n",
        "# on the reviews, we drop all reviews with less than 5 words.\n",
        "# This is equivalent to only keeping instances with at least 5 words.\n",
        "reviews = [review for review in reviews if len(review.split(\" \")) >=  (2*CONTEXT_OFFSET + 1)]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjNI3ufc6nFa"
      },
      "source": [
        "reviews_word_list=list((\" \".join(reviews)).split())\n",
        "\n",
        "frequency = pd.value_counts(reviews_word_list)\n",
        "infrequent_words = list(frequency[frequency <= 1].keys())\n",
        "frequent_words = list(frequency[frequency > 1].keys())\n",
        "\n",
        "is_infrequent = {}\n",
        "for infrequent_word in infrequent_words:\n",
        "  is_infrequent[infrequent_word] = 1\n",
        "\n",
        "for frequent_word in frequent_words:\n",
        "  is_infrequent[frequent_word] = 0"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B7bSUCDOWjk",
        "outputId": "261d6bc2-dcf3-490a-af48-440c94f87fe5"
      },
      "source": [
        "print(\"The length of the frequent_words:{}\".format(len(frequent_words)))\n",
        "print(\"The length of the infrequent_words:{}\".format(len(infrequent_words)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of the frequent_words:24730\n",
            "The length of the infrequent_words:23833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxUXynX0M5iz"
      },
      "source": [
        "# In order to build the corpus for the reviews \n",
        "# we want to find every distinct word that occurs\n",
        "# in at least one review.\n",
        "# We join all reviews into one large string and then\n",
        "# split it at every space to receive a list of words\n",
        "# Then the set method is used in order to only\n",
        "# retain unique words.\n",
        "# This list is then alphabetically sorted\n",
        "review_words = \" \".join(reviews).split()\n",
        "review_words = [w for w in review_words if not is_infrequent[w]]\n",
        "reviews_vocabulary = sorted(set(review_words))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bw6FFmeNW5Or",
        "outputId": "a208a249-1ccb-47ee-96f6-d19338dc0f77"
      },
      "source": [
        "reviews_vocabulary_size = len(reviews_vocabulary)\n",
        "print(\"The reviews use a vocabulary comprising {} different words.\".format(reviews_vocabulary_size))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The reviews use a vocabulary comprising 24730 different words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErFC1k2kNpCE"
      },
      "source": [
        "word2index = {w:i for i,w in enumerate(reviews_vocabulary)} # Lookup table mapping words to indices\n",
        "index2word = {i:w for i,w in enumerate(reviews_vocabulary)} # Lookup table mapping indices to words"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQYOKdbna5aC"
      },
      "source": [
        "#To clean the infrequnt words\n",
        "\n",
        "def clean_infrequent_words(reviews):\n",
        "  new_reviews=[]\n",
        "  for review in reviews:\n",
        "    infrequent_index=[]\n",
        "    raw_text = review.split()\n",
        "    for i in range(0,len(raw_text)):\n",
        "      if is_infrequent[raw_text[i]]==1:\n",
        "        infrequent_index.append(i)\n",
        "    for i in sorted(infrequent_index,reverse=True):\n",
        "      del raw_text[i]\n",
        "    review=' '.join(raw_text)\n",
        "    new_reviews.append(review)\n",
        "  return new_reviews\n",
        "\n",
        "reviews=clean_infrequent_words(reviews)\n",
        "# drop review with less than 5 words again\n",
        "reviews = [review for review in reviews if len(review.split(\" \")) >=  (2*CONTEXT_OFFSET + 1)]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBwNZhIKcqAo"
      },
      "source": [
        "data = []\n",
        "for review in reviews:\n",
        "  raw_text = review.split()\n",
        "  for i in range(CONTEXT_OFFSET, len(raw_text) - CONTEXT_OFFSET):\n",
        "      context = [raw_text[i - 2], raw_text[i - 1],\n",
        "                raw_text[i + 1], raw_text[i + 2]]\n",
        "      #print(context)\n",
        "      target = raw_text[i]\n",
        "      data.append((context, target))\n",
        "\n",
        "# Show some sample 'context -> center word' mappings\n",
        "#for i in range(5):\n",
        "#  print(data[i])\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqPIILq3f9er",
        "outputId": "42f9052b-53f9-41c6-e5fe-0a21099bbc14"
      },
      "source": [
        "# The following function transforms the context\n",
        "# into index notation\n",
        "def make_context_vector(context, word2index):\n",
        "    idxs = [word2index[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "# Show one transformed sample context\n",
        "make_context_vector(data[0][0], word2index)  # example"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([14585, 10605, 15681,  9535])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "069rMbD_Vju4"
      },
      "source": [
        "class HotelReviewsDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.y[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKSnkqC7aPGV"
      },
      "source": [
        "X = np.array([i[0] for i in data])\n",
        "X_vectors = list(map(lambda elem: make_context_vector(elem, word2index) , X))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElJk2mFlJG6Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f084042-a162-40f7-b4de-6ddefb926d86"
      },
      "source": [
        "# Print some vetorized sample contexts\n",
        "for i in range(5):\n",
        "  print(X_vectors[i])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([14585, 10605, 15681,  9535])\n",
            "tensor([10605,  7848,  9535,  9494])\n",
            "tensor([ 7848, 15681,  9494,  5718])\n",
            "tensor([15681,  9535,  5718, 20853])\n",
            "tensor([ 9535,  9494, 20853, 10605])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIrePOCkaQrB"
      },
      "source": [
        "y = np.array([i[1] for i in data])\n",
        "y_vectors = list(map(lambda elem: make_context_vector([elem], word2index), y))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFP9_whfKkxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1ea4e8c-3b09-4390-b98a-3cae2ac115db"
      },
      "source": [
        "# Print some vectorized sample center words\n",
        "for i in range(5):\n",
        "  print(y_vectors[i])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([7848])\n",
            "tensor([15681])\n",
            "tensor([9535])\n",
            "tensor([9494])\n",
            "tensor([5718])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wFmcwnO9Dq7"
      },
      "source": [
        "# Split into training and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectors[:500000], y_vectors[:500000], test_size=0.2, random_state=42, shuffle=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWkaD9JtaT9t"
      },
      "source": [
        "# Create the training dataset from vectors\n",
        "hotel_reviews_dataset = HotelReviewsDataset(X_train, y_train)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OapsjmhTfHxw"
      },
      "source": [
        "### 3. Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RvGC_MGH6k0"
      },
      "source": [
        "hotel_reviews_loader = DataLoader(dataset=hotel_reviews_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3_xQQ5Mw3JQ"
      },
      "source": [
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "      super(CBOW, self).__init__()\n",
        "      self.embeddings = nn.Embedding(vocab_size, embedding_dim, device=device)\n",
        "      self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "      self.activation_function1 = nn.ReLU()\n",
        "      self.linear2 = nn.Linear(128, vocab_size)\n",
        "      self.activation_function2 = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "      embeds = self.embeddings(inputs).view(inputs.size(0), -1)\n",
        "      out = self.linear1(embeds)\n",
        "      out = self.activation_function1(out)\n",
        "      out = self.linear2(out)\n",
        "      out = self.activation_function2(out)\n",
        "      return out"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB1FCwfH4miH"
      },
      "source": [
        "def train_model(model, data_loader, epochs, word2index):\n",
        "\n",
        "  losses = np.zeros(epochs)\n",
        "  loss_function = nn.NLLLoss()\n",
        "  optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for step, (context_vectors, target_vector) in enumerate(data_loader):\n",
        "\n",
        "      context_vectors = context_vectors.to(device) # Move the batch of context vectors into GPU memory\n",
        "      target_vector = target_vector.to(device) # Move the batch of target vectors into GPU memory \n",
        "\n",
        "      model.zero_grad() # Reset all gradients back to zero\n",
        "\n",
        "      log_probs = model(context_vectors) # forward pass\n",
        "      loss = loss_function(log_probs, torch.squeeze(target_vector)) # compute loss for batch\n",
        "      losses[epoch] += loss.item() # accumulate loss\n",
        "\n",
        "      loss.backward() # backpropagation\n",
        "      optimizer.step() # update the model weights\n",
        "\n",
        "    print(\"Epoch {0}/{1} ... Average loss {2}\".format(epoch+1, epochs, losses[epoch] / len(data_loader.dataset))) # Print average loss in this episode\n",
        "  return losses"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkd783s1i4tS",
        "outputId": "24fe5248-6368-4d6b-c1ec-cbb04ee9eecc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Only run this cell if you want to load a saved CBOW model including embeddings.\n",
        "model = CBOW(reviews_vocabulary_size, EMBEDDING_DIM_HOTEL, 2*CONTEXT_OFFSET).to(device)\n",
        "model_path='./hotel_reviews_model_weights.pth'\n",
        "\n",
        "\n",
        "# try:\n",
        "#   model.load_state_dict(torch.load(model_path))\n",
        "#   model.eval()\n",
        "# except Exception as e:\n",
        "print(\"No saved embeddings exist.\")\n",
        "print(\"Starting to learn word embeddings.\")\n",
        "losses = train_model(model, hotel_reviews_loader, EPOCHS_HOTEL, word2index)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No saved embeddings exist.\n",
            "Starting to learn word embeddings.\n",
            "Epoch 1/30 ... Average loss 0.1199288639485836\n",
            "Epoch 2/30 ... Average loss 0.1108083882677555\n",
            "Epoch 3/30 ... Average loss 0.10474874131917954\n",
            "Epoch 4/30 ... Average loss 0.10001808035969734\n",
            "Epoch 5/30 ... Average loss 0.09631988632798195\n",
            "Epoch 6/30 ... Average loss 0.0932952623140812\n",
            "Epoch 7/30 ... Average loss 0.0908410360121727\n",
            "Epoch 8/30 ... Average loss 0.08877986954450608\n",
            "Epoch 9/30 ... Average loss 0.08704138385176659\n",
            "Epoch 10/30 ... Average loss 0.08551703315019607\n",
            "Epoch 11/30 ... Average loss 0.08420399794816971\n",
            "Epoch 12/30 ... Average loss 0.08305943539381028\n",
            "Epoch 13/30 ... Average loss 0.08209802722454071\n",
            "Epoch 14/30 ... Average loss 0.08122771304488183\n",
            "Epoch 15/30 ... Average loss 0.08048059151530265\n",
            "Epoch 16/30 ... Average loss 0.07980409189999103\n",
            "Epoch 17/30 ... Average loss 0.07923497488558293\n",
            "Epoch 18/30 ... Average loss 0.07865650351166725\n",
            "Epoch 19/30 ... Average loss 0.07818474381029605\n",
            "Epoch 20/30 ... Average loss 0.07771708149373531\n",
            "Epoch 21/30 ... Average loss 0.07729950921714306\n",
            "Epoch 22/30 ... Average loss 0.07693665847539902\n",
            "Epoch 23/30 ... Average loss 0.0765779773414135\n",
            "Epoch 24/30 ... Average loss 0.07626300867378712\n",
            "Epoch 25/30 ... Average loss 0.0759441754835844\n",
            "Epoch 26/30 ... Average loss 0.0756610481864214\n",
            "Epoch 27/30 ... Average loss 0.07540792542278767\n",
            "Epoch 28/30 ... Average loss 0.07517929400682449\n",
            "Epoch 29/30 ... Average loss 0.07494981319069863\n",
            "Epoch 30/30 ... Average loss 0.07471913980722428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fj0D26mgDpS"
      },
      "source": [
        "# Save the trained CBOW model\n",
        "torch.save(model.state_dict(), model_path)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI4F2RhxQsli"
      },
      "source": [
        "### Sci-Fi story dataset\n",
        "### 2. Data Preprocessing\n",
        "#### 2.1 Data Acquisition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyhZrrzEM4oK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ea4addb-2acc-4708-90b4-d0c86a26e69c"
      },
      "source": [
        "# Loading the scifi txt\n",
        "url = 'https://raw.githubusercontent.com/abandonedrepo/test/master/scifi.txt'\n",
        "scifi_dataset = requests.get(url).text\n",
        "print(scifi_dataset[:100])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MARCH # All Stories New and Complete Publisher Editor IF is published bi-monthly by Quinn Publishing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sLOZKW_Q-Xx"
      },
      "source": [
        "#### 2.2 Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JbPCiNSQ-Xy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdbe3837-e5e8-46af-f03f-ade5e1685255"
      },
      "source": [
        "# Clean the scifi text by removing punctuation and stop words\n",
        "\n",
        "def clean_document(x):\n",
        "    x = re.sub(r'\\w*\\d\\w*', ' ', x)\n",
        "    x = re.sub(r'[^a-zA-Z\\s]', ' ', x.lower(), re.I|re.A)\n",
        "    x = re.sub(r'[\\!#+_@*\\/$:)\"\\'.;,?&({}[]]*', ' ', x)\n",
        "    x = re.sub(r'\\b\\w{1,2}\\b', ' ', x)\n",
        "    x = re.sub('-', ' ', x)\n",
        "    tokens = wpt.tokenize(x)\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    x = ' '.join(filtered_tokens)\n",
        "    return x\n",
        "\n",
        "\n",
        "scifi_txt = clean_document(scifi_dataset)\n",
        "print(scifi_txt[:100])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "march stories new complete publisher editor published monthly quinn publishing company inc kingston \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RqYPvfAWjft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f09cf12-ffa8-4c02-d17f-c5662e278450"
      },
      "source": [
        "# Split the scifi text into individual words\n",
        "scifi_word_list=scifi_txt.split()\n",
        "print(scifi_word_list[:10])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['march', 'stories', 'new', 'complete', 'publisher', 'editor', 'published', 'monthly', 'quinn', 'publishing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7enDbQkVZX7_",
        "outputId": "94b9940b-33bb-425b-c9d4-0e815b9bdea0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"The length of scifi_word_list without cleaning infrequent words:{}\".format(len(scifi_word_list)))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of scifi_word_list without cleaning infrequent words:7602971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmH_3Ac3TVcy"
      },
      "source": [
        "frequency = pd.value_counts(scifi_word_list)\n",
        "infrequent_words = list(frequency[frequency <= 100].keys())\n",
        "frequent_words = list(frequency[frequency > 100].keys())\n",
        "\n",
        "is_infrequent = {}\n",
        "for infrequent_word in infrequent_words:\n",
        "  is_infrequent[infrequent_word] = 1\n",
        "for frequent_word in frequent_words:\n",
        "  is_infrequent[frequent_word] = 0"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2jCyVPOTDgN",
        "outputId": "d17405dc-730e-4789-869b-eb183e165d34"
      },
      "source": [
        "#To clean the infrequnt words\n",
        "\n",
        "def clean_infrequent_words(scifi_txt):\n",
        "  scifi_new_list=[]\n",
        "  raw_text = scifi_txt.split()\n",
        "  for i in range(0,len(raw_text)):\n",
        "    a=raw_text[i]\n",
        "    if is_infrequent[a]==0:\n",
        "        scifi_new_list.append(a)\n",
        "  scifi_txt_new=' '.join(scifi_new_list)\n",
        "  return scifi_txt_new\n",
        "\n",
        "scifi_txt=clean_infrequent_words(scifi_txt)\n",
        "print(scifi_txt[:1000])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "march stories new complete publisher editor published monthly quinn publishing company inc new york volume copyright quinn publishing company inc application entry second class matter post office buffalo new york subscription issues possessions canada issues elsewhere four weeks change address stories appearing magazine fiction similarity actual persons coincidental printed chat editor science fiction magazine called title selected much thought theory field easy remember tentative title morning remember cup coffee discarded great deal thought effort gone formation magazine aid several generous people grateful much due assistance bulk work done try maintain one finest books market great public demand magazine short buy cannot honesty say publish times best science fiction field would true access best stories get fair share works best writers definitely talk adult juvenile relative content feel terms would rather think times terms story greatest literature ever written treasure island in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBtVoODKVPXb"
      },
      "source": [
        "# list of unique words from the scifi txt\n",
        "scifi_word_list=scifi_txt.split()\n",
        "scifi_vocabulary = sorted(set(scifi_word_list))\n",
        "scifi_vocabulary_size=len(scifi_vocabulary)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCmsMOjQaVFd",
        "outputId": "4c215cae-5516-420d-e0aa-70ffc28aa574",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"The length of scifi_word_list without cleaning infrequent words:{}\".format(len(scifi_word_list)))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of scifi_word_list without cleaning infrequent words:6495063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiMugJGcQ-Xz"
      },
      "source": [
        "word2index_scifi = {w:i for i,w in enumerate(scifi_vocabulary)} # Lookup table mapping words to indices\n",
        "index2word_scifi = {i:w for i,w in enumerate(scifi_vocabulary)} # Lookup table mapping indices to words"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihpxw4ADQ-X0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cde2bde8-c678-4452-ec7d-3a684546b532"
      },
      "source": [
        "scifi_data = []\n",
        "for i in range(CONTEXT_OFFSET, len(scifi_word_list) - CONTEXT_OFFSET): # To prevent the colab ram from crashing, we chose the first 5000000 words for trainning\n",
        "    context = [scifi_word_list[i - 2], scifi_word_list[i - 1],\n",
        "              scifi_word_list[i + 1], scifi_word_list[i + 2]]\n",
        "    target = scifi_word_list[i]\n",
        "    scifi_data.append((context, target))\n",
        "print(scifi_data[:5])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['march', 'stories', 'complete', 'publisher'], 'new'), (['stories', 'new', 'publisher', 'editor'], 'complete'), (['new', 'complete', 'editor', 'published'], 'publisher'), (['complete', 'publisher', 'published', 'monthly'], 'editor'), (['publisher', 'editor', 'monthly', 'quinn'], 'published')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yla0GZr5Q-X0"
      },
      "source": [
        "X = np.array([i[0] for i in scifi_data])\n",
        "X_vectors = list(map(lambda elem: make_context_vector(elem, word2index_scifi) , X))\n",
        "y = np.array([i[1] for i in scifi_data])\n",
        "y_vectors = list(map(lambda elem: make_context_vector([elem], word2index_scifi), y))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMPNVlD1Q-X1"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_vectors, y_vectors, test_size=0.2, random_state=42)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o_8RGmkQ-X1"
      },
      "source": [
        "scifi_training_dataset = HotelReviewsDataset(X_train, y_train)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkQx-wKOQ-X2"
      },
      "source": [
        "### 3. Modelling (Sci-Fi)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQdR_2MmoTCY"
      },
      "source": [
        "scifi_model_path = './scifi_model_weights.pth'"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAtAVyqvQ-X2"
      },
      "source": [
        "scifi_data_loader = DataLoader(dataset=scifi_training_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "model_scifi = CBOW(scifi_vocabulary_size, EMBEDDING_DIM_SCIFI, 2*CONTEXT_OFFSET).to(device)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyMzhEQom5zL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e6129ff-d2ae-4557-f866-723820cf4e21"
      },
      "source": [
        "# try:\n",
        "#   model_scifi.load_state_dict(torch.load(scifi_model_path))\n",
        "#   model.eval()\n",
        "# except Exception as e:\n",
        "print(\"No saved embeddings exist.\")\n",
        "print(\"Starting to learn word embeddings.\")\n",
        "losses = train_model(model_scifi, scifi_data_loader, EPOCHS_SCIFI, word2index_scifi)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No saved embeddings exist.\n",
            "Starting to learn word embeddings.\n",
            "Epoch 1/5 ... Average loss 0.12501986855710195\n",
            "Epoch 2/5 ... Average loss 0.12329275439496001\n",
            "Epoch 3/5 ... Average loss 0.12303136416265682\n",
            "Epoch 4/5 ... Average loss 0.12293282898662417\n",
            "Epoch 5/5 ... Average loss 0.12287961013402021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i0k9drfoMzu"
      },
      "source": [
        "# Save the trained CBOW model\n",
        "torch.save(model.state_dict(), scifi_model_path)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaGUoP7D9b28"
      },
      "source": [
        "# Part 2: Test your embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvwdYFgdL9nq"
      },
      "source": [
        "## 2. find 5 neighbours of each of the 9 words from the hotel reviews dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXR8AUQSCcGV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0921ef98-a144-4923-d3a5-3f9af40697f2"
      },
      "source": [
        "# check the frequencies of the words\n",
        "reviews_word_list=list((\" \".join(reviews)).split())\n",
        "frequency = pd.value_counts(reviews_word_list)\n",
        "print(\"Most frequent words are\\n{}\\n------------------------------\".format(frequency.head(20)))\n",
        "print(\"Medium frequent words are\\n{}\\n------------------------------\".format(frequency.iloc[500:520]))\n",
        "print(\"Less frequent words are\\n{}\\n------------------------------\".format(frequency.iloc[1000:1020]))\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent words are\n",
            "hotel        49877\n",
            "room         35357\n",
            "great        21482\n",
            "good         17418\n",
            "staff        16637\n",
            "stay         15413\n",
            "nice         12646\n",
            "rooms        12407\n",
            "location     11353\n",
            "stayed       10500\n",
            "service      10373\n",
            "night        10164\n",
            "time         10132\n",
            "beach        10068\n",
            "day           9979\n",
            "breakfast     9737\n",
            "clean         9599\n",
            "food          9425\n",
            "like          8254\n",
            "resort        8152\n",
            "dtype: int64\n",
            "------------------------------\n",
            "Medium frequent words are\n",
            "daughter      705\n",
            "received      704\n",
            "issue         698\n",
            "turn          697\n",
            "directly      697\n",
            "watch         695\n",
            "makes         694\n",
            "adequate      694\n",
            "surprised     693\n",
            "royal         689\n",
            "true          688\n",
            "elevator      688\n",
            "break         688\n",
            "cab           687\n",
            "bavaro        686\n",
            "complaints    684\n",
            "quickly       684\n",
            "basic         684\n",
            "recently      684\n",
            "smoking       683\n",
            "dtype: int64\n",
            "------------------------------\n",
            "Less frequent words are\n",
            "range         342\n",
            "italy         341\n",
            "gone          341\n",
            "added         341\n",
            "taxis         341\n",
            "cafes         341\n",
            "month         340\n",
            "filled        340\n",
            "comment       340\n",
            "apart         340\n",
            "lines         340\n",
            "fee           339\n",
            "additional    339\n",
            "aware         338\n",
            "heat          338\n",
            "base          336\n",
            "lift          336\n",
            "members       336\n",
            "sister        335\n",
            "wow           335\n",
            "dtype: int64\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh_v4l5lGJ5o"
      },
      "source": [
        "# We chose 3 nouns, 3 verbs, and 3 adjectives respectively from the above 3 frequency levels.\n",
        "chosen_words = ['hotel','great', 'clean', # From most frequent words\n",
        "                'issue','adequate','smoking', # From medium frequent words\n",
        "                'italy','filled','comment'] # From least frequent words"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sur7tef3M4hW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24b73943-c1df-4d2a-fc61-a35cbad29e3c"
      },
      "source": [
        "def get_closest_word(word, topn):\n",
        "  word_distance = []\n",
        "  emb = model.embeddings\n",
        "  pdist = nn.PairwiseDistance()\n",
        "  i = word2index[word]\n",
        "  lookup_tensor_i = torch.tensor([i],dtype=torch.long).to(device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "  v_i = emb(lookup_tensor_i)\n",
        "  for j in range(len(reviews_vocabulary)):\n",
        "    if j !=i:\n",
        "      lookup_tensor_j = torch.tensor([j],dtype=torch.long).to(device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "      v_j = emb(lookup_tensor_j)\n",
        "      word_distance.append((index2word[j],float(pdist(v_i,v_j))))\n",
        "  word_distance.sort(key=lambda x:x[1])\n",
        "  return word_distance[:topn]\n",
        "\n",
        "example = get_closest_word('beach', 5)\n",
        "print(example)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('fussed', 6.01870059967041), ('barong', 6.195380210876465), ('dealer', 6.453127384185791), ('messenger', 6.489078521728516), ('grounds', 6.497330665588379)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SKL5FNpM4fK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4c3ff5d-9af4-4e02-ad00-c141431f7a76"
      },
      "source": [
        "def get_closest_word_from_a_list(chosen_words,  topn):\n",
        "  chosen_words_and_their_neighbours=[]\n",
        "  for word in chosen_words:\n",
        "    get_result = get_closest_word(word,  topn)\n",
        "    neighbours = [nb[0] for nb in get_result]\n",
        "    chosen_words_and_their_neighbours.append((neighbours,word))\n",
        "  return chosen_words_and_their_neighbours\n",
        "\n",
        "neighbours = get_closest_word_from_a_list(chosen_words,5)\n",
        "neighbours"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['tuilieres', 'overload', 'bam', 'asun', 'visiting'], 'hotel'),\n",
              " (['fantastic', 'excellent', 'superb', 'best', 'palmetto'], 'great'),\n",
              " (['beds', 'kalverstraat', 'bathrooms', 'victorian', 'spacious'], 'clean'),\n",
              " (['amzing', 'hawai', 'talk', 'greasy', 'like'], 'issue'),\n",
              " (['small', 'tiananmen', 'single', 'expecting', 'wood'], 'adequate'),\n",
              " (['suitehotel', 'scenarios', 'legacy', 'caretakers', 'pillow'], 'smoking'),\n",
              " (['hotel', 'embargo', 'declare', 'crazy', 'hairstylist'], 'italy'),\n",
              " (['stupid', 'hanson', 'fra', 'ankle', 'caren'], 'filled'),\n",
              " (['hotel', 'bedding', 'intensive', 'walls', 'definitely'], 'comment')]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQAjrX1BNKuv"
      },
      "source": [
        "## 3. find 5 neighbours of each of the 9 words from the scifi dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-pFBNeQNKuw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07db7e22-3977-4abd-86c7-0f57bde865ae"
      },
      "source": [
        "# check the frequencies of the words\n",
        "frequency = pd.value_counts(scifi_word_list)\n",
        "print(\"The most frequent words are\\n{}\\n------------------------------\".format(frequency.head(20)))\n",
        "print(\"The less frequent words are\\n{}\\n------------------------------\".format(frequency.iloc[500:520]))\n",
        "print(\"The much less frequent words are\\n{}\\n------------------------------\".format(frequency.iloc[800:820]))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most frequent words are\n",
            "said      76385\n",
            "one       57263\n",
            "would     46663\n",
            "could     41425\n",
            "like      36472\n",
            "time      32907\n",
            "back      32185\n",
            "man       30097\n",
            "know      28632\n",
            "get       24516\n",
            "two       21847\n",
            "see       21211\n",
            "way       21081\n",
            "even      20510\n",
            "right     19564\n",
            "first     19159\n",
            "well      18729\n",
            "got       17908\n",
            "little    17267\n",
            "think     17003\n",
            "dtype: int64\n",
            "------------------------------\n",
            "The less frequent words are\n",
            "pointed      2320\n",
            "shot         2318\n",
            "laughed      2314\n",
            "happen       2312\n",
            "lips         2306\n",
            "paper        2294\n",
            "alive        2287\n",
            "shall        2284\n",
            "although     2282\n",
            "attention    2280\n",
            "ships        2278\n",
            "area         2278\n",
            "died         2275\n",
            "position     2271\n",
            "stuff        2249\n",
            "reach        2248\n",
            "broke        2245\n",
            "dear         2244\n",
            "speak        2240\n",
            "answered     2239\n",
            "dtype: int64\n",
            "------------------------------\n",
            "The much less frequent words are\n",
            "chest         1514\n",
            "aside         1509\n",
            "ears          1506\n",
            "possibly      1506\n",
            "indeed        1505\n",
            "steve         1504\n",
            "spread        1503\n",
            "forced        1503\n",
            "venus         1500\n",
            "ancient       1499\n",
            "threw         1498\n",
            "weight        1497\n",
            "class         1493\n",
            "mark          1490\n",
            "expression    1488\n",
            "seeing        1488\n",
            "creatures     1488\n",
            "final         1487\n",
            "growing       1485\n",
            "telling       1482\n",
            "dtype: int64\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFytFPDbNKuw"
      },
      "source": [
        "# We chose 3 nouns, 3 verbs, and 3 adjectives respectively from the above 3 frequency levels.\n",
        "chosen_words_scifi = ['time','think','right',\n",
        "                      'blood','smile','tiny',\n",
        "                      'party','worry','warm']"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-4MXoB1NKux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c10fe9-cc8b-4b18-93d4-3ccfdcb02559"
      },
      "source": [
        "def get_closest_word_scifi(word, topn):\n",
        "  word_distance = []\n",
        "  emb = model_scifi.embeddings\n",
        "  pdist = nn.PairwiseDistance()\n",
        "  i = word2index_scifi[word]\n",
        "  lookup_tensor_i = torch.tensor([i],dtype=torch.long).to(device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "  v_i = emb(lookup_tensor_i)\n",
        "  for j in range(len(scifi_vocabulary)):\n",
        "    if j !=i:\n",
        "      lookup_tensor_j = torch.tensor([j],dtype=torch.long).to(device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "      v_j = emb(lookup_tensor_j)\n",
        "      word_distance.append((index2word_scifi[j],float(pdist(v_i,v_j))))\n",
        "  word_distance.sort(key=lambda x:x[1])\n",
        "  return word_distance[:topn]\n",
        "\n",
        "def get_closest_word_from_a_list_scifi(chosen_words, topn):\n",
        "  chosen_words_and_their_neighbours=[]\n",
        "  for word in chosen_words:\n",
        "    get_result = get_closest_word_scifi(word, topn)\n",
        "    neighbours = [nb[0] for nb in get_result]\n",
        "    chosen_words_and_their_neighbours.append((neighbours,word))\n",
        "  return chosen_words_and_their_neighbours\n",
        "\n",
        "\n",
        "neighbours = get_closest_word_from_a_list_scifi(chosen_words_scifi, 5)\n",
        "neighbours"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['said', 'first', 'thought', 'made', 'like'], 'time'),\n",
              " (['know', 'tell', 'enough', 'want', 'sure'], 'think'),\n",
              " (['said', 'man', 'made', 'course', 'little'], 'right'),\n",
              " (['single', 'small', 'back', 'little', 'light'], 'blood'),\n",
              " (['power', 'ship', 'back', 'right', 'said'], 'smile'),\n",
              " (['small', 'dark', 'turned', 'came', 'bright'], 'tiny'),\n",
              " (['course', 'way', 'point', 'man', 'men'], 'party'),\n",
              " (['seems', 'however', 'right', 'girl', 'must'], 'worry'),\n",
              " (['watched', 'snapped', 'went', 'saw', 'looking'], 'warm')]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbA6fBZaa6Iv"
      },
      "source": [
        "## 5. Choose two words and retrive their 5 closest neighbours from both datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EcLEX5zbHB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd007e22-124f-4973-fd83-83b3dde48706"
      },
      "source": [
        "chosen_words=['good','job']\n",
        "neighbours_from_reviews = get_closest_word_from_a_list(chosen_words, 5)\n",
        "neighbours_from_scifi = get_closest_word_from_a_list_scifi(chosen_words, 5)\n",
        "\n",
        "\n",
        "print(\"5 closest neighbours of the chosen words in hotel reviews dataset:\")\n",
        "print(neighbours_from_reviews)\n",
        "print(\"5 closest neighbours of the chosen words in scifi dataset:\")\n",
        "print(neighbours_from_scifi)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 closest neighbours of the chosen words in hotel reviews dataset:\n",
            "[(['amandari', 'blooming', 'best', 'great', 'decent'], 'good'), (['nameless', 'experience', 'precinct', 'bouncy', 'whopping'], 'job')]\n",
            "5 closest neighbours of the chosen words in scifi dataset:\n",
            "[(['time', 'said', 'knew', 'like', 'first'], 'good'), (['even', 'work', 'point', 'time', 'though'], 'job')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiHz2oAFOHo1"
      },
      "source": [
        ""
      ],
      "execution_count": 58,
      "outputs": []
    }
  ]
}