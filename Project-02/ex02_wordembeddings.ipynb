{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "ex02_wordembeddings_hongjie.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jan-kreischer/UZH_ML4NLP/blob/main/ex02_wordembeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFNNOVmLD-Fz"
      },
      "source": [
        "# Project 2 - Word Embeddings with PyTorch\n",
        "\n",
        "The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep\n",
        "learning. It is a model that tries to predict words given the context of\n",
        "a few words before and a few words after the target word. This is\n",
        "distinct from language modeling, since CBOW is not sequential and does\n",
        "not have to be probabilistic. Typcially, CBOW is used to quickly train\n",
        "word embeddings, and these embeddings are used to initialize the\n",
        "embeddings of some more complicated model. Usually, this is referred to\n",
        "as *pretraining embeddings*. It almost always helps performance a couple\n",
        "of percent.\n",
        "\n",
        "The CBOW model is as follows. Given a target word $w_i$ and an\n",
        "$N$ context window on each side, $w_{i-1}, \\dots, w_{i-N}$\n",
        "and $w_{i+1}, \\dots, w_{i+N}$, referring to all context words\n",
        "collectively as $C$, CBOW tries to minimize\n",
        "\n",
        "\\begin{align}-\\log p(w_i | C) = -\\log \\text{Softmax}(A(\\sum_{w \\in C} q_w) + b)\\end{align}\n",
        "\n",
        "where $q_w$ is the embedding for word $w$.\n",
        "\n",
        "Implement this model in Pytorch by filling in the class below. Some\n",
        "tips:\n",
        "\n",
        "* Think about which parameters you need to define.\n",
        "* Make sure you know what shape each operation expects. Use .view() if you need to\n",
        "  reshape.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHXvfNTXD4Ca"
      },
      "source": [
        "## Part 1: Training CBOW embeddings for both datasets\n",
        "### 1. Setup\n",
        "#### 1.1 Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EATjb-X-0lM",
        "outputId": "f55ffd16-36f4-4842-9e3f-d06a18b7378e"
      },
      "source": [
        "# Loading all required external modules\n",
        "import os\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# numpy and pandas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('max_colwidth', 800)\n",
        "\n",
        "# tokenization\n",
        "import nltk;\n",
        "nltk.download('stopwords');\n",
        "%matplotlib inline\n",
        "\n",
        "from argparse import Namespace\n",
        "from collections import Counter\n",
        "import json\n",
        "import string\n",
        "import itertools\n",
        "import regex as re\n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.model_selection import train_test_split\n",
        "import requests"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKjgo1cEEQPw"
      },
      "source": [
        "#### 1.2 Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9Tszg7FdPrZ",
        "outputId": "ecbd1ccd-fb86-472f-8e1f-3a1feab9c790"
      },
      "source": [
        "# Check if device supports CUDA interface\n",
        "CUDA = torch.cuda.is_available()\n",
        "# Make program run on gpu (cuda:0) if available\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu:0')\n",
        "torch.cuda.set_device(device)\n",
        "print('Using device:', device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4ZgunZ9qSvu",
        "outputId": "b8933513-59a9-4168-f0e5-4466abe0a5b0"
      },
      "source": [
        "# Check and print information about available GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct 26 17:57:02 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keYYVJLbnpbD",
        "outputId": "cc81fe03-7dff-4e45-e0fb-3a6aca2d5a6b"
      },
      "source": [
        "# Get GPU name\n",
        "!nvidia-smi -L"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-33ed3149-e1cb-65ff-04b8-d1ed1ce43f25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGFVru-nqX1n",
        "outputId": "27a59005-d13d-4c02-ac8b-66328123bec0"
      },
      "source": [
        "# Check Memory\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW7vvEZkEdrX"
      },
      "source": [
        "#### 1.3 Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grG2YIi8-aYu"
      },
      "source": [
        "# Define necessary constants\n",
        "# which were given to us\n",
        "# and dont change throughout the execution\n",
        "FEATURE_COLUMN = 'Review'\n",
        "CONTEXT_OFFSET = 2 # n words to the left, n to the right\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Since overfitting is not an issue according\n",
        "# to Claudio, we decided to train for more episodes\n",
        "EPOCHS_HOTEL = 30\n",
        "EMBEDDING_DIM_HOTEL = 50\n",
        "\n",
        "# We decided to stay with 2 eposides here\n",
        "# because the corpus is very big.\n",
        "EPOCHS_SCIFI = 2\n",
        "EMBEDDING_DIM_SCIFI = 50"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Gy3d3hJQZB9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAqmKotTv5FX"
      },
      "source": [
        "### Hotel Reviews dataset\n",
        "### 2. Data Preprocessing\n",
        "#### 2.1 Data Acquisition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSA-k-FGGYVx",
        "outputId": "f0d6213a-0421-44de-b14e-74cb30041481"
      },
      "source": [
        "# Loading the tripadvisor data\n",
        "url_tripadvisor = (r'https://raw.githubusercontent.com/abandonedrepo/test/master/tripadvisor_hotel_reviews.csv')\n",
        "reviews_dataset = pd.read_csv(url_tripadvisor)\n",
        "reviews_dataset.info()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20491 entries, 0 to 20490\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Review  20491 non-null  object\n",
            " 1   Rating  20491 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 320.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "MpAY67nPvFw9",
        "outputId": "7a2d7e74-27aa-4c28-d1a8-e915e435667c"
      },
      "source": [
        "reviews_dataset.head(5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>nice hotel expensive parking got good deal stay hotel anniversary, arrived late evening took advice previous reviews did valet parking, check quick easy, little disappointed non-existent view room room clean nice size, bed comfortable woke stiff neck high pillows, not soundproof like heard music room night morning loud bangs doors opening closing hear people talking hallway, maybe just noisy neighbors, aveda bath products nice, did not goldfish stay nice touch taken advantage staying longer, location great walking distance shopping, overall nice experience having pay 40 parking night,</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ok nothing special charge diamond member hilton decided chain shot 20th anniversary seattle, start booked suite paid extra website description not, suite bedroom bathroom standard hotel room, took printed reservation desk showed said things like tv couch ect desk clerk told oh mixed suites description kimpton website sorry free breakfast, got kidding, embassy suits sitting room bathroom bedroom unlike kimpton calls suite, 5 day stay offer correct false advertising, send kimpton preferred guest website email asking failure provide suite advertised website reservation description furnished hard copy reservation printout website desk manager duty did not reply solution, send email trip guest survey did not follow email mail, guess tell concerned guest.the staff ranged indifferent not help...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>nice rooms not 4* experience hotel monaco seattle good hotel n't 4* level.positives large bathroom mediterranean suite comfortable bed pillowsattentive housekeeping staffnegatives ac unit malfunctioned stay desk disorganized, missed 3 separate wakeup calls, concierge busy hard touch, did n't provide guidance special requests.tv hard use ipod sound dock suite non functioning. decided book mediterranean suite 3 night weekend stay 1st choice rest party filled, comparison w spent 45 night larger square footage room great soaking tub whirlpool jets nice shower.before stay hotel arrange car service price 53 tip reasonable driver waiting arrival.checkin easy downside room picked 2 person jacuzi tub no bath accessories salts bubble bath did n't stay, night got 12/1a checked voucher bottle cham...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>unique, great stay, wonderful time hotel monaco, location excellent short stroll main downtown shopping area, pet friendly room showed no signs animal hair smells, monaco suite sleeping area big striped curtains pulled closed nice touch felt cosy, goldfish named brandi enjoyed, did n't partake free wine coffee/tea service lobby thought great feature, great staff friendly, free wireless internet hotel worked suite 2 laptops, decor lovely eclectic mix pattens color palatte, animal print bathrobes feel like rock stars, nice did n't look like sterile chain hotel hotel personality excellent stay,</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>great stay great stay, went seahawk game awesome, downfall view building did n't complain, room huge staff helpful, booked hotels website seahawk package, no charge parking got voucher taxi, problem taxi driver did n't want accept voucher barely spoke english, funny thing speak arabic called started making comments girlfriend cell phone buddy, took second realize just said fact speak language face priceless, ass told, said large city, told head doorman issue called cab company promply answer did n't, apologized offered pay taxi, bucks 2 miles stadium, game plan taxi return going humpin, great walk did n't mind, right christmas wonderful lights, homeless stowed away building entrances leave, police presence not greatest area stadium, activities 7 blocks pike street waterfront great coff...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Review  Rating\n",
              "0                                                                                                                                                                                                                nice hotel expensive parking got good deal stay hotel anniversary, arrived late evening took advice previous reviews did valet parking, check quick easy, little disappointed non-existent view room room clean nice size, bed comfortable woke stiff neck high pillows, not soundproof like heard music room night morning loud bangs doors opening closing hear people talking hallway, maybe just noisy neighbors, aveda bath products nice, did not goldfish stay nice touch taken advantage staying longer, location great walking distance shopping, overall nice experience having pay 40 parking night,         4\n",
              "1  ok nothing special charge diamond member hilton decided chain shot 20th anniversary seattle, start booked suite paid extra website description not, suite bedroom bathroom standard hotel room, took printed reservation desk showed said things like tv couch ect desk clerk told oh mixed suites description kimpton website sorry free breakfast, got kidding, embassy suits sitting room bathroom bedroom unlike kimpton calls suite, 5 day stay offer correct false advertising, send kimpton preferred guest website email asking failure provide suite advertised website reservation description furnished hard copy reservation printout website desk manager duty did not reply solution, send email trip guest survey did not follow email mail, guess tell concerned guest.the staff ranged indifferent not help...       2\n",
              "2  nice rooms not 4* experience hotel monaco seattle good hotel n't 4* level.positives large bathroom mediterranean suite comfortable bed pillowsattentive housekeeping staffnegatives ac unit malfunctioned stay desk disorganized, missed 3 separate wakeup calls, concierge busy hard touch, did n't provide guidance special requests.tv hard use ipod sound dock suite non functioning. decided book mediterranean suite 3 night weekend stay 1st choice rest party filled, comparison w spent 45 night larger square footage room great soaking tub whirlpool jets nice shower.before stay hotel arrange car service price 53 tip reasonable driver waiting arrival.checkin easy downside room picked 2 person jacuzi tub no bath accessories salts bubble bath did n't stay, night got 12/1a checked voucher bottle cham...       3\n",
              "3                                                                                                                                                                                                         unique, great stay, wonderful time hotel monaco, location excellent short stroll main downtown shopping area, pet friendly room showed no signs animal hair smells, monaco suite sleeping area big striped curtains pulled closed nice touch felt cosy, goldfish named brandi enjoyed, did n't partake free wine coffee/tea service lobby thought great feature, great staff friendly, free wireless internet hotel worked suite 2 laptops, decor lovely eclectic mix pattens color palatte, animal print bathrobes feel like rock stars, nice did n't look like sterile chain hotel hotel personality excellent stay,         5\n",
              "4  great stay great stay, went seahawk game awesome, downfall view building did n't complain, room huge staff helpful, booked hotels website seahawk package, no charge parking got voucher taxi, problem taxi driver did n't want accept voucher barely spoke english, funny thing speak arabic called started making comments girlfriend cell phone buddy, took second realize just said fact speak language face priceless, ass told, said large city, told head doorman issue called cab company promply answer did n't, apologized offered pay taxi, bucks 2 miles stadium, game plan taxi return going humpin, great walk did n't mind, right christmas wonderful lights, homeless stowed away building entrances leave, police presence not greatest area stadium, activities 7 blocks pike street waterfront great coff...       5"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jay7YAfne0wo"
      },
      "source": [
        "#### 2.2 Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0rNpT9gd4bp"
      },
      "source": [
        "wpt = nltk.WordPunctTokenizer()\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def clean_document(x):\n",
        "    x = re.sub(r'\\w*\\d\\w*', ' ', x) # Remove all words containing a number at any point e.g 20th\n",
        "    x = re.sub(r'[^a-zA-Z\\s]', ' ', x.lower(), re.I|re.A)\n",
        "    x = re.sub(r'[\\-!+_@*#\\/$:)\"\\'.;,?&({}[]]*', ' ', x) # Remove all punctuation\n",
        "    x = re.sub(r'\\b\\w{1,2}\\b', ' ', x) # Removed short words with a length of less than 3\n",
        "    x = re.sub(' +', ' ', x) # Substitute all multi spaces into single spaces.\n",
        "    tokens = wpt.tokenize(x) # Tokenize and remove stopwords\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    x = ' '.join(filtered_tokens)\n",
        "    return x\n",
        "\n",
        "clean_corpus = np.vectorize(clean_document) # Apply cleaning to every document in the corpus"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgtrtfqq-kwj"
      },
      "source": [
        "# Clean the reviews by removing punctuation characters and stopwords.\n",
        "reviews = clean_corpus(reviews_dataset['Review'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwSUTo0zz-q-",
        "outputId": "d88f881f-234c-4e34-fb8c-5a5a7e2fc5f1"
      },
      "source": [
        "np.random.choice(reviews, 5)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['great hotel great location hotel located rambla city centre close access shops tour buses room immaculate plasma screen screen bathroom nice blue ambient lighting room good amenities bathroom large equipped concierge staff overall friendly helpful recommended lot restaurants night spots visit absolutely wonderful business centre use free open hours thing disliked face room safe bit small fit laptop definitely recommend hotel',\n",
              "       'botel great looking reasonably priced hotel amsterdam location excellent minute walk central station mins dam square building work going botel day quiet night rooms basic clean small suite shower talked locals hotels amsterdam private facilities gives botel advantage breakfast pretty basic caters tastes road botel brilliant mexican restaurant called guadalupe worth visit handy staying botel',\n",
              "       'nice star hotel clean hotel helpful staff enjoyable sip glass wine outside patio grand canal booked room low end hotel price scale adequate said clean really important esp foreign country room probably bit larger std european room size small american standards overall room decent nothing home given high prices venice rooms competitively priced compared bit high',\n",
              "       'great location incredible price room stayed king bedroom right size bed comfortable slept bathroom bit small sink vanity got creative spreading stuff shower glass stand shower cool leaks door crack prepared mop slip air conditioning usually fine fairly warm day room got bit hot fan room cracked window night air circulating street noise coming hotel fan drown sound location awesome blocks chinatown union square best shopping nyc park ave opinion rented car whopping day car parked little street parking needed car travel wedding events wine country need car enjoy street great sushi restaurant open midnight yummy',\n",
              "       'wow wow great place location great tourist thing service incredible room wonderful beds comfortable loved little touches like complimentary wine tasting day definitely stay'],\n",
              "      dtype='<U12382')"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtL6IIv1HC4H"
      },
      "source": [
        "# Since we want to train a CBOW model with context width of 2\n",
        "# on the reviews, we drop all reviews with less than 5 words.\n",
        "# This is equivalent to only keeping instances with at least 5 words.\n",
        "reviews = [review for review in reviews if len(review.split(\" \")) >=  (2*CONTEXT_OFFSET + 1)]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjNI3ufc6nFa"
      },
      "source": [
        "# Remove infrequent words\n",
        "reviews_word_list=list((\" \".join(reviews)).split())\n",
        "\n",
        "frequency = pd.value_counts(reviews_word_list)\n",
        "infrequent_words = list(frequency[frequency <= 1].keys())\n",
        "frequent_words = list(frequency[frequency > 1].keys())\n",
        "\n",
        "# Create lookup table to check if word is infrequent (1) or not (0)\n",
        "is_infrequent = {}\n",
        "for infrequent_word in infrequent_words:\n",
        "  is_infrequent[infrequent_word] = 1\n",
        "\n",
        "for frequent_word in frequent_words:\n",
        "  is_infrequent[frequent_word] = 0"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B7bSUCDOWjk",
        "outputId": "a3d83e3f-9b65-4834-a124-f590c1e96e1f"
      },
      "source": [
        "print(\"The length of the frequent_words:{}\".format(len(frequent_words)))\n",
        "print(\"The length of the infrequent_words:{}\".format(len(infrequent_words)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of the frequent_words:24730\n",
            "The length of the infrequent_words:23833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxUXynX0M5iz"
      },
      "source": [
        "# In order to build the corpus for the reviews \n",
        "# we want to find every distinct word that occurs\n",
        "# in at least one review.\n",
        "# We join all reviews into one large string and then\n",
        "# split it at every space to receive a list of words\n",
        "# Then the set method is used in order to only\n",
        "# retain unique words.\n",
        "# This list is then alphabetically sorted\n",
        "review_words = \" \".join(reviews).split()\n",
        "review_words = [w for w in review_words if not is_infrequent[w]]\n",
        "reviews_vocabulary = sorted(set(review_words))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bw6FFmeNW5Or",
        "outputId": "9c7e90a6-2b28-408b-9fc0-fe27655bc7fe"
      },
      "source": [
        "reviews_vocabulary_size = len(reviews_vocabulary)\n",
        "print(\"The reviews use a vocabulary comprising {} different words.\".format(reviews_vocabulary_size))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The reviews use a vocabulary comprising 24730 different words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErFC1k2kNpCE"
      },
      "source": [
        "word2index = {w:i for i,w in enumerate(reviews_vocabulary)} # Lookup table mapping words to indices\n",
        "index2word = {i:w for i,w in enumerate(reviews_vocabulary)} # Lookup table mapping indices to words"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQYOKdbna5aC"
      },
      "source": [
        "# Removed infrequent words from our corpus\n",
        "def clean_infrequent_words(reviews):\n",
        "  new_reviews=[]\n",
        "  for review in reviews:\n",
        "    infrequent_index=[]\n",
        "    raw_text = review.split()\n",
        "    for i in range(0,len(raw_text)):\n",
        "      if is_infrequent[raw_text[i]]==1:\n",
        "        infrequent_index.append(i)\n",
        "    for i in sorted(infrequent_index,reverse=True):\n",
        "      del raw_text[i]\n",
        "    review=' '.join(raw_text)\n",
        "    new_reviews.append(review)\n",
        "  return new_reviews\n",
        "\n",
        "reviews=clean_infrequent_words(reviews)\n",
        "# drop review with less than 5 words again\n",
        "reviews = [review for review in reviews if len(review.split(\" \")) >=  (2*CONTEXT_OFFSET + 1)]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBwNZhIKcqAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c7170b8-4a55-434e-83ab-12361dfbb645"
      },
      "source": [
        "# Create the context => center word tuples\n",
        "# from the documents\n",
        "data = []\n",
        "for review in reviews:\n",
        "  raw_text = review.split()\n",
        "  for i in range(CONTEXT_OFFSET, len(raw_text) - CONTEXT_OFFSET):\n",
        "      context = [raw_text[i - 2], raw_text[i - 1],\n",
        "                raw_text[i + 1], raw_text[i + 2]]\n",
        "      #print(context)\n",
        "      target = raw_text[i]\n",
        "      data.append((context, target))\n",
        "\n",
        "# Show some sample 'context -> center word' mappings\n",
        "for i in range(5):\n",
        "  print(data[i])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['nice', 'hotel', 'parking', 'got'], 'expensive')\n",
            "(['hotel', 'expensive', 'got', 'good'], 'parking')\n",
            "(['expensive', 'parking', 'good', 'deal'], 'got')\n",
            "(['parking', 'got', 'deal', 'stay'], 'good')\n",
            "(['got', 'good', 'stay', 'hotel'], 'deal')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqPIILq3f9er",
        "outputId": "e83ceceb-3fe8-42e2-afca-d98d3e2c04a5"
      },
      "source": [
        "# The following function transforms the context\n",
        "# into index notation\n",
        "def make_context_vector(context, word2index):\n",
        "    idxs = [word2index[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "# Show one transformed sample context\n",
        "make_context_vector(data[0][0], word2index)  # example"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([14585, 10605, 15681,  9535])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "069rMbD_Vju4"
      },
      "source": [
        "# Dataset containing the hotel reviews\n",
        "# Complies with pytorchs Dataset interface\n",
        "class WordContextsDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.y[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKSnkqC7aPGV"
      },
      "source": [
        "X = np.array([i[0] for i in data])\n",
        "X_vectors = list(map(lambda elem: make_context_vector(elem, word2index) , X))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElJk2mFlJG6Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e57c6c-3bd8-4423-813e-b3f091c46e05"
      },
      "source": [
        "# Print some vetorized sample contexts\n",
        "for i in range(5):\n",
        "  print(X_vectors[i])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([14585, 10605, 15681,  9535])\n",
            "tensor([10605,  7848,  9535,  9494])\n",
            "tensor([ 7848, 15681,  9494,  5718])\n",
            "tensor([15681,  9535,  5718, 20853])\n",
            "tensor([ 9535,  9494, 20853, 10605])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIrePOCkaQrB"
      },
      "source": [
        "y = np.array([i[1] for i in data])\n",
        "y_vectors = list(map(lambda elem: make_context_vector([elem], word2index), y))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFP9_whfKkxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "669ec065-7568-46b9-8416-82f7edde7877"
      },
      "source": [
        "# Print some vectorized sample center words\n",
        "for i in range(5):\n",
        "  print(y_vectors[i])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([7848])\n",
            "tensor([15681])\n",
            "tensor([9535])\n",
            "tensor([9494])\n",
            "tensor([5718])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wFmcwnO9Dq7"
      },
      "source": [
        "# Split into training and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectors, y_vectors, test_size=0.2, random_state=42, shuffle=True)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWkaD9JtaT9t"
      },
      "source": [
        "# Create the training dataset from vectors\n",
        "hotel_reviews_dataset = WordContextsDataset(X_train, y_train)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OapsjmhTfHxw"
      },
      "source": [
        "### 3. Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RvGC_MGH6k0"
      },
      "source": [
        "hotel_reviews_loader = DataLoader(dataset=hotel_reviews_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3_xQQ5Mw3JQ"
      },
      "source": [
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "      super(CBOW, self).__init__()\n",
        "      self.embeddings = nn.Embedding(vocab_size, embedding_dim, device=device)\n",
        "      self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "      self.activation_function1 = nn.ReLU()\n",
        "      self.linear2 = nn.Linear(128, vocab_size)\n",
        "      self.activation_function2 = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "      embeds = self.embeddings(inputs).view(inputs.size(0), -1)\n",
        "      out = self.linear1(embeds)\n",
        "      out = self.activation_function1(out)\n",
        "      out = self.linear2(out)\n",
        "      out = self.activation_function2(out)\n",
        "      return out"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB1FCwfH4miH"
      },
      "source": [
        "def train_model(model, data_loader, epochs, word2index):\n",
        "\n",
        "  losses = np.zeros(epochs)\n",
        "  loss_function = nn.NLLLoss()\n",
        "  optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for step, (context_vectors, target_vector) in enumerate(data_loader):\n",
        "\n",
        "      context_vectors = context_vectors.to(device) # Move the batch of context vectors into GPU memory\n",
        "      target_vector = target_vector.to(device) # Move the batch of target vectors into GPU memory \n",
        "\n",
        "      model.zero_grad() # Reset all gradients back to zero\n",
        "\n",
        "      log_probs = model(context_vectors) # forward pass\n",
        "      loss = loss_function(log_probs, torch.squeeze(target_vector)) # compute loss for batch\n",
        "      losses[epoch] += loss.item() # accumulate loss\n",
        "\n",
        "      loss.backward() # backpropagation\n",
        "      optimizer.step() # update the model weights\n",
        "\n",
        "    print(\"Epoch {0}/{1} ... Average loss {2}\".format(epoch+1, epochs, losses[epoch] / len(data_loader.dataset))) # Print average loss in this episode\n",
        "  return losses"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEuw6rg5zs-o"
      },
      "source": [
        "model = CBOW(reviews_vocabulary_size, EMBEDDING_DIM_HOTEL, 2*CONTEXT_OFFSET).to(device)\n",
        "model_path='./hotel_reviews_model_weights.pth'"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBzHbZ2R4_Au",
        "outputId": "24fe5248-6368-4d6b-c1ec-cbb04ee9eecc"
      },
      "source": [
        "# Train the CBOW model if no saved embeddings exist yet\n",
        "# Otherwise load the already trained embeddings\n",
        "\n",
        "# Since overfitting is not an issue according\n",
        "# to Claudio, we decided to train for more episodes\n",
        "try:\n",
        "  model.load_state_dict(torch.load(model_path))\n",
        "  model.eval()\n",
        "except Exception as e:\n",
        "  print(\"No saved embeddings exist.\")\n",
        "  print(\"Starting to learn word embeddings.\")\n",
        "  losses = train_model(model, hotel_reviews_loader, EPOCHS_HOTEL, word2index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No saved embeddings exist.\n",
            "Starting to learn word embeddings.\n",
            "Epoch 1/30 ... Average loss 0.1199288639485836\n",
            "Epoch 2/30 ... Average loss 0.1108083882677555\n",
            "Epoch 3/30 ... Average loss 0.10474874131917954\n",
            "Epoch 4/30 ... Average loss 0.10001808035969734\n",
            "Epoch 5/30 ... Average loss 0.09631988632798195\n",
            "Epoch 6/30 ... Average loss 0.0932952623140812\n",
            "Epoch 7/30 ... Average loss 0.0908410360121727\n",
            "Epoch 8/30 ... Average loss 0.08877986954450608\n",
            "Epoch 9/30 ... Average loss 0.08704138385176659\n",
            "Epoch 10/30 ... Average loss 0.08551703315019607\n",
            "Epoch 11/30 ... Average loss 0.08420399794816971\n",
            "Epoch 12/30 ... Average loss 0.08305943539381028\n",
            "Epoch 13/30 ... Average loss 0.08209802722454071\n",
            "Epoch 14/30 ... Average loss 0.08122771304488183\n",
            "Epoch 15/30 ... Average loss 0.08048059151530265\n",
            "Epoch 16/30 ... Average loss 0.07980409189999103\n",
            "Epoch 17/30 ... Average loss 0.07923497488558293\n",
            "Epoch 18/30 ... Average loss 0.07865650351166725\n",
            "Epoch 19/30 ... Average loss 0.07818474381029605\n",
            "Epoch 20/30 ... Average loss 0.07771708149373531\n",
            "Epoch 21/30 ... Average loss 0.07729950921714306\n",
            "Epoch 22/30 ... Average loss 0.07693665847539902\n",
            "Epoch 23/30 ... Average loss 0.0765779773414135\n",
            "Epoch 24/30 ... Average loss 0.07626300867378712\n",
            "Epoch 25/30 ... Average loss 0.0759441754835844\n",
            "Epoch 26/30 ... Average loss 0.0756610481864214\n",
            "Epoch 27/30 ... Average loss 0.07540792542278767\n",
            "Epoch 28/30 ... Average loss 0.07517929400682449\n",
            "Epoch 29/30 ... Average loss 0.07494981319069863\n",
            "Epoch 30/30 ... Average loss 0.07471913980722428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in7fjn19zg8U"
      },
      "source": [
        "# Save the trained CBOW model\n",
        "torch.save(model.state_dict(), model_path)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI4F2RhxQsli"
      },
      "source": [
        "### Sci-Fi story dataset\n",
        "### 2. Data Preprocessing\n",
        "#### 2.1 Data Acquisition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyhZrrzEM4oK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9896d6b6-f4d3-4f2c-eed4-a2d3a97d4e85"
      },
      "source": [
        "# Loading the scifi txt\n",
        "url = 'https://raw.githubusercontent.com/abandonedrepo/test/master/scifi.txt'\n",
        "scifi_dataset = requests.get(url).text\n",
        "print(scifi_dataset[:100])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MARCH # All Stories New and Complete Publisher Editor IF is published bi-monthly by Quinn Publishing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sLOZKW_Q-Xx"
      },
      "source": [
        "#### 2.2 Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JbPCiNSQ-Xy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ede8a2a0-0671-4f14-9dac-4cb337ea85e9"
      },
      "source": [
        "# Clean the scifi text by removing punctuation and stop words\n",
        "scifi_txt = clean_document(scifi_dataset)\n",
        "print(scifi_txt[:100])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "march stories new complete publisher editor published monthly quinn publishing company inc kingston \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RqYPvfAWjft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b742f7d2-1acd-4258-9bca-0f29c2e76055"
      },
      "source": [
        "# Split the scifi text into individual words\n",
        "scifi_word_list=scifi_txt.split()\n",
        "print(scifi_word_list[:10])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['march', 'stories', 'new', 'complete', 'publisher', 'editor', 'published', 'monthly', 'quinn', 'publishing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7enDbQkVZX7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b74345d3-f45a-48fb-e1f4-59ed4c6b5168"
      },
      "source": [
        "print(\"The size of the scifi vocabular before removing infrequent words: {}\".format(len(scifi_word_list)))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of the scifi vocabular before removing infrequent words: 7602971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmH_3Ac3TVcy"
      },
      "source": [
        "frequency = pd.value_counts(scifi_word_list)\n",
        "infrequent_words = list(frequency[frequency <= 100].keys())\n",
        "frequent_words = list(frequency[frequency > 100].keys())\n",
        "\n",
        "is_infrequent = {}\n",
        "for infrequent_word in infrequent_words:\n",
        "  is_infrequent[infrequent_word] = 1\n",
        "for frequent_word in frequent_words:\n",
        "  is_infrequent[frequent_word] = 0"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2jCyVPOTDgN",
        "outputId": "e0914ae4-c2d2-4249-f635-c4cd548fa5e5"
      },
      "source": [
        "#To clean the infrequnt words\n",
        "def clean_infrequent_words(scifi_txt):\n",
        "  scifi_new_list=[]\n",
        "  raw_text = scifi_txt.split()\n",
        "  for i in range(0,len(raw_text)):\n",
        "    a=raw_text[i]\n",
        "    if is_infrequent[a]==0:\n",
        "        scifi_new_list.append(a)\n",
        "  scifi_txt_new=' '.join(scifi_new_list)\n",
        "  return scifi_txt_new\n",
        "\n",
        "scifi_txt=clean_infrequent_words(scifi_txt)\n",
        "print(scifi_txt[:1000])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "march stories new complete publisher editor published monthly quinn publishing company inc new york volume copyright quinn publishing company inc application entry second class matter post office buffalo new york subscription issues possessions canada issues elsewhere four weeks change address stories appearing magazine fiction similarity actual persons coincidental printed chat editor science fiction magazine called title selected much thought theory field easy remember tentative title morning remember cup coffee discarded great deal thought effort gone formation magazine aid several generous people grateful much due assistance bulk work done try maintain one finest books market great public demand magazine short buy cannot honesty say publish times best science fiction field would true access best stories get fair share works best writers definitely talk adult juvenile relative content feel terms would rather think times terms story greatest literature ever written treasure island in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBtVoODKVPXb"
      },
      "source": [
        "# Create vocabulary list for the scifi text\n",
        "scifi_word_list=scifi_txt.split()\n",
        "scifi_vocabulary = sorted(set(scifi_word_list))\n",
        "scifi_vocabulary_size=len(scifi_vocabulary)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCmsMOjQaVFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a7defe-d06b-44b3-cf59-98208efee59c"
      },
      "source": [
        "print(\"The size of the scifi vocabular after removing infrequent words: {}\".format(len(scifi_word_list)))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of the scifi vocabular after removing infrequent words: 6495063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiMugJGcQ-Xz"
      },
      "source": [
        "word2index_scifi = {w:i for i,w in enumerate(scifi_vocabulary)} # Lookup table mapping words to indices\n",
        "index2word_scifi = {i:w for i,w in enumerate(scifi_vocabulary)} # Lookup table mapping indices to words"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihpxw4ADQ-X0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca5554f4-4493-4ce1-f479-58ab4617995c"
      },
      "source": [
        "scifi_data = []\n",
        "for i in range(CONTEXT_OFFSET, len(scifi_word_list) - CONTEXT_OFFSET): # To prevent the colab ram from crashing, we chose the first 5000000 words for trainning\n",
        "    context = [scifi_word_list[i - 2], scifi_word_list[i - 1],\n",
        "              scifi_word_list[i + 1], scifi_word_list[i + 2]]\n",
        "    target = scifi_word_list[i]\n",
        "    scifi_data.append((context, target))\n",
        "print(scifi_data[:5])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['march', 'stories', 'complete', 'publisher'], 'new'), (['stories', 'new', 'publisher', 'editor'], 'complete'), (['new', 'complete', 'editor', 'published'], 'publisher'), (['complete', 'publisher', 'published', 'monthly'], 'editor'), (['publisher', 'editor', 'monthly', 'quinn'], 'published')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yla0GZr5Q-X0"
      },
      "source": [
        "X = np.array([i[0] for i in scifi_data])\n",
        "X_vectors = list(map(lambda elem: make_context_vector(elem, word2index_scifi) , X))\n",
        "y = np.array([i[1] for i in scifi_data])\n",
        "y_vectors = list(map(lambda elem: make_context_vector([elem], word2index_scifi), y))"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMPNVlD1Q-X1"
      },
      "source": [
        "# Split into training and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectors, y_vectors, test_size=0.2, random_state=42)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o_8RGmkQ-X1"
      },
      "source": [
        "scifi_training_dataset = WordContextsDataset(X_train, y_train)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkQx-wKOQ-X2"
      },
      "source": [
        "### 3. Modelling (Sci-Fi)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQdR_2MmoTCY"
      },
      "source": [
        "scifi_model_path = './scifi_model_weights.pth'"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAtAVyqvQ-X2"
      },
      "source": [
        "scifi_data_loader = DataLoader(dataset=scifi_training_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "model_scifi = CBOW(scifi_vocabulary_size, EMBEDDING_DIM_SCIFI, 2*CONTEXT_OFFSET).to(device)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyMzhEQom5zL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7811ddc8-557d-4514-e7bd-32faf2f761cd"
      },
      "source": [
        "# Train the CBOW model if no saved embeddings exist yet\n",
        "# Otherwise load the already trained embeddings\n",
        "try:\n",
        "  model_scifi.load_state_dict(torch.load(scifi_model_path))\n",
        "  model_scifi.eval()\n",
        "except Exception as e:\n",
        "  print(\"No saved embeddings exist.\")\n",
        "  print(\"Starting to learn word embeddings.\")\n",
        "  losses = train_model(model_scifi, scifi_data_loader, EPOCHS_SCIFI, word2index_scifi)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No saved embeddings exist.\n",
            "Starting to learn word embeddings.\n",
            "Epoch 1/2 ... Average loss 0.12509822989747724\n",
            "Epoch 2/2 ... Average loss 0.1233596718698413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i0k9drfoMzu"
      },
      "source": [
        "# Save the trained CBOW model\n",
        "torch.save(model.state_dict(), scifi_model_path)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaGUoP7D9b28"
      },
      "source": [
        "# Part 2: Test your embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvwdYFgdL9nq"
      },
      "source": [
        "## 3. find the 5 closest words for 9 words from the hotel reviews dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXR8AUQSCcGV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0921ef98-a144-4923-d3a5-3f9af40697f2"
      },
      "source": [
        "# check the frequencies of the words\n",
        "reviews_word_list=list((\" \".join(reviews)).split())\n",
        "frequency = pd.value_counts(reviews_word_list)\n",
        "print(\"Most frequent words are\\n{}\\n------------------------------\".format(frequency.head(20)))\n",
        "print(\"Medium frequent words are\\n{}\\n------------------------------\".format(frequency.iloc[500:520]))\n",
        "print(\"Less frequent words are\\n{}\\n------------------------------\".format(frequency.iloc[1000:1020]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent words are\n",
            "hotel        49877\n",
            "room         35357\n",
            "great        21482\n",
            "good         17418\n",
            "staff        16637\n",
            "stay         15413\n",
            "nice         12646\n",
            "rooms        12407\n",
            "location     11353\n",
            "stayed       10500\n",
            "service      10373\n",
            "night        10164\n",
            "time         10132\n",
            "beach        10068\n",
            "day           9979\n",
            "breakfast     9737\n",
            "clean         9599\n",
            "food          9425\n",
            "like          8254\n",
            "resort        8152\n",
            "dtype: int64\n",
            "------------------------------\n",
            "Medium frequent words are\n",
            "daughter      705\n",
            "received      704\n",
            "issue         698\n",
            "turn          697\n",
            "directly      697\n",
            "watch         695\n",
            "makes         694\n",
            "adequate      694\n",
            "surprised     693\n",
            "royal         689\n",
            "true          688\n",
            "elevator      688\n",
            "break         688\n",
            "cab           687\n",
            "bavaro        686\n",
            "complaints    684\n",
            "quickly       684\n",
            "basic         684\n",
            "recently      684\n",
            "smoking       683\n",
            "dtype: int64\n",
            "------------------------------\n",
            "Less frequent words are\n",
            "range         342\n",
            "italy         341\n",
            "gone          341\n",
            "added         341\n",
            "taxis         341\n",
            "cafes         341\n",
            "month         340\n",
            "filled        340\n",
            "comment       340\n",
            "apart         340\n",
            "lines         340\n",
            "fee           339\n",
            "additional    339\n",
            "aware         338\n",
            "heat          338\n",
            "base          336\n",
            "lift          336\n",
            "members       336\n",
            "sister        335\n",
            "wow           335\n",
            "dtype: int64\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh_v4l5lGJ5o"
      },
      "source": [
        "# We chose 3 nouns, 3 verbs, and 3 adjectives respectively from the above 3 frequency levels.\n",
        "chosen_words = ['hotel','great', 'clean', # From most frequent words\n",
        "                'issue','adequate','smoking', # From medium frequent words\n",
        "                'italy','filled','comment'] # From least frequent words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sur7tef3M4hW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24b73943-c1df-4d2a-fc61-a35cbad29e3c"
      },
      "source": [
        "def get_closest_word(word, topn):\n",
        "  word_distance = []\n",
        "  emb = model.embeddings\n",
        "  pdist = nn.PairwiseDistance()\n",
        "  i = word2index[word]\n",
        "  lookup_tensor_i = torch.tensor([i],dtype=torch.long).to(device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "  v_i = emb(lookup_tensor_i)\n",
        "  for j in range(len(reviews_vocabulary)):\n",
        "    if j !=i:\n",
        "      lookup_tensor_j = torch.tensor([j],dtype=torch.long).to(device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "      v_j = emb(lookup_tensor_j)\n",
        "      word_distance.append((index2word[j],float(pdist(v_i,v_j))))\n",
        "  word_distance.sort(key=lambda x:x[1])\n",
        "  return word_distance[:topn]\n",
        "\n",
        "example = get_closest_word('beach', 5)\n",
        "print(example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('fussed', 6.01870059967041), ('barong', 6.195380210876465), ('dealer', 6.453127384185791), ('messenger', 6.489078521728516), ('grounds', 6.497330665588379)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SKL5FNpM4fK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4c3ff5d-9af4-4e02-ad00-c141431f7a76"
      },
      "source": [
        "def get_closest_word_from_a_list(chosen_words,  topn):\n",
        "  chosen_words_and_their_neighbours=[]\n",
        "  for word in chosen_words:\n",
        "    get_result = get_closest_word(word,  topn)\n",
        "    neighbours = [nb[0] for nb in get_result]\n",
        "    chosen_words_and_their_neighbours.append((neighbours,word))\n",
        "  return chosen_words_and_their_neighbours\n",
        "\n",
        "neighbours = get_closest_word_from_a_list(chosen_words,5)\n",
        "neighbours"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['tuilieres', 'overload', 'bam', 'asun', 'visiting'], 'hotel'),\n",
              " (['fantastic', 'excellent', 'superb', 'best', 'palmetto'], 'great'),\n",
              " (['beds', 'kalverstraat', 'bathrooms', 'victorian', 'spacious'], 'clean'),\n",
              " (['amzing', 'hawai', 'talk', 'greasy', 'like'], 'issue'),\n",
              " (['small', 'tiananmen', 'single', 'expecting', 'wood'], 'adequate'),\n",
              " (['suitehotel', 'scenarios', 'legacy', 'caretakers', 'pillow'], 'smoking'),\n",
              " (['hotel', 'embargo', 'declare', 'crazy', 'hairstylist'], 'italy'),\n",
              " (['stupid', 'hanson', 'fra', 'ankle', 'caren'], 'filled'),\n",
              " (['hotel', 'bedding', 'intensive', 'walls', 'definitely'], 'comment')]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQAjrX1BNKuv"
      },
      "source": [
        "## 3. find the 5 closest words for 9 words from the scifi dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-pFBNeQNKuw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07db7e22-3977-4abd-86c7-0f57bde865ae"
      },
      "source": [
        "# check the frequencies of the words\n",
        "frequency = pd.value_counts(scifi_word_list)\n",
        "print(\"The most frequent words are\\n{}\\n------------------------------\".format(frequency.head(20)))\n",
        "print(\"The less frequent words are\\n{}\\n------------------------------\".format(frequency.iloc[500:520]))\n",
        "print(\"The much less frequent words are\\n{}\\n------------------------------\".format(frequency.iloc[800:820]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most frequent words are\n",
            "said      76385\n",
            "one       57263\n",
            "would     46663\n",
            "could     41425\n",
            "like      36472\n",
            "time      32907\n",
            "back      32185\n",
            "man       30097\n",
            "know      28632\n",
            "get       24516\n",
            "two       21847\n",
            "see       21211\n",
            "way       21081\n",
            "even      20510\n",
            "right     19564\n",
            "first     19159\n",
            "well      18729\n",
            "got       17908\n",
            "little    17267\n",
            "think     17003\n",
            "dtype: int64\n",
            "------------------------------\n",
            "The less frequent words are\n",
            "pointed      2320\n",
            "shot         2318\n",
            "laughed      2314\n",
            "happen       2312\n",
            "lips         2306\n",
            "paper        2294\n",
            "alive        2287\n",
            "shall        2284\n",
            "although     2282\n",
            "attention    2280\n",
            "ships        2278\n",
            "area         2278\n",
            "died         2275\n",
            "position     2271\n",
            "stuff        2249\n",
            "reach        2248\n",
            "broke        2245\n",
            "dear         2244\n",
            "speak        2240\n",
            "answered     2239\n",
            "dtype: int64\n",
            "------------------------------\n",
            "The much less frequent words are\n",
            "chest         1514\n",
            "aside         1509\n",
            "ears          1506\n",
            "possibly      1506\n",
            "indeed        1505\n",
            "steve         1504\n",
            "spread        1503\n",
            "forced        1503\n",
            "venus         1500\n",
            "ancient       1499\n",
            "threw         1498\n",
            "weight        1497\n",
            "class         1493\n",
            "mark          1490\n",
            "expression    1488\n",
            "seeing        1488\n",
            "creatures     1488\n",
            "final         1487\n",
            "growing       1485\n",
            "telling       1482\n",
            "dtype: int64\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFytFPDbNKuw"
      },
      "source": [
        "# We chose 3 nouns, 3 verbs, and 3 adjectives respectively from the above 3 frequency levels.\n",
        "chosen_words_scifi = ['time','think','right',\n",
        "                      'blood','smile','tiny',\n",
        "                      'party','worry','warm']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-4MXoB1NKux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c10fe9-cc8b-4b18-93d4-3ccfdcb02559"
      },
      "source": [
        "def get_closest_word_scifi(word, topn):\n",
        "  word_distance = []\n",
        "  emb = model_scifi.embeddings\n",
        "  pdist = nn.PairwiseDistance()\n",
        "  i = word2index_scifi[word]\n",
        "  lookup_tensor_i = torch.tensor([i],dtype=torch.long).to(device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "  v_i = emb(lookup_tensor_i)\n",
        "  for j in range(len(scifi_vocabulary)):\n",
        "    if j !=i:\n",
        "      lookup_tensor_j = torch.tensor([j],dtype=torch.long).to(device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "      v_j = emb(lookup_tensor_j)\n",
        "      word_distance.append((index2word_scifi[j],float(pdist(v_i,v_j))))\n",
        "  word_distance.sort(key=lambda x:x[1])\n",
        "  return word_distance[:topn]\n",
        "\n",
        "def get_closest_word_from_a_list_scifi(chosen_words, topn):\n",
        "  chosen_words_and_their_neighbours=[]\n",
        "  for word in chosen_words:\n",
        "    get_result = get_closest_word_scifi(word, topn)\n",
        "    neighbours = [nb[0] for nb in get_result]\n",
        "    chosen_words_and_their_neighbours.append((neighbours,word))\n",
        "  return chosen_words_and_their_neighbours\n",
        "\n",
        "\n",
        "neighbours = get_closest_word_from_a_list_scifi(chosen_words_scifi, 5)\n",
        "neighbours"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['said', 'first', 'thought', 'made', 'like'], 'time'),\n",
              " (['know', 'tell', 'enough', 'want', 'sure'], 'think'),\n",
              " (['said', 'man', 'made', 'course', 'little'], 'right'),\n",
              " (['single', 'small', 'back', 'little', 'light'], 'blood'),\n",
              " (['power', 'ship', 'back', 'right', 'said'], 'smile'),\n",
              " (['small', 'dark', 'turned', 'came', 'bright'], 'tiny'),\n",
              " (['course', 'way', 'point', 'man', 'men'], 'party'),\n",
              " (['seems', 'however', 'right', 'girl', 'must'], 'worry'),\n",
              " (['watched', 'snapped', 'went', 'saw', 'looking'], 'warm')]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbA6fBZaa6Iv"
      },
      "source": [
        "## 5. Choose two words and retrive their 5 closest neighbours from both datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EcLEX5zbHB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd007e22-124f-4973-fd83-83b3dde48706"
      },
      "source": [
        "chosen_words=['good','job']\n",
        "neighbours_from_reviews = get_closest_word_from_a_list(chosen_words, 5)\n",
        "neighbours_from_scifi = get_closest_word_from_a_list_scifi(chosen_words, 5)\n",
        "\n",
        "\n",
        "print(\"5 closest neighbours of the chosen words in hotel reviews dataset:\")\n",
        "print(neighbours_from_reviews)\n",
        "print(\"5 closest neighbours of the chosen words in scifi dataset:\")\n",
        "print(neighbours_from_scifi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 closest neighbours of the chosen words in hotel reviews dataset:\n",
            "[(['amandari', 'blooming', 'best', 'great', 'decent'], 'good'), (['nameless', 'experience', 'precinct', 'bouncy', 'whopping'], 'job')]\n",
            "5 closest neighbours of the chosen words in scifi dataset:\n",
            "[(['time', 'said', 'knew', 'like', 'first'], 'good'), (['even', 'work', 'point', 'time', 'though'], 'job')]\n"
          ]
        }
      ]
    }
  ]
}