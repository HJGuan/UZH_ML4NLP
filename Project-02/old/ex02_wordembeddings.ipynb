{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAoDAObfnyAO"
   },
   "source": [
    "# Importing all the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zqsIOuQjnyAP"
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "CUDA = torch.cuda.is_available()\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "DY4qiVJxnyAT",
    "outputId": "f5608ffc-a5cb-42bd-f3ab-109fe84ee0fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4d94a155a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "p50aJzcDnyAX",
    "outputId": "56b3a2c9-1331-4fe0-9b2d-1bd075409bee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0SJujminyAa"
   },
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 4\n",
    "EMBEDDING_DIM = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ef76cw-8nyAd"
   },
   "source": [
    "# Reading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "rC72DQcun6_H",
    "outputId": "869e0f45-772d-4a30-c9ed-a7d599003d8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "bT02fxSon6MB",
    "outputId": "724841d2-6965-476f-8a8c-4c657acd54a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1030_2.vb\t    Resume.docx\t\t\t   'Untitled document.gdoc'\n",
      "'Colab Notebooks'  'Rishav Resume.pdf'\n",
      "'Google Buzz'\t    tripadvisor_hotel_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/content/drive/My Drive\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gYREVKynyAe"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('tripadvisor_hotel_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "H33Yt7irnyAh",
    "outputId": "930b29f0-a1f9-42b7-d5ce-56c7686d8f0a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nice hotel expensive parking got good deal sta...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok nothing special charge diamond member hilto...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nice rooms not 4* experience hotel monaco seat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unique, great stay, wonderful time hotel monac...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great stay great stay, went seahawk game aweso...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  nice hotel expensive parking got good deal sta...       4\n",
       "1  ok nothing special charge diamond member hilto...       2\n",
       "2  nice rooms not 4* experience hotel monaco seat...       3\n",
       "3  unique, great stay, wonderful time hotel monac...       5\n",
       "4  great stay great stay, went seahawk game aweso...       5"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "3oAEgPUQnyAk",
    "outputId": "ff030697-c621-4487-ff7d-6e3dc9ea048e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20491 entries, 0 to 20490\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Review  20491 non-null  object\n",
      " 1   Rating  20491 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 320.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhjdrnznnyAn"
   },
   "outputs": [],
   "source": [
    "del df['Rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSL2gH7jnyAr"
   },
   "source": [
    "# Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kF0QVOT1nyAs"
   },
   "outputs": [],
   "source": [
    "df['Review'] = df['Review'].apply(lambda x: re.sub('[!@*#/$:).;,?&(\\{\\}\\[\\]\\d+]', '', x.lower()))\n",
    "df['Review'] = df['Review'].apply(lambda x: re.sub('  ', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "kPwr1CDRnyAv",
    "outputId": "ddfa4775-3245-4c73-b6c8-d5d77da5f883"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nice hotel expensive parking got good deal sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok nothing special charge diamond member hilto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nice rooms not experience hotel monaco seattle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unique great stay wonderful time hotel monaco ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great stay great stay went seahawk game awesom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review\n",
       "0  nice hotel expensive parking got good deal sta...\n",
       "1  ok nothing special charge diamond member hilto...\n",
       "2  nice rooms not experience hotel monaco seattle...\n",
       "3  unique great stay wonderful time hotel monaco ...\n",
       "4  great stay great stay went seahawk game awesom..."
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpdpF8KZnyAy"
   },
   "source": [
    "# Combine all the items together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E42LDxxhnyAy"
   },
   "outputs": [],
   "source": [
    "all_reviews = df['Review'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Jy-LS-s2nyA1",
    "outputId": "79d72acc-ed66-4b81-d748-f8dfd76e24df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20491, (20491, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_reviews), df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5JhAjxGnyA3"
   },
   "source": [
    "# Method to join all the records in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_t9emwunyA4"
   },
   "outputs": [],
   "source": [
    "def joinList(reviews):\n",
    "    raw_data = \" \".join(all_reviews)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWNTjDsjGZ5T"
   },
   "source": [
    "# Reading the Scifi file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "tLYxZwdGGjrX",
    "outputId": "cfede804-394c-42ed-fc97-5cc066b39675"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "JpWWI4BLGjhU",
    "outputId": "8284cc9f-7d09-4e48-d26e-5730d4a220d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Colab Notebooks'\n",
      " DSC_1060.JPG\n",
      " DSC_1756.JPG\n",
      " DSC_1759.JPG\n",
      " DSC_1891.JPG\n",
      " embeddings_scifi1_40epoc_60emb.npy\n",
      " ex01_lc.ipynb\n",
      " ex01_mlp.ipynb\n",
      "'Ex 2-Scifi.ipynb'\n",
      " master.csv\n",
      "'MS OFFICE ACTIVATOR.rar'\n",
      " MyResume.doc\n",
      " MyResume.doc.gdoc\n",
      " Resume\n",
      "'Resume (1).docx'\n",
      " Resume.docx\n",
      " Road-Rash_Win_EN_RIP-Version.zip\n",
      " scifi.txt\n",
      " Statement_1535772563820.pdf\n",
      " tanu\n",
      " tripadvisor_hotel_reviews.gsheet\n",
      "'tripadvisor_hotel_reviews - tripadvisor_hotel_reviews.csv'\n",
      "'Untitled document.gdoc'\n",
      "'Veronica Chelea Sustainable Tourism Development (1).gslides'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/content/drive/My Drive\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "O8TpK2QKGZ5U"
   },
   "outputs": [],
   "source": [
    "with open('scifi.txt', 'r') as text:\n",
    "    textfile = text.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2eyWaTdRGZ5X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCld0gHYGZ5a"
   },
   "source": [
    "# Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zfRi3LvmGZ5a"
   },
   "outputs": [],
   "source": [
    "textfile = re.sub(r'\\s+', ' ',textfile)\n",
    "textfile = re.sub('[-!@*#/$:).;,?&(\\{\\}\\[\\]\\d+]', '', textfile)\n",
    "textfile = re.sub(r'\\s+', ' ',textfile)\n",
    "textfile = re.sub(r'['+string.punctuation+']', '',textfile)\n",
    "textfile = re.sub('  ','',textfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jadC_3DGZ5d"
   },
   "source": [
    "# Combine all the items together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GwL3pjRLGZ5e"
   },
   "outputs": [],
   "source": [
    "textfile_list = textfile.split()\n",
    "vocab = set(textfile_list)\n",
    "word2index = {w:i for i,w in enumerate(vocab)}\n",
    "index2word = {i:w for i,w in enumerate(vocab)}\n",
    "    \n",
    "textfile_list = textfile_list[:2000000]\n",
    "vocab = set(textfile_list)\n",
    "word2index = {w:i for i,w in enumerate(vocab)}\n",
    "index2word = {i:w for i,w in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEKz68r8nyA7"
   },
   "source": [
    "## Class for building the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CzOJR9ysnyA7"
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "            \n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "    def add_token(self, token):\n",
    "        index = len(self._token_to_idx)\n",
    "        self._token_to_idx[token] = index\n",
    "        self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def getidx(self):\n",
    "        return self._token_to_idx\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-qXn2ZgnyBA"
   },
   "outputs": [],
   "source": [
    "# test = Vocabulary()\n",
    "# raw_text = \"\"\"We Chennai about to study the idea of a computational process.\n",
    "# Computational processes are Chennai beings that inhabit computers.\n",
    "# As they evolve, Chennai manimpulate other abstract things called data.\n",
    "# The evolution of a process is Chennai by a pattern of rules\n",
    "# called a program. People create programs to direct processes. In effect,\n",
    "# we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kw08yf6mnyBD"
   },
   "outputs": [],
   "source": [
    "# for word in raw_text:\n",
    "#     test.add_token(word) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUB4Qr24nyBG"
   },
   "outputs": [],
   "source": [
    "# print(test.getidx())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3gwjAuEnyBK"
   },
   "source": [
    "## Class for building the Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3YLMO_ngnyBK"
   },
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "    def __init__(self, review_vocab=None):\n",
    "        self.review_vocab = review_vocab\n",
    "\n",
    "    def vectorize(self):\n",
    "        data = []\n",
    "        for i in range(2, len(self.review_vocab)-2):\n",
    "            context = [self.review_vocab[i-2], self.review_vocab[i-1], self.review_vocab[i+1], self.review_vocab[i+2]]\n",
    "            target = self.review_vocab[i]\n",
    "            data.append((context,target))\n",
    "        #print(\"data -> \",data[:5])\n",
    "        return data\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8H1HxzVnyBO"
   },
   "outputs": [],
   "source": [
    "# raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "# Computational processes are abstract beings that inhabit computers.\n",
    "# As they evolve, processes manimpulate other abstract things called data.\n",
    "# The evolution of a process is directed by a pattern of rules\n",
    "# called a program. People create programs to direct processes. In effect,\n",
    "# we conjure the spirits of the computer with our spells.\"\"\".split() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4krSebmnyBS"
   },
   "outputs": [],
   "source": [
    "# reviewVec = ReviewVectorizer(raw_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18FzwqkunyBV"
   },
   "outputs": [],
   "source": [
    "# dataret = reviewVec.vectorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCYDzQGqnyBX"
   },
   "outputs": [],
   "source": [
    "# print(dataret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFYN6IbHnyBZ"
   },
   "outputs": [],
   "source": [
    "# new_df = pd.DataFrame(dataret, columns=[\"context_review\",\"target_review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0YleDNRnyBc"
   },
   "outputs": [],
   "source": [
    "# new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cW_lkH9tnyBr"
   },
   "outputs": [],
   "source": [
    "# def get_index(x):\n",
    "#     return (list(test.lookup_token(token) for token in x))\n",
    "# new_df['context_review'] = new_df['context_review'].apply(get_index)\n",
    "# new_df['target_review'] = new_df['target_review'].apply(test.lookup_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ypxy7S5enyB5"
   },
   "outputs": [],
   "source": [
    "# new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYe0OuvNnyCE"
   },
   "source": [
    "# Creating Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0NhnbxqnyCI"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, X, y, scale_X=True):\n",
    "        if not torch.is_tensor(X):\n",
    "            if CUDA:\n",
    "                self.X = torch.tensor(X).cuda()\n",
    "            else:\n",
    "                self.X = torch.tensor(X)\n",
    "        if not torch.is_tensor(y):\n",
    "            if CUDA:\n",
    "                self.y = torch.tensor(y).cuda()\n",
    "            else:\n",
    "                self.y = torch.tensor(y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    \n",
    "def generate_batches(dataset, batch_size,\n",
    "                     drop_last=False, device=\"cpu\"):\n",
    "    train, test = train_test_split(list(range(dataset.X.shape[0])), test_size=.3)\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            sampler=SubsetRandomSampler(train),num_workers=0)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mo4LvoPinyCP"
   },
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(dataset=dataset, batch_size=32,\n",
    "#                             sampler=SubsetRandomSampler(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1drR5xanyCV"
   },
   "source": [
    "# Creating Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJn8rmT9nyCY"
   },
   "outputs": [],
   "source": [
    "class CBOW2(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW2,self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size,embedding_dim)\n",
    "        if CUDA:\n",
    "            self.embeddings = self.embeddings.cuda()\n",
    "        self.layer1 = nn.Linear(embedding_dim, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(128, vocab_size)\n",
    "    \n",
    "    def forward(self, data_input):\n",
    "#         print(\"data_input shape\",data_input.shape)\n",
    "        embeddings = self.embeddings(data_input.long())\n",
    "#         print(\"embedding shape\", embeddings.shape)\n",
    "        output_layer1 = self.layer1(embeddings)\n",
    "#         print(\"output layer 1 shape\", output_layer1.shape)\n",
    "        output_layer1 = torch.mean(output_layer1, dim=1)\n",
    "#         print(\"Output layer 1 after mean shape\", output_layer1.shape)\n",
    "        output_layer1_relu = self.relu(output_layer1)\n",
    "#         print(\"output layer 1 shape relu\", output_layer1.shape)\n",
    "        output_layer2 = self.layer2(output_layer1)\n",
    "#         print(\"output layer 2 shape\", output_layer2.shape)\n",
    "        log_softmax = F.log_softmax(output_layer2)\n",
    "        return log_softmax\n",
    "\n",
    "    \n",
    "    def write_embedding_to_file(self,filename):\n",
    "      for i in self.embeddings.parameters():\n",
    "        weights = i.data.numpy()\n",
    "      np.save(filename,weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kfj3cT_3nyCj"
   },
   "source": [
    "# Creating the runner class for Tripadvisor dataset to process further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPcC55jnnyCk"
   },
   "outputs": [],
   "source": [
    "def runner():\n",
    "    all_reviews = df['Review'].tolist()\n",
    "   \n",
    "    corpus_df = [x.split() for x in all_reviews]\n",
    "    vocab = list(set(itertools.chain.from_iterable(corpus_df)))\n",
    "    corpus = list(itertools.chain.from_iterable(corpus_df))\n",
    "    \n",
    "    word2index = {w:i for i,w in enumerate(vocab)}\n",
    "    index2word = {i:w for i,w in enumerate(vocab)}\n",
    "\n",
    "\n",
    "    review_vectorizer = ReviewVectorizer(corpus)\n",
    "\n",
    "\n",
    "    context_target_pair = review_vectorizer.vectorize()\n",
    "    \n",
    "\n",
    "    # print(context_target_pair)\n",
    "\n",
    "    review_new_df = pd.DataFrame(context_target_pair, columns=[\"context_review\",\"target_review\"])\n",
    "    # print(review_new_df)\n",
    "    \n",
    "    def get_index(x):\n",
    "        return (list(word2index[token] for token in x))\n",
    "\n",
    "    review_new_df['context_review'] = review_new_df['context_review'].apply(get_index)\n",
    "    review_new_df['target_review'] = review_new_df['target_review'].apply(lambda x: word2index[x])\n",
    "    \n",
    "    # print(review_new_df)\n",
    "    \n",
    "    X, y = np.array(review_new_df['context_review'].tolist()), np.array(review_new_df['target_review'].tolist())\n",
    "    dataset = ReviewDataset(X,y)\n",
    "    \n",
    "    # print(\"dataset->\",dataset)\n",
    "    \n",
    "    dataloader = generate_batches(dataset=dataset,batch_size=64)\n",
    "    \n",
    "    return dataloader,len(vocab)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1SiExThnyCr"
   },
   "outputs": [],
   "source": [
    "dataloader,vocab_length = runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "z8xwtIEenyCz",
    "outputId": "51c0b4e2-3e1c-45ff-cd1b-2a662ae124c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[43737, 44832, 32328, 68666],\n",
       "         [39101, 31144, 49980, 49980],\n",
       "         [50396, 67379, 43058,  2209],\n",
       "         [33126, 34818, 42311, 47976],\n",
       "         [42786, 20104, 31212, 51362],\n",
       "         [29889, 74203, 13981,  7970],\n",
       "         [46148, 70191, 22988, 17418],\n",
       "         [58465, 71840, 63830, 51362],\n",
       "         [76987, 34575, 43048, 12648],\n",
       "         [62805, 22417, 18439, 13454],\n",
       "         [ 8619, 72023, 22311, 65951],\n",
       "         [59494, 38840, 56613, 61753],\n",
       "         [30253, 24107, 74093, 64208],\n",
       "         [32328, 24785, 32328, 43909],\n",
       "         [32328, 36462, 15152, 43316],\n",
       "         [32783, 53907, 43048,  3722],\n",
       "         [58465, 15237, 30253, 63755],\n",
       "         [26174, 17657, 32423,  8506],\n",
       "         [56244, 76439, 57220, 56244],\n",
       "         [65662, 54185, 68666, 65966],\n",
       "         [57010, 29847, 45520, 49894],\n",
       "         [57010, 62805, 34687, 20979],\n",
       "         [31272, 65834, 32086, 48398],\n",
       "         [46871, 12366, 26622, 51741],\n",
       "         [26153, 24584,  7637, 38063],\n",
       "         [56452, 21052, 28432, 58740],\n",
       "         [31041, 56504, 29851, 77104],\n",
       "         [23923, 51362, 17657, 70065],\n",
       "         [51233, 47004, 51814, 75937],\n",
       "         [ 7705, 34114, 20007, 64121],\n",
       "         [19722, 30694, 49866, 65833],\n",
       "         [77104, 26419, 32328,  7705],\n",
       "         [77262,  8619, 52791, 77104],\n",
       "         [23358,  5315, 62805, 77104],\n",
       "         [55080, 35791, 34575, 17486],\n",
       "         [61220, 45952, 47991, 24864],\n",
       "         [30282, 67379, 29847, 77936],\n",
       "         [18526, 67379, 41074, 30253],\n",
       "         [62770, 41764, 35289, 34375],\n",
       "         [51362, 22754, 32328, 34575],\n",
       "         [36577, 18182, 34114, 64121],\n",
       "         [45893, 12366, 30169, 51362],\n",
       "         [37444, 34114, 56244, 10236],\n",
       "         [65951, 52791, 24785, 55776],\n",
       "         [63960, 36225, 64304, 58881],\n",
       "         [ 2378, 24940,  3217, 62253],\n",
       "         [72350, 30170, 46944, 77819],\n",
       "         [65662,    41, 65966,  7695],\n",
       "         [59647, 71258, 68666,  7781],\n",
       "         [ 3217, 56504, 39822, 75513],\n",
       "         [77104, 55120, 58223, 70356],\n",
       "         [51362, 44816, 52014, 40090],\n",
       "         [59888, 69785, 63144, 32328],\n",
       "         [76850, 21142, 10007, 29581],\n",
       "         [13393, 44152, 62039, 53801],\n",
       "         [30578,  6054, 34998, 56292],\n",
       "         [74884, 16240, 72571,  7789],\n",
       "         [32328, 61746, 18439, 56504],\n",
       "         [65966, 32328, 38840,  7107],\n",
       "         [56244, 37281, 35133, 50948],\n",
       "         [34818, 19904, 48743,  3194],\n",
       "         [38840, 10928, 74082, 52723],\n",
       "         [15229, 69939, 35576, 24589],\n",
       "         [24575, 25305, 43052, 35310]], device='cuda:0'),\n",
       " tensor([65966, 49363, 39822, 43585, 52886, 64546, 56587, 15327, 75416, 32328,\n",
       "         49393, 16240, 61220, 38689, 43176, 75416, 72375, 25967, 10941, 52163,\n",
       "         57861, 49992, 53974, 58948, 46640, 35940, 75063, 22417, 44832,  9505,\n",
       "         17077, 32423,  3762, 49325, 48898, 31160, 15093, 66392, 61568, 68537,\n",
       "         65212, 44210, 62805, 77104, 64520, 65951, 20355, 12366, 21373, 65966,\n",
       "         36755, 25197, 12510, 57687, 26821, 15544, 40000, 12366, 51149, 23821,\n",
       "          3217, 51345, 32423, 58465], device='cuda:0')]"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(dataloader).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mN9VSiIunyC6"
   },
   "outputs": [],
   "source": [
    "# for index,(context, target) in enumerate(dataloader):\n",
    "\n",
    "#         # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "#         # into integer indices and wrap them in tensors)\n",
    "#         context_tensor = Variable(context)\n",
    "#         target_tensor = Variable(target)\n",
    "        \n",
    "#         print(context_tensor.shape)\n",
    "#         print(target_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "lzVZaTbQnyC_",
    "outputId": "ea62dd9f-97ed-4875-ca70-92e3ec556ee7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vocab_length)\n",
    "np.sqrt(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "KrT7-kKinyDI",
    "outputId": "082e7399-489b-4b6a-9fab-9c58f3489a57"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  2%|▏         | 1/50 [02:53<2:21:33, 173.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Batch loss: 437.357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [17:17<2:06:33, 172.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Batch loss: 424.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [31:36<1:51:45, 171.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Batch loss: 422.277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [45:55<1:37:31, 172.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50], Batch loss: 421.026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [1:00:18<1:23:23, 172.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/50], Batch loss: 420.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [1:14:40<1:08:54, 172.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/50], Batch loss: 419.336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [1:28:59<54:24, 171.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/50], Batch loss: 418.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [1:43:18<40:05, 171.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/50], Batch loss: 418.071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [1:57:37<25:45, 171.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/50], Batch loss: 417.523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [2:11:54<11:26, 171.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/50], Batch loss: 417.014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▍| 47/50 [2:14:46<08:34, 171.47s/it]"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "model = CBOW2(vocab_length, EMBEDDING_DIM)\n",
    "if CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in tqdm.tqdm(range(50)):\n",
    "    total_loss = 0\n",
    "    for index,(context, target) in enumerate(dataloader):\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_tensor = Variable(context)\n",
    "        target_tensor = Variable(target)\n",
    "        \n",
    "        preds = model(context_tensor)\n",
    "        target = target.type(torch.LongTensor)\n",
    "        \n",
    "        loss = loss_function(preds, target_tensor)\n",
    "\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "#         log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "#         loss = loss_function(log_probs, torch.tensor([ret[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "    \n",
    "    mbl = np.mean(np.sqrt(losses)).round(3)\n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Epoch [{}/{}], Batch loss: {}\".format(epoch, 50, mbl))  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZ19fIH2nyDS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeeZE5zzd_ho"
   },
   "outputs": [],
   "source": [
    "model.cpu().write_embedding_to_file('embeddings_table_40epoc_60emb.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYilQwoGGZ6Q"
   },
   "source": [
    "# Creating the runner class for Scifi dataset to process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "zPuyQ9zpGZ6R"
   },
   "outputs": [],
   "source": [
    "def runner():\n",
    "   \n",
    "    vocab = set(textfile_list)\n",
    "    word2index = {w:i for i,w in enumerate(vocab)}\n",
    "    index2word = {i:w for i,w in enumerate(vocab)}\n",
    "    \n",
    "\n",
    "\n",
    "    review_vectorizer = ReviewVectorizer(textfile_list)\n",
    "\n",
    "\n",
    "    context_target_pair = review_vectorizer.vectorize()\n",
    "    \n",
    "\n",
    "    print(context_target_pair)\n",
    "\n",
    "    review_new_df = pd.DataFrame(context_target_pair, columns=[\"context_review\",\"target_review\"])\n",
    "    print(review_new_df)\n",
    "    \n",
    "    def get_index(x):\n",
    "        return (list(word2index[token] for token in x))\n",
    "\n",
    "    review_new_df['context_review'] = review_new_df['context_review'].apply(get_index)\n",
    "    review_new_df['target_review'] = review_new_df['target_review'].apply(lambda x: word2index[x])\n",
    "    \n",
    "    print(review_new_df)\n",
    "    \n",
    "    X, y = np.array(review_new_df['context_review'].tolist()), np.array(review_new_df['target_review'].tolist())\n",
    "    dataset = ReviewDataset(X,y)\n",
    "    \n",
    "    print(\"dataset->\",dataset)\n",
    "    \n",
    "    dataloader = generate_batches(dataset=dataset,batch_size=64)\n",
    "    \n",
    "    return dataloader,len(vocab)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "k4Uapyn7GZ6T",
    "outputId": "5971dbe2-971e-4a21-c9d4-8c5113487665"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              context_review target_review\n",
      "0                     [MARCH, All, New, and]       Stories\n",
      "1              [All, Stories, and, Complete]           New\n",
      "2        [Stories, New, Complete, Publisher]           and\n",
      "3              [New, and, Publisher, Editor]      Complete\n",
      "4                [and, Complete, Editor, IF]     Publisher\n",
      "...                                      ...           ...\n",
      "1999991         [perversity, of, man, whose]             a\n",
      "1999992                 [of, a, whose, mind]           man\n",
      "1999993                  [a, man, mind, and]         whose\n",
      "1999994             [man, whose, and, heart]          mind\n",
      "1999995            [whose, mind, heart, are]           and\n",
      "\n",
      "[1999996 rows x 2 columns]\n",
      "                       context_review  target_review\n",
      "0        [37008, 60454, 39271, 31197]          24068\n",
      "1        [60454, 24068, 31197, 22903]          39271\n",
      "2          [24068, 39271, 22903, 872]          31197\n",
      "3          [39271, 31197, 872, 44926]          22903\n",
      "4        [31197, 22903, 44926, 22127]            872\n",
      "...                               ...            ...\n",
      "1999991  [34139, 34397, 29250, 56672]          51144\n",
      "1999992  [34397, 51144, 56672, 52562]          29250\n",
      "1999993  [51144, 29250, 52562, 31197]          56672\n",
      "1999994  [29250, 56672, 31197, 50870]          52562\n",
      "1999995  [56672, 52562, 50870, 54835]          31197\n",
      "\n",
      "[1999996 rows x 2 columns]\n",
      "dataset-> <__main__.ReviewDataset object at 0x7fdbf8ddbbe0>\n"
     ]
    }
   ],
   "source": [
    "dataloader,vocab_length = runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_SYdyAL4GZ6V",
    "outputId": "70f48787-ab40-4bc4-b414-e0b7eac3f5ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[57207, 24697, 44153, 31197],\n",
       "         [44332, 10544,  9721, 52127],\n",
       "         [16148,  7486, 50840, 59899],\n",
       "         [56435, 43178, 45580, 38811],\n",
       "         [52333, 12177, 13440,  3094],\n",
       "         [51144, 31209, 24697,  7486],\n",
       "         [12516, 50840, 42201, 30961],\n",
       "         [43084, 16275, 15876, 21511],\n",
       "         [61655, 36829, 57207, 31197],\n",
       "         [53941, 63463, 37778,  7486],\n",
       "         [17792, 31539, 43885, 26711],\n",
       "         [63484, 24697,   657, 19735],\n",
       "         [32683,  3094,  7693,  3132],\n",
       "         [55709, 49128,  3094,  6347],\n",
       "         [14550, 56411, 59899, 38987],\n",
       "         [19728,  9929, 54835, 12046],\n",
       "         [52680,  2670, 62558, 42167],\n",
       "         [26711, 42389, 11562, 59899],\n",
       "         [65619, 37693, 48405, 44862],\n",
       "         [51144,  1435, 18105, 48405],\n",
       "         [ 3116, 33050, 24714, 23051],\n",
       "         [60371, 34310, 21901,  7332],\n",
       "         [61655, 56060, 26711, 51144],\n",
       "         [25838, 15522, 42434,  9975],\n",
       "         [14241, 45753, 51144, 27945],\n",
       "         [13440,  7990, 24697, 62163],\n",
       "         [36308, 51627, 31197, 46255],\n",
       "         [53306, 18179,  7693, 50803],\n",
       "         [ 1647,  7332, 18850, 58274],\n",
       "         [ 5753, 50840, 45836, 11562],\n",
       "         [ 7486,  7693, 57982, 59541],\n",
       "         [40956, 23040, 52333, 51081],\n",
       "         [24373, 27909, 29608,  4310],\n",
       "         [43885, 32842,  2452, 31197],\n",
       "         [35693, 52670, 42714, 42446],\n",
       "         [28497, 42407, 41077, 24714],\n",
       "         [64618, 54024,  3094, 22570],\n",
       "         [  499, 36417,  5305, 24714],\n",
       "         [12899, 21359, 22124, 24714],\n",
       "         [ 7097, 59246,  7162, 41069],\n",
       "         [34397, 32020, 55268, 25646],\n",
       "         [21359,  7693, 35895, 10667],\n",
       "         [49128,  9975, 58537, 56630],\n",
       "         [60203, 13618, 59899, 52061],\n",
       "         [ 3094, 28621, 15778, 52897],\n",
       "         [11431, 34397, 39581, 28621],\n",
       "         [51144, 25756, 49166, 51144],\n",
       "         [29843, 11082, 41610, 29709],\n",
       "         [49401, 20706, 51843, 26711],\n",
       "         [ 3094, 63833, 14469, 35467],\n",
       "         [49166,  7013, 56346, 59549],\n",
       "         [40145, 24714,  1959, 51144],\n",
       "         [37250, 34349, 65913, 62827],\n",
       "         [64317, 31037, 44862, 18646],\n",
       "         [39581, 48717, 46255, 64662],\n",
       "         [50084, 12046, 45507, 26711],\n",
       "         [53718, 49122, 58177, 24714],\n",
       "         [26711, 64370, 59899, 50090],\n",
       "         [18105, 46341, 32212, 24714],\n",
       "         [31197,  9848, 13440, 38765],\n",
       "         [24714, 52333, 45265,  3094],\n",
       "         [34397, 18219, 17792, 30446],\n",
       "         [47614, 28874, 14004, 47748],\n",
       "         [39581, 40956, 35079, 18105]], device='cuda:0'),\n",
       " tensor([58537, 45534, 17778, 28521, 38794, 50190, 59899, 31197, 28521, 24697,\n",
       "          6153, 13620, 38547, 38446, 15522, 56060, 31197, 57207, 21130, 13909,\n",
       "          6789, 26711, 12948, 26659, 62827, 27898, 50913, 30691, 58537, 38794,\n",
       "         15762, 23868, 50803, 12403, 35169, 18105, 11082, 28744,  9848, 34397,\n",
       "          6697, 53276, 21359, 32545, 48405, 57965, 34009, 59899, 48914, 63026,\n",
       "         55268,  8503, 25057, 48405, 38133, 59899, 12948,  9975, 24714, 63922,\n",
       "         29250,  3094, 47748, 34397], device='cuda:0')]"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(dataloader).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "4sf-Tw5JGZ6X"
   },
   "outputs": [],
   "source": [
    "# for index,(context, target) in enumerate(dataloader):\n",
    "\n",
    "#         # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "#         # into integer indices and wrap them in tensors)\n",
    "#         context_tensor = Variable(context)\n",
    "#         target_tensor = Variable(target)\n",
    "        \n",
    "#         print(context_tensor.shape)\n",
    "#         print(target_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "xju6W0gdGZ6a",
    "outputId": "5b55226c-a2f0-499a-97bf-029fa917b145"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66521\n"
     ]
    }
   ],
   "source": [
    "print(vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "B8I_SdJGGZ6d",
    "outputId": "66fab7e0-f6d7-402b-e7b4-d4d52390f53d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  2%|▏         | 1/50 [02:25<1:58:39, 145.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Batch loss: 418.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [14:26<1:45:58, 144.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Batch loss: 402.465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [26:32<1:34:23, 145.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Batch loss: 399.524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [38:40<1:22:26, 145.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50], Batch loss: 397.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [50:48<1:10:22, 145.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/50], Batch loss: 396.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [1:02:56<58:15, 145.65s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/50], Batch loss: 395.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [1:15:03<46:02, 145.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/50], Batch loss: 395.047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [1:27:12<34:00, 145.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/50], Batch loss: 394.385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [1:39:21<21:52, 145.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/50], Batch loss: 393.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [1:51:28<09:40, 145.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/50], Batch loss: 393.275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [2:01:04<00:00, 145.29s/it]\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "model = CBOW2(vocab_length, EMBEDDING_DIM)\n",
    "if CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in tqdm.tqdm(range(50)):\n",
    "    total_loss = 0\n",
    "    for index,(context, target) in enumerate(dataloader):\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_tensor = Variable(context)\n",
    "        target_tensor = Variable(target)\n",
    "        \n",
    "        preds = model(context_tensor)\n",
    "        target = target.type(torch.LongTensor)\n",
    "        \n",
    "        loss = loss_function(preds, target_tensor)\n",
    "\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "#         log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "#         loss = loss_function(log_probs, torch.tensor([ret[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "\n",
    "    mbl = np.mean(np.sqrt(losses)).round(3)\n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Epoch [{}/{}], Batch loss: {}\".format(epoch\n",
    "                                                     , 50, mbl))  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PGKm3ikeus6m"
   },
   "outputs": [],
   "source": [
    "verb = ['was','had',  'have']\n",
    "noun = ['man', 'eyes', 'people']\n",
    "adjective = ['first', 'good', 'different']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "E--1DHGsoSDd"
   },
   "outputs": [],
   "source": [
    "model_trip = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "0rBTFLigo1c3",
    "outputId": "da506618-64b2-4fc6-89c5-193c7fd6d511"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3494,  0.2627,  0.0063,  ..., -1.5024, -0.7096,  1.7995],\n",
       "        [ 1.6593, -1.3654, -4.1932,  ...,  1.1848, -2.0572,  3.5361],\n",
       "        [ 0.1847, -1.8835, -0.1488,  ..., -0.2514,  0.8871, -0.0710],\n",
       "        ...,\n",
       "        [ 0.3431,  1.1005,  0.1094,  ..., -1.1764,  0.2043,  2.0469],\n",
       "        [ 1.8896,  0.3658,  1.8923,  ..., -1.2124, -1.1381,  0.2541],\n",
       "        [-0.1498, -1.2524,  1.8340,  ..., -2.4046, -2.9674, -2.4239]],\n",
       "       grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 111,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "gAL0cUiQos5w",
    "outputId": "a513e1ce-0f2d-4973-87ff-b5fa617997bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 114,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_trip.embedding.weight.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "xIe9lsd8N3tB",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_emb_dict = dict(zip(vocab, model_trip.embedding.weight.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "f_jq4D7BN3tR",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_closest_word(word, topn = 5):\n",
    "    word_distance = []\n",
    "    emb = word_emb_dict\n",
    "    pdist = nn.PairwiseDistance()\n",
    "    i = word2index[word]\n",
    "    v_i = torch.tensor(emb[word]).view(1,-1)\n",
    "    for j in range(len(vocab)):\n",
    "        if j != i:\n",
    "            lookup_tensor_word = index2word[j]\n",
    "            v_j = torch.tensor(emb[lookup_tensor_word]).view(1,-1)\n",
    "            distance = pdist(v_i, v_j)\n",
    "            distance = distance.numpy()\n",
    "            word_distance.append( (lookup_tensor_word, float(distance)) )\n",
    "    word_distance.sort(key=lambda x: x[1]) \n",
    "    return word_distance[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "1f2ICOP0pOQu"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(corpus)\n",
    "word_counts_df = pd.DataFrame.from_dict(word_counts, orient='index').reset_index()\n",
    "word_counts_df.columns = ['words', 'counts']\n",
    "word_counts_df.sort_values(by='counts', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "GcOHAieIpcxx"
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "uBzbJIZTpxXX"
   },
   "outputs": [],
   "source": [
    "common_words = ['the', 'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "H0n6tV8Ipt9t",
    "outputId": "d1c2d319-d4f9-4f8e-e50d-0b73684b61af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in the corpus --> the  and its nearest neighbours are:\n",
      "[('courteouswe', 14.91102409362793), ('amusic', 15.502732276916504), ('crazymoney', 15.514968872070312)]\n",
      "Most common words in the corpus --> a  and its nearest neighbours are:\n",
      "[('fiddled', 13.259047508239746), ('terrific', 13.411040306091309), ('cottagesbungalow', 13.41502571105957)]\n"
     ]
    }
   ],
   "source": [
    "for word in common_words:\n",
    "  print(\"Most common words in the corpus -->\", word, \" and its nearest neighbours are:\")\n",
    "  print(get_closest_word(word, topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "VRvSsHM1pOAR",
    "outputId": "ac440c73-877b-458a-b1fe-52bbac34995a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8170</th>\n",
       "      <td>a</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     words  counts\n",
       "8170     a      73"
      ]
     },
     "execution_count": 124,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts_df[word_counts_df.words == 'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llU5WeUjpNv8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "_1FVfiIHkIwJ"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(corpus)\n",
    "word_counts_df = pd.DataFrame.from_dict(word_counts, orient='index').reset_index()\n",
    "word_counts_df.columns = ['words', 'counts']\n",
    "word_counts_df.sort_values(by='counts', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "uYUXEapckzfs"
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "tLUw-DAVlEro"
   },
   "outputs": [],
   "source": [
    "verb = ['was','had',  'have']\n",
    "noun = ['man', 'eyes', 'people']\n",
    "adjective = ['first', 'good', 'different']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "PPtfza8kmd39",
    "outputId": "e2530a60-02f0-4ac8-92f2-f571bfc9c1eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common verbs in the corpus - was  and its nearest neighbours are:\n",
      "[('nextincommands', 14.108367919921875), ('wasnt', 14.500380516052246), ('alish', 14.795513153076172), ('tacitly', 14.84989070892334), ('cancellation', 14.955382347106934)]\n",
      "Most common verbs in the corpus - had  and its nearest neighbours are:\n",
      "[('bfen', 14.994134902954102), ('dornots', 15.030157089233398), ('atlanta', 15.20980167388916), ('conveying', 15.240338325500488), ('hadnt', 15.268654823303223)]\n",
      "Most common verbs in the corpus - have  and its nearest neighbours are:\n",
      "[('lascdio', 14.696041107177734), ('chosing', 14.704346656799316), ('foreknowledge', 14.778069496154785), ('goofs', 15.003154754638672), ('lanargon', 15.173529624938965)]\n"
     ]
    }
   ],
   "source": [
    "for word in verb:\n",
    "  print(\"Most common verbs in the corpus -->\", word, \" and its nearest neighbours are:\")\n",
    "  print(get_closest_word(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "MHhV6dSVmdwR",
    "outputId": "42f4d138-45f3-4fb5-94a0-a14034b31096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common noun in the corpus --> man  and its nearest neighbours are:\n",
      "[('person', 14.04078197479248), ('cwtrol', 14.744359970092773), ('auid', 14.977381706237793), ('timetravelers', 15.193326950073242), ('gorgeously', 15.277854919433594)]\n",
      "Most common noun in the corpus --> eyes  and its nearest neighbours are:\n",
      "[('parrotlike', 17.926855087280273), ('bevatron', 18.14840316772461), ('thoughts', 18.29469871520996), ('aunties', 18.540016174316406), ('itf', 18.575946807861328)]\n",
      "Most common noun in the corpus --> people  and its nearest neighbours are:\n",
      "[('compartnienls', 13.266700744628906), ('men', 13.309320449829102), ('uninteresting', 13.346628189086914), ('dulling', 13.83996295928955), ('resurrect', 13.864710807800293)]\n"
     ]
    }
   ],
   "source": [
    "for word in noun:\n",
    "  print(\"Most common noun in the corpus -->\", word, \" and its nearest neighbours are:\")\n",
    "  print(get_closest_word(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "Cdgj98QMmdnV",
    "outputId": "911b2be4-baa4-4318-c650-09a9ea015895"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common adjective in the corpus --> first  and its nearest neighbours are:\n",
      "[('propagandist', 14.926827430725098), ('whereever', 15.226884841918945), ('coiils', 15.30133056640625), ('seventeento', 15.305699348449707), ('saltpan', 15.426239013671875)]\n",
      "Most common adjective in the corpus --> good  and its nearest neighbours are:\n",
      "[('thighlength', 14.115311622619629), ('betuned', 14.56428050994873), ('pharaoh', 14.575023651123047), ('erde', 14.591188430786133), ('isoi', 14.591279983520508)]\n",
      "Most common adjective in the corpus --> different  and its nearest neighbours are:\n",
      "[('coptered', 15.005867004394531), ('lashings', 15.01534366607666), ('statism', 15.02927303314209), ('bfen', 15.057822227478027), ('blindwindows', 15.09548282623291)]\n"
     ]
    }
   ],
   "source": [
    "for word in adjective:\n",
    "  print(\"Most common adjective in the corpus -->\", word, \" and its nearest neighbours are:\")\n",
    "  print(get_closest_word(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "yy0FL1Zbxz2K",
    "outputId": "1b5f5ef0-0d63-4b5a-c8b9-df9e25ec4836"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in the corpus --> the  and its nearest neighbours are:\n",
      "[('muggier', 11.156322479248047), ('anticomputerites', 11.638870239257812), ('wellfurnished', 11.651493072509766)]\n",
      "Most common words in the corpus --> a  and its nearest neighbours are:\n",
      "[('randalls', 14.078964233398438), ('peasized', 14.20406436920166), ('flavoring', 14.23133373260498)]\n"
     ]
    }
   ],
   "source": [
    "common_words = ['the', 'a']\n",
    "for word in common_words:\n",
    "  print(\"Most common words in the corpus -->\", word, \" and its nearest neighbours are:\")\n",
    "  print(get_closest_word(word, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mVMeOnz8Loz5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Ex 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
