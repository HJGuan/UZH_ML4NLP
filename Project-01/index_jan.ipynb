{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "index_jan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jan-kreischer/UZH_ML4NLP/blob/main/Project-01/index_jan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvBmTj50hvR5"
      },
      "source": [
        "# Exercise 01 - Linear Classification\n",
        "\n",
        "\n"
      ],
      "id": "zvBmTj50hvR5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPHMAkQcl6HA",
        "outputId": "fe3b424e-68a7-40dd-f0a0-a7b2e4229dd8"
      },
      "source": [
        "!pip install demoji"
      ],
      "id": "rPHMAkQcl6HA",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting demoji\n",
            "  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▋                        | 10 kB 25.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 20 kB 30.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 30 kB 35.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 40 kB 37.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 42 kB 1.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: demoji\n",
            "Successfully installed demoji-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Usn0rTSte6zX",
        "outputId": "513ba4a1-57d8-46a5-aa19-c2079162dce5"
      },
      "source": [
        "! pip install alphabet-detector"
      ],
      "id": "Usn0rTSte6zX",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting alphabet-detector\n",
            "  Downloading alphabet-detector-0.0.7.tar.gz (1.6 kB)\n",
            "Building wheels for collected packages: alphabet-detector\n",
            "  Building wheel for alphabet-detector (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alphabet-detector: filename=alphabet_detector-0.0.7-py3-none-any.whl size=2443 sha256=f81f0ff0ed639c0214eb25ce5c8d58bf2ab5b0c22bcec2f35de7d3eed2b7fb2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/8c/ab/4afb1765f2b8450f894a1f06c9aa2b3f8e73f2fb8b55849e17\n",
            "Successfully built alphabet-detector\n",
            "Installing collected packages: alphabet-detector\n",
            "Successfully installed alphabet-detector-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aoiO-Esh19N"
      },
      "source": [
        "## Imports"
      ],
      "id": "2aoiO-Esh19N"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43d2c898-ea76-49c3-ae4d-dc87d657efe0"
      },
      "source": [
        "import csv\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 200)  \n",
        "pd.set_option('display.max_columns', 200)   \n",
        "pd.set_option('display.width', 4000) \n",
        "\n",
        "from io import StringIO\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import demoji\n",
        "\n",
        "from alphabet_detector import AlphabetDetector\n",
        "ad = AlphabetDetector()\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report"
      ],
      "id": "43d2c898-ea76-49c3-ae4d-dc87d657efe0",
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8zj6rO6hKo2"
      },
      "source": [
        "url_train_dev = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vTOZ2rC82rhNsJduoyKYTsVeH6ukd7Bpxvxn_afOibn3R-eadZGXu82eCU9IRpl4CK_gefEGsYrA_oM/pub?gid=1863430984&single=true&output=tsv'\n",
        "url_test = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vT-KNR9nuYatLkSbzSRgpz6Ku1n4TN4w6kKmFLkA6QJHTfQzmX0puBsLF7PAAQJQAxUpgruDd_RRgK7/pub?gid=417546901&single=true&output=tsv'"
      ],
      "id": "o8zj6rO6hKo2",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkIJ7QuNv_lt"
      },
      "source": [
        "## Constants"
      ],
      "id": "RkIJ7QuNv_lt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYruJqdrsPOS"
      },
      "source": [
        "TARGET_COLUMN = 'label'"
      ],
      "id": "DYruJqdrsPOS",
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okInclFn5rzc"
      },
      "source": [
        "## 1. Data Acquisition"
      ],
      "id": "okInclFn5rzc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdwxfjxjhN58"
      },
      "source": [
        "def load_dataset(url):\n",
        "    r = requests.get(url)\n",
        "    data = r.content.decode('utf8')\n",
        "    df = pd.read_csv(StringIO(data), sep='\\t')\n",
        "    df.columns = ['tweet', 'label']\n",
        "    return df"
      ],
      "id": "SdwxfjxjhN58",
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmOiJT-2hPpt"
      },
      "source": [
        "training_data = load_dataset(url_train_dev)\n",
        "test_data = load_dataset(url_test)"
      ],
      "id": "ZmOiJT-2hPpt",
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLhRDofrsShY"
      },
      "source": [
        "dataset = pd.concat([training_data, test_data], axis=0) # Merge into one dataset for the pre-processing"
      ],
      "id": "LLhRDofrsShY",
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSWYkwJnsZ9h",
        "outputId": "b3450c48-290e-41b7-ce36-16cb0958bd39"
      },
      "source": [
        "print(\"The length of the combined dataset is {0} training samples + {1} test samples = {2} samples\".format(len(training_data), len(test_data), len(dataset)))"
      ],
      "id": "rSWYkwJnsZ9h",
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of the combined dataset is 52675 training samples + 13279 test samples = 65954 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJDwSNkfuMcE"
      },
      "source": [
        "dataset = dataset.sample(frac=1).reset_index(drop=True) # Randomly shuffle the data"
      ],
      "id": "yJDwSNkfuMcE",
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "TvoAwKLCzd7N",
        "outputId": "5ef95d38-29c7-4401-f476-4c8ddeac5344"
      },
      "source": [
        "dataset.head(10)"
      ],
      "id": "TvoAwKLCzd7N",
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>43350</td>\n",
              "      <td>Every athlete with passion for their respectiv...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>39166</td>\n",
              "      <td>AutoCAD and Calculus for tomorrow. Fighting!</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>63465</td>\n",
              "      <td>@LukeKorns ily</td>\n",
              "      <td>und</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>34814</td>\n",
              "      <td>Sabtu, 19 Juli 2014 Jam 09:27</td>\n",
              "      <td>id</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5562</td>\n",
              "      <td>Unlock Her Legs http://t.co/pvrvc0hxdV</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>54735</td>\n",
              "      <td>ついに見てしまった！！！笑</td>\n",
              "      <td>ja</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>18046</td>\n",
              "      <td>クッキー気付いたら20枚食べてたわ</td>\n",
              "      <td>ja</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>22354</td>\n",
              "      <td>私がいつもバンＴでバイト行くき、バイト先の人からそんな男の子みたいな格好せんでもっと可愛い服...</td>\n",
              "      <td>ja</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1720</td>\n",
              "      <td>After that appointment. Today officially sucks.</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>46016</td>\n",
              "      <td>@fleissmeister  Chris the new bachelor?!!!! pl...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index                                              tweet label\n",
              "0  43350  Every athlete with passion for their respectiv...    en\n",
              "1  39166       AutoCAD and Calculus for tomorrow. Fighting!    en\n",
              "2  63465                                     @LukeKorns ily   und\n",
              "3  34814                      Sabtu, 19 Juli 2014 Jam 09:27    id\n",
              "4   5562             Unlock Her Legs http://t.co/pvrvc0hxdV    en\n",
              "5  54735                                      ついに見てしまった！！！笑    ja\n",
              "6  18046                                  クッキー気付いたら20枚食べてたわ    ja\n",
              "7  22354  私がいつもバンＴでバイト行くき、バイト先の人からそんな男の子みたいな格好せんでもっと可愛い服...    ja\n",
              "8   1720    After that appointment. Today officially sucks.    en\n",
              "9  46016  @fleissmeister  Chris the new bachelor?!!!! pl...    en"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OZ3EaAQ6Ahq"
      },
      "source": [
        "## 2. Data Exploration"
      ],
      "id": "7OZ3EaAQ6Ahq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heZh4YHxhhQA"
      },
      "source": [
        "def data_exploration(df, name=''):\n",
        "  '''\n",
        "\n",
        "  '''\n",
        "  print('--- {} ---'.format(name))\n",
        "  n_labels = len(np.unique(df[\"label\"]))\n",
        "  df = df.sort_values('label')\n",
        "  print(\"{0} contains the columns: {1}\".format(name, list(df.keys())))\n",
        "  print(\"with a total of {} observations\".format(len(df)))\n",
        "  print(\"and {} different possible labels.\".format(n_labels))\n",
        "  print(\"The unique labels are {}\".format(df[\"label\"].unique()))\n",
        "  plt.figure(figsize=(15, 3))\n",
        "  plt.hist(df[\"label\"], bins=n_labels)\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.yscale(\"log\")\n",
        "  plt.xlabel(\"Language\")\n",
        "  plt.ylabel(\"#Occurences\")\n",
        "  plt.show()"
      ],
      "id": "heZh4YHxhhQA",
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "IZtCEi7_hYQW",
        "outputId": "587d3eda-f3fa-432b-991c-1df2e0271d05"
      },
      "source": [
        "data_exploration(dataset, '')"
      ],
      "id": "IZtCEi7_hYQW",
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---  ---\n",
            " contains the columns: ['tweet', 'label']\n",
            "with a total of 65954 observations\n",
            "and 78 different possible labels.\n",
            "The unique labels are ['ar' 'ar_LATN' 'az' 'bg' 'bn' 'bs' 'ca' 'cs' 'cy' 'da' 'de' 'dv' 'el'\n",
            " 'en' 'es' 'et' 'eu' 'fa' 'fi' 'fr' 'gl' 'ha' 'he' 'hi' 'hi-Latn' 'hr'\n",
            " 'ht' 'hu' 'hy' 'id' 'is' 'it' 'ja' 'ja_LATN' 'jv' 'km' 'ko' 'ko_LATN'\n",
            " 'la' 'lv' 'mk' 'mn' 'mr' 'ms' 'ne' 'nl' 'no' 'pl' 'ps' 'ps_LATN' 'pt'\n",
            " 'ro' 'ru' 'si' 'sk' 'sl' 'sq' 'sr' 'su' 'sv' 'sw' 'ta' 'ta_LATN' 'th'\n",
            " 'tl' 'tn' 'tr' 'uk' 'und' 'ur' 'ur_LATN' 'vi' 'wo' 'xh' 'yo' 'zh-CN'\n",
            " 'zh-TW' 'zu']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYLOL2_B5OrT"
      },
      "source": [
        "## 3. Text Cleaning"
      ],
      "id": "IYLOL2_B5OrT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np67DQkz5JEm"
      },
      "source": [
        "This is generally a good idea as many text classification tools rely on counting the occurrences of words. If both upper and lower case versions of the same word are found in the text then the algorithm will count them as different words even though the meaning is the same. Of course this does mean that where the capitalised versions of a word exists, that does have a different meaning. For example the company Apple vs the fruit apple. This could result in poorer performance for some data sets. This is one area of NLP where you may try different methods to see how they affect the overall performance of the model."
      ],
      "id": "np67DQkz5JEm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsh1XvSsAwYL"
      },
      "source": [
        "def remove_all_emojis(text):\n",
        "  dem = demoji.findall(text)\n",
        "  for item in dem.keys():\n",
        "    text = text.replace(item, '')\n",
        "  return text"
      ],
      "id": "vsh1XvSsAwYL",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQfxTUB36n4C"
      },
      "source": [
        "def  clean_data(df, column):\n",
        "    df = df.copy(deep=True) # Make deep copy of tweets\n",
        "    df[column] = df[column].str.lower() # Transform into all lowercase\n",
        "    \n",
        "    patterns = []\n",
        "    retweet_pattern = '^RT'\n",
        "    patterns.append(retweet_pattern)\n",
        "    xml_pattern = '&\\S+;'\n",
        "    patterns.append(xml_pattern)\n",
        "    hashtag_pattern = '#[A-Za-z0-9_]+'\n",
        "    patterns.append(hashtag_pattern)\n",
        "    twitter_mention_pattern = '@[A-Za-z0-9_]+'\n",
        "    patterns.append(twitter_mention_pattern)\n",
        "    http_pattern = 'http\\S+'\n",
        "    patterns.append(http_pattern)\n",
        "    www_pattern = 'www\\S+'\n",
        "    patterns.append(www_pattern)\n",
        "    tab_pattern = '\\t'\n",
        "    patterns.append(tab_pattern)\n",
        "    punctuation_pattern = '[!\"#$%&\\\\()*+,-./:;<=>?@\\[\\]^_`\\'{}~]+'\n",
        "    patterns.append(punctuation_pattern)\n",
        "    numeric_pattern = '[0-9]+'\n",
        "    patterns.append(numeric_pattern)\n",
        "    regex = \"|\".join(patterns)\n",
        "\n",
        "    #df[column] = df[column].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem)) \n",
        "    df[column] = df[column].apply(lambda elem: re.sub(r\"{}\".format(regex), \"\", elem))\n",
        "    df[column] = df[column].apply(remove_all_emojis)\n",
        "    \n",
        "    return df"
      ],
      "id": "kQfxTUB36n4C",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtIoEEyK8Dxx"
      },
      "source": [
        "# Now we want to find out which special characters need to be removed from tweets in order to make the prediction better.\n",
        "# We go over the printed list an not down the symbold which are not needed for language identification.\n",
        "# These will be removed in a later step.\n",
        "languages = list(np.unique(test_data['label']))\n",
        "for language in languages:\n",
        "  localized_tweets = training_data[training_data['label'] == language]\n",
        "  # Clean and compare them\n",
        "  cleaned_localized_tweets = clean_text(localized_tweets, 'tweet')\n",
        "  comparison_view = pd.concat([localized_tweets.drop(['label'], axis=1), cleaned_localized_tweets], axis=1)\n",
        "  print(comparison_view.head(5))\n",
        "  #print(localized_tweets.head(5))\n",
        "  print(\"---\")\n",
        "\n",
        "# Symbols like @<mention>, #, http://link !, numeric values (e.g 16000), \" do not help for language identification."
      ],
      "id": "JtIoEEyK8Dxx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht7tAk-r5Re8"
      },
      "source": [
        "cleaned_dataset = clean_data(dataset, \"tweet\")"
      ],
      "id": "ht7tAk-r5Re8",
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "zJRjDV7zUAxP",
        "outputId": "09cb0b6b-0dcf-410d-a628-99d3dccc3422"
      },
      "source": [
        "pd.concat([dataset.drop(['label'], axis=1), cleaned_dataset], axis=1).sample(20, axis=0)"
      ],
      "id": "zJRjDV7zUAxP",
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>21801</th>\n",
              "      <td>マスター、ピンチの時は私のスキル、「回復の歌」を使ってくださいね。  ＨＰが２５００回復しますよ。</td>\n",
              "      <td>マスター、ピンチの時は私のスキル、「回復の歌」を使ってくださいね。  ｈｐが２５００回復しますよ。</td>\n",
              "      <td>ja</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59638</th>\n",
              "      <td>want a mcdonalds</td>\n",
              "      <td>want a mcdonalds</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43368</th>\n",
              "      <td>やっべこれ超期待</td>\n",
              "      <td>やっべこれ超期待</td>\n",
              "      <td>ja</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43195</th>\n",
              "      <td>予告編が出るようになって、マーニーの正体はがどっちなのか気になって気になって堪らない。 本屋...</td>\n",
              "      <td>予告編が出るようになって、マーニーの正体はがどっちなのか気になって気になって堪らない。 本屋...</td>\n",
              "      <td>ja</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53016</th>\n",
              "      <td>ゆう、えりかたん、まこたん、りさ、ぽんた☜愛してます。って定期。</td>\n",
              "      <td>ゆう、えりかたん、まこたん、りさ、ぽんた☜愛してます。って定期。</td>\n",
              "      <td>ja</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63232</th>\n",
              "      <td>I've collected 15,807 gold coins! http://t.co/...</td>\n",
              "      <td>ive collected  gold coins</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56508</th>\n",
              "      <td>先生が生徒を呼び出しテストの答案を並べて生徒に尋ねた。「君の回答が君の隣の席の生徒と全く同じ...</td>\n",
              "      <td>先生が生徒を呼び出しテストの答案を並べて生徒に尋ねた。「君の回答が君の隣の席の生徒と全く同じ...</td>\n",
              "      <td>ja</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5749</th>\n",
              "      <td>Jangan hiraukan mereka yg berusaha menjatuhkan...</td>\n",
              "      <td>jangan hiraukan mereka yg berusaha menjatuhkan...</td>\n",
              "      <td>id</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30678</th>\n",
              "      <td>Love Madison 💕 http://t.co/aTkfk4I37q</td>\n",
              "      <td>love madison</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15945</th>\n",
              "      <td>La copa se consigue punto a punto vamos x 3 pr...</td>\n",
              "      <td>la copa se consigue punto a punto vamos x  pri...</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22216</th>\n",
              "      <td>⛄️❄️⛄️❄️⛄️❄️⛄️ 🐧FOLLOW ME PLS🐧 Luke from 5SOS ...</td>\n",
              "      <td>️️️️ follow me pls luke from sos    i lub u to...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4690</th>\n",
              "      <td>Don't you ever get it fucked up!</td>\n",
              "      <td>dont you ever get it fucked up</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5184</th>\n",
              "      <td>@SirSamMacklen I was just listening to take ca...</td>\n",
              "      <td>i was just listening to take care driving aro...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1519</th>\n",
              "      <td>A vida não é complicada e nem difícil, tudo de...</td>\n",
              "      <td>a vida não é complicada e nem difícil tudo dep...</td>\n",
              "      <td>pt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14262</th>\n",
              "      <td>@minorin1116 @chokomints2 おーそれじゃあ8/8（金）に混ぜてくれ～...</td>\n",
              "      <td>おーそれじゃあ（金）に混ぜてくれ～同じく一日空いてるから～よし！並ぶか！！よろしくね～</td>\n",
              "      <td>ja</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49940</th>\n",
              "      <td>雨はうっとうしいけど、おかげで近所の子供たちも静か（笑）。</td>\n",
              "      <td>雨はうっとうしいけど、おかげで近所の子供たちも静か（笑）。</td>\n",
              "      <td>ja</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29277</th>\n",
              "      <td>@Ini_Talkshow Sule nelpon pemadam kebakaran sa...</td>\n",
              "      <td>sule nelpon pemadam kebakaran saat haruka mas...</td>\n",
              "      <td>id</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33292</th>\n",
              "      <td>@Nannitaweber...diga o que sente, nadar e nada...</td>\n",
              "      <td>diga o que sente nadar e nadar e morrer na pra...</td>\n",
              "      <td>pt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57828</th>\n",
              "      <td>I would have never changed if I would have nev...</td>\n",
              "      <td>i would have never changed if i would have nev...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49466</th>\n",
              "      <td>@y_bandura всё замечательно)</td>\n",
              "      <td>всё замечательно</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   tweet                                              tweet label\n",
              "21801  マスター、ピンチの時は私のスキル、「回復の歌」を使ってくださいね。  ＨＰが２５００回復しますよ。  マスター、ピンチの時は私のスキル、「回復の歌」を使ってくださいね。  ｈｐが２５００回復しますよ。    ja\n",
              "59638                                   want a mcdonalds                                   want a mcdonalds    en\n",
              "43368                                           やっべこれ超期待                                           やっべこれ超期待    ja\n",
              "43195  予告編が出るようになって、マーニーの正体はがどっちなのか気になって気になって堪らない。 本屋...  予告編が出るようになって、マーニーの正体はがどっちなのか気になって気になって堪らない。 本屋...    ja\n",
              "53016                   ゆう、えりかたん、まこたん、りさ、ぽんた☜愛してます。って定期。                   ゆう、えりかたん、まこたん、りさ、ぽんた☜愛してます。って定期。    ja\n",
              "63232  I've collected 15,807 gold coins! http://t.co/...                      ive collected  gold coins        en\n",
              "56508  先生が生徒を呼び出しテストの答案を並べて生徒に尋ねた。「君の回答が君の隣の席の生徒と全く同じ...  先生が生徒を呼び出しテストの答案を並べて生徒に尋ねた。「君の回答が君の隣の席の生徒と全く同じ...    ja\n",
              "5749   Jangan hiraukan mereka yg berusaha menjatuhkan...  jangan hiraukan mereka yg berusaha menjatuhkan...    id\n",
              "30678              Love Madison 💕 http://t.co/aTkfk4I37q                                     love madison      en\n",
              "15945  La copa se consigue punto a punto vamos x 3 pr...  la copa se consigue punto a punto vamos x  pri...    es\n",
              "22216  ⛄️❄️⛄️❄️⛄️❄️⛄️ 🐧FOLLOW ME PLS🐧 Luke from 5SOS ...  ️️️️ follow me pls luke from sos    i lub u to...    en\n",
              "4690                    Don't you ever get it fucked up!                     dont you ever get it fucked up    en\n",
              "5184   @SirSamMacklen I was just listening to take ca...   i was just listening to take care driving aro...    en\n",
              "1519   A vida não é complicada e nem difícil, tudo de...  a vida não é complicada e nem difícil tudo dep...    pt\n",
              "14262  @minorin1116 @chokomints2 おーそれじゃあ8/8（金）に混ぜてくれ～...        おーそれじゃあ（金）に混ぜてくれ～同じく一日空いてるから～よし！並ぶか！！よろしくね～    ja\n",
              "49940                      雨はうっとうしいけど、おかげで近所の子供たちも静か（笑）。                      雨はうっとうしいけど、おかげで近所の子供たちも静か（笑）。    ja\n",
              "29277  @Ini_Talkshow Sule nelpon pemadam kebakaran sa...   sule nelpon pemadam kebakaran saat haruka mas...    id\n",
              "33292  @Nannitaweber...diga o que sente, nadar e nada...  diga o que sente nadar e nadar e morrer na pra...    pt\n",
              "57828  I would have never changed if I would have nev...  i would have never changed if i would have nev...    en\n",
              "49466                       @y_bandura всё замечательно)                                   всё замечательно    ru"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFp9mg6b5weg"
      },
      "source": [
        "# 4.Data Augmentation"
      ],
      "id": "FFp9mg6b5weg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIrKnkJQ6FQ2"
      },
      "source": [
        "## 4.1 Back translation"
      ],
      "id": "rIrKnkJQ6FQ2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isQllvdnBNjy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72ad5e66-452e-4fcf-982d-dfc96dea132b"
      },
      "source": [
        "! pip install transformers\n",
        "! pip install mosestokenizer\n",
        "! pip install SentencePiece"
      ],
      "id": "isQllvdnBNjy",
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.2.0)\n",
            "Collecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 3.7 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 36.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 35.2 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 41.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.19 pyyaml-5.4.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n",
            "Collecting mosestokenizer\n",
            "  Downloading mosestokenizer-1.1.0.tar.gz (37 kB)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from mosestokenizer) (0.6.2)\n",
            "Collecting openfile\n",
            "  Downloading openfile-0.0.7-py3-none-any.whl (2.4 kB)\n",
            "Collecting uctools\n",
            "  Downloading uctools-1.3.0.tar.gz (4.6 kB)\n",
            "Collecting toolwrapper\n",
            "  Downloading toolwrapper-2.1.0.tar.gz (3.2 kB)\n",
            "Building wheels for collected packages: mosestokenizer, toolwrapper, uctools\n",
            "  Building wheel for mosestokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mosestokenizer: filename=mosestokenizer-1.1.0-py3-none-any.whl size=49117 sha256=afa01154e7dc242dfc4287f3de2dc95980affe66336277b2da2936285d53685d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/31/94/fef279382208e85a65c1a7f5c4d0020115477b0af74f296b57\n",
            "  Building wheel for toolwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for toolwrapper: filename=toolwrapper-2.1.0-py3-none-any.whl size=3354 sha256=958bd3b671b269e1e1107585298c778ae66e9258b766df57012b3c1e7de750ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/4f/33/54741ffe08e38ececb1d28068a153729b4fe820bafa0a0691f\n",
            "  Building wheel for uctools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uctools: filename=uctools-1.3.0-py3-none-any.whl size=6163 sha256=e00d53de7516470ef1aa067c7c9f27d0f0c767a715987875f62834937c0ffe54\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/44/e9/914cf8fa71f0141f9314f862538d1218fcf2b94542a0fb7d35\n",
            "Successfully built mosestokenizer toolwrapper uctools\n",
            "Installing collected packages: uctools, toolwrapper, openfile, mosestokenizer\n",
            "Successfully installed mosestokenizer-1.1.0 openfile-0.0.7 toolwrapper-2.1.0 uctools-1.3.0\n",
            "Collecting SentencePiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 8.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: SentencePiece\n",
            "Successfully installed SentencePiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd9bQWlWBE4K"
      },
      "source": [
        "import transformers, mosestokenizer"
      ],
      "id": "bd9bQWlWBE4K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlKxqKom6GON"
      },
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "# Helper function to download data for a language\n",
        "def download(model_name):\n",
        "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "    model = MarianMTModel.from_pretrained(model_name)\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "\n",
        "# download model for English -> Romance\n",
        "#tmp_lang_tokenizer, tmp_lang_model = download('Helsinki-NLP/opus-mt-en-ROMANCE')\n",
        "# download model for Romance -> English\n",
        "#src_lang_tokenizer, src_lang_model = download('Helsinki-NLP/opus-mt-ROMANCE-en')\n",
        "\n",
        "\n",
        "\n",
        "def translate(texts, model, tokenizer, language):\n",
        "    \"\"\"Translate texts into a target language\"\"\"\n",
        "    # Format the text as expected by the model\n",
        "    formatter_fn = lambda txt: f\"{txt}\" if language == \"en\" else f\">>{language}<< {txt}\"\n",
        "    original_texts = [formatter_fn(txt) for txt in texts]\n",
        "\n",
        "    # Tokenize (text to tokens)\n",
        "    tokens = tokenizer.prepare_seq2seq_batch(original_texts)\n",
        "\n",
        "    # Translate\n",
        "    translated = model.generate(**tokens)\n",
        "\n",
        "    # Decode (tokens to text)\n",
        "    translated_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "\n",
        "    return translated_texts\n",
        "\n",
        "\n",
        "def back_translate(texts, language_src, language_dst):\n",
        "    \"\"\"Implements back translation\"\"\"\n",
        "    # Translate from source to target language\n",
        "    translated = translate(texts, tmp_lang_model, tmp_lang_tokenizer, language_dst)\n",
        "\n",
        "    # Translate from target language back to source language\n",
        "    back_translated = translate(translated, src_lang_model, src_lang_tokenizer, language_src)\n",
        "\n",
        "    return back_translated\n",
        "\n",
        "\n"
      ],
      "id": "zlKxqKom6GON",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "1TROFkqPCrox",
        "outputId": "7a1a18c5-2e4a-4078-d68d-25aff2a186ae"
      },
      "source": [
        "model_name = 'Helsinki-NLP/opus-mt-aav-en'\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "tokenizer.supported_language_codes"
      ],
      "id": "1TROFkqPCrox",
      "execution_count": null,
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-02880439e212>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Helsinki-NLP/opus-mt-aav-en'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMarianTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupported_language_codes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'supported_language_codes'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "C6SAY2P5DvRv",
        "outputId": "e2076b11-2e16-4921-fe61-d931ff67274d"
      },
      "source": [
        "language_code_list =df_train_dev['label'].unique\n",
        "\n",
        "def get_one_language_data(df, n):\n",
        "  #get the n'th set of language from df_train_dev\n",
        "  df_one_lang=df[df['label'].isin([language_code_list[n]])]\n",
        "  return df_one_lang\n",
        "\n",
        "def generate_new_rows(df_one_lang):\n",
        "  lable=df_one_lang['label']\n",
        "  df_one_lang_new = pd.DataFrame(columns=['text','label'])\n",
        "  for rows in df_one_lang.itterows():\n",
        "    new_lang = back_translate(df_one_lang['text'],label,'en')\n",
        "    df_one_lang_new.append({'text':new_lang,'label'label})\n",
        "    df_one_lang=df_one_lang.append(df_one_lang_new)\n",
        "  return df_one_lang\n",
        "\n",
        "def augdata_by_back_translation(df):\n",
        "  df_new = pd.DataFrame(columns=['text','label'])\n",
        "  for n in range(0,len(language_code_list)):\n",
        "    df_one_lang_new = generate_new_rows(get_one_language_data(df,n))\n",
        "    df_new = df_new.append(df_one_lang_new)\n",
        "  return df_new\n",
        "\n"
      ],
      "id": "C6SAY2P5DvRv",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>يا من أناديها ويخنقني البكاء  ويكاد صمت الدمع ...</td>\n",
              "      <td>ar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>فيه فرق بين اهل غزة اللى مطحونين من ناحيتين وب...</td>\n",
              "      <td>ar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ﻋﻦ ﺍﻟﻠﺤﻈﺔ اﻟﺤﻠﻮﺓﺓ ﺍﻟﻠﻲ ﺑﺘﻐﻤﺾ ﻓﻴﻬﺎ ﻋﻴﻨﻴﻚ ﺑﺘﻔﻜﺮ ...</td>\n",
              "      <td>ar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>يا ابو سلو عرفتني</td>\n",
              "      <td>ar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ب50 ريال أكفل معتمر في رمضان ، ولك بإذن الله م...</td>\n",
              "      <td>ar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52670</th>\n",
              "      <td>其實我很想問，噗浪怎麼鎖回應然後又開啟的#### http://t.co/sQBh6jmVoW</td>\n",
              "      <td>zh-TW</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52671</th>\n",
              "      <td>我喜歡一部 @YouTube 影片 http://t.co/Kouv0xaEos FIGHT...</td>\n",
              "      <td>zh-TW</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52672</th>\n",
              "      <td>一分鐘世界盃！http://t.co/yEvaMrp7ki</td>\n",
              "      <td>zh-TW</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52673</th>\n",
              "      <td>{CWB} 桃園縣 一週天氣預報(07/27 05:00發布): 07/27 白天 溫度:2...</td>\n",
              "      <td>zh-TW</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52674</th>\n",
              "      <td>這個週末兩天都跟 Navel 很有緣...</td>\n",
              "      <td>zh-TW</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>52675 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   tweet  label\n",
              "0      يا من أناديها ويخنقني البكاء  ويكاد صمت الدمع ...     ar\n",
              "1      فيه فرق بين اهل غزة اللى مطحونين من ناحيتين وب...     ar\n",
              "2      ﻋﻦ ﺍﻟﻠﺤﻈﺔ اﻟﺤﻠﻮﺓﺓ ﺍﻟﻠﻲ ﺑﺘﻐﻤﺾ ﻓﻴﻬﺎ ﻋﻴﻨﻴﻚ ﺑﺘﻔﻜﺮ ...     ar\n",
              "3                                      يا ابو سلو عرفتني     ar\n",
              "4      ب50 ريال أكفل معتمر في رمضان ، ولك بإذن الله م...     ar\n",
              "...                                                  ...    ...\n",
              "52670    其實我很想問，噗浪怎麼鎖回應然後又開啟的#### http://t.co/sQBh6jmVoW  zh-TW\n",
              "52671  我喜歡一部 @YouTube 影片 http://t.co/Kouv0xaEos FIGHT...  zh-TW\n",
              "52672                      一分鐘世界盃！http://t.co/yEvaMrp7ki  zh-TW\n",
              "52673  {CWB} 桃園縣 一週天氣預報(07/27 05:00發布): 07/27 白天 溫度:2...  zh-TW\n",
              "52674                              這個週末兩天都跟 Navel 很有緣...  zh-TW\n",
              "\n",
              "[52675 rows x 2 columns]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSFOzsETf-1l"
      },
      "source": [
        "![back_translation.png](attachment:back_translation.png)"
      ],
      "id": "qSFOzsETf-1l"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEYYB4oLf-1l"
      },
      "source": [
        ""
      ],
      "id": "BEYYB4oLf-1l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w203pWaLQHc"
      },
      "source": [
        "## 2.2 Bigram flipping\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "-w203pWaLQHc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaGC3PJqMitW"
      },
      "source": [
        "def augdata_by_bigram_flipping(df):\n",
        "\n",
        "  return df_new"
      ],
      "id": "oaGC3PJqMitW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtt_ulLmMQsy"
      },
      "source": [
        "##  2.3 Replacing Entities"
      ],
      "id": "dtt_ulLmMQsy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgcHgHxNMWgv"
      },
      "source": [
        "def augdata_by_replacing_entities(df):\n",
        "\n",
        "  return df_new"
      ],
      "id": "NgcHgHxNMWgv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_FysLekMYC-"
      },
      "source": [
        ""
      ],
      "id": "H_FysLekMYC-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN4Yixr4MYdS"
      },
      "source": [
        ""
      ],
      "id": "jN4Yixr4MYdS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxvhXe7FMm_O"
      },
      "source": [
        "# 5.Pre-Processing"
      ],
      "id": "PxvhXe7FMm_O"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOPUlDj0MxYp"
      },
      "source": [
        ""
      ],
      "id": "iOPUlDj0MxYp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vurheEapMyML"
      },
      "source": [
        "# 6.Feature Engineering\n"
      ],
      "id": "vurheEapMyML"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr05cQodnyyT"
      },
      "source": [
        "def detect_alphabet(df, column):\n",
        "  df['alphabet'] = df[column].apply(lambda tweet: list(ad.detect_alphabet(tweet)))\n",
        "  mlb = MultiLabelBinarizer()\n",
        "  df = df.join(pd.DataFrame(mlb.fit_transform(df.pop('alphabet')),\n",
        "                            columns=mlb.classes_,\n",
        "                            index=df.index))\n",
        "  return df"
      ],
      "id": "Hr05cQodnyyT",
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBa6fVtbnshU"
      },
      "source": [
        "dataset_w_alphabet = detect_alphabet(dataset, 'tweet')"
      ],
      "id": "sBa6fVtbnshU",
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "id": "I4CgbGlVof9V",
        "outputId": "fe76f8b4-0cd7-43e9-86b6-b6501b499950"
      },
      "source": [
        "dataset_w_alphabet.head(10)"
      ],
      "id": "I4CgbGlVof9V",
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>ANGSTROM</th>\n",
              "      <th>ARABIC</th>\n",
              "      <th>ARMENIAN</th>\n",
              "      <th>BENGALI</th>\n",
              "      <th>BLACK-LETTER</th>\n",
              "      <th>BOPOMOFO</th>\n",
              "      <th>CANADIAN</th>\n",
              "      <th>CARON</th>\n",
              "      <th>CJK</th>\n",
              "      <th>CYRILLIC</th>\n",
              "      <th>DEVANAGARI</th>\n",
              "      <th>FEMININE</th>\n",
              "      <th>FULLWIDTH</th>\n",
              "      <th>GEORGIAN</th>\n",
              "      <th>GREEK</th>\n",
              "      <th>GUJARATI</th>\n",
              "      <th>GURMUKHI</th>\n",
              "      <th>HALFWIDTH</th>\n",
              "      <th>HANGUL</th>\n",
              "      <th>HEBREW</th>\n",
              "      <th>HIRAGANA</th>\n",
              "      <th>IDEOGRAPHIC</th>\n",
              "      <th>KANNADA</th>\n",
              "      <th>KATAKANA</th>\n",
              "      <th>KATAKANA-HIRAGANA</th>\n",
              "      <th>KHMER</th>\n",
              "      <th>LAO</th>\n",
              "      <th>LATIN</th>\n",
              "      <th>MASCULINE</th>\n",
              "      <th>MICRO</th>\n",
              "      <th>MODIFIER</th>\n",
              "      <th>ORIYA</th>\n",
              "      <th>SCRIPT</th>\n",
              "      <th>SINHALA</th>\n",
              "      <th>TAMIL</th>\n",
              "      <th>TELUGU</th>\n",
              "      <th>THAI</th>\n",
              "      <th>TIBETAN</th>\n",
              "      <th>YI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>44082</td>\n",
              "      <td>Yeah , I Can't Do This !</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>32973</td>\n",
              "      <td>The Hangover 3 is a yawn</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38888</td>\n",
              "      <td>بغى يروح غلطة كاميرا خفيه  http://t.co/lpnZX8k...</td>\n",
              "      <td>ar</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56483</td>\n",
              "      <td>I've harvested 60 of food! http://t.co/9VtaeM2...</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>58096</td>\n",
              "      <td>Lol, it's because of Luis Suarez\"@Footy_Jokes:...</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>34504</td>\n",
              "      <td>Pierre Edmonds meeting @HughesFT @TheLondonBea...</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>47922</td>\n",
              "      <td>cuando vienes a puntaa? — cuando aga un poco d...</td>\n",
              "      <td>es</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>43694</td>\n",
              "      <td>من باب راحة الصدر ؟  استغفر الله العظيم و اتو...</td>\n",
              "      <td>ar</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>65290</td>\n",
              "      <td>@Paulcraig20 u r a liar</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>23591</td>\n",
              "      <td>Brito pone parte de lo que robó con #Ciccone. ...</td>\n",
              "      <td>es</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index                                              tweet label  ANGSTROM  ARABIC  ARMENIAN  BENGALI  BLACK-LETTER  BOPOMOFO  CANADIAN  CARON  CJK  CYRILLIC  DEVANAGARI  FEMININE  FULLWIDTH  GEORGIAN  GREEK  GUJARATI  GURMUKHI  HALFWIDTH  HANGUL  HEBREW  HIRAGANA  IDEOGRAPHIC  KANNADA  KATAKANA  KATAKANA-HIRAGANA  KHMER  LAO  LATIN  MASCULINE  MICRO  MODIFIER  ORIYA  SCRIPT  SINHALA  TAMIL  TELUGU  THAI  TIBETAN  YI\n",
              "0  44082                           Yeah , I Can't Do This !    en         0       0         0        0             0         0         0      0    0         0           0         0          0         0      0         0         0          0       0       0         0            0        0         0                  0      0    0      1          0      0         0      0       0        0      0       0     0        0   0\n",
              "1  32973                           The Hangover 3 is a yawn    en         0       0         0        0             0         0         0      0    0         0           0         0          0         0      0         0         0          0       0       0         0            0        0         0                  0      0    0      1          0      0         0      0       0        0      0       0     0        0   0\n",
              "2  38888  بغى يروح غلطة كاميرا خفيه  http://t.co/lpnZX8k...    ar         0       1         0        0             0         0         0      0    0         0           0         0          0         0      0         0         0          0       0       0         0            0        0         0                  0      0    0      1          0      0         0      0       0        0      0       0     0        0   0\n",
              "3  56483  I've harvested 60 of food! http://t.co/9VtaeM2...    en         0       0         0        0             0         0         0      0    0         0           0         0          0         0      0         0         0          0       0       0         0            0        0         0                  0      0    0      1          0      0         0      0       0        0      0       0     0        0   0\n",
              "4  58096  Lol, it's because of Luis Suarez\"@Footy_Jokes:...    en         0       0         0        0             0         0         0      0    0         0           0         0          0         0      0         0         0          0       0       0         0            0        0         0                  0      0    0      1          0      0         0      0       0        0      0       0     0        0   0\n",
              "5  34504  Pierre Edmonds meeting @HughesFT @TheLondonBea...    en         0       0         0        0             0         0         0      0    0         0           0         0          0         0      0         0         0          0       0       0         0            0        0         0                  0      0    0      1          0      0         0      0       0        0      0       0     0        0   0\n",
              "6  47922  cuando vienes a puntaa? — cuando aga un poco d...    es         0       0         0        0             0         0         0      0    0         0           0         0          0         0      0         0         0          0       0       0         0            0        0         0                  0      0    0      1          0      0         0      0       0        0      0       0     0        0   0\n",
              "7  43694   من باب راحة الصدر ؟  استغفر الله العظيم و اتو...    ar         0       1         0        0             0         0         0      0    0         0           0         0          0         0      0         0         0          0       0       0         0            0        0         0                  0      0    0      0          0      0         0      0       0        0      0       0     0        0   0\n",
              "8  65290                            @Paulcraig20 u r a liar    en         0       0         0        0             0         0         0      0    0         0           0         0          0         0      0         0         0          0       0       0         0            0        0         0                  0      0    0      1          0      0         0      0       0        0      0       0     0        0   0\n",
              "9  23591  Brito pone parte de lo que robó con #Ciccone. ...    es         0       0         0        0             0         0         0      0    0         0           0         0          0         0      0         0         0          0       0       0         0            0        0         0                  0      0    0      1          0      0         0      0       0        0      0       0     0        0   0"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyc0hYa-M3cV"
      },
      "source": [
        "# 7.Modeling "
      ],
      "id": "Tyc0hYa-M3cV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmqZ0wkZxjcd"
      },
      "source": [
        "def train_val_test_split(X, y, test_size=0.3, val_size=1/3):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=True)\n",
        "  X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=val_size, shuffle=True)\n",
        "  return X_train, X_val, X_test, y_train, y_val, y_test"
      ],
      "id": "jmqZ0wkZxjcd",
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W17WU1oNx8tZ"
      },
      "source": [
        "X = dataset_w_alphabet.drop([TARGET_COLUMN, 'tweet'], axis=1)\n",
        "y = dataset_w_alphabet[TARGET_COLUMN]\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X, y)"
      ],
      "id": "W17WU1oNx8tZ",
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3VVKnZsM60K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcfa9d76-d729-4075-c679-709632ce29ac"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "sgdc = SGDClassifier(max_iter=1000, tol=0.01, verbose=True)\n",
        "sgdc.fit(X_train, y_train)"
      ],
      "id": "D3VVKnZsM60K",
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Norm: 251.88, NNZs: 11, Bias: -7.589198, T: 1246509, Avg. loss: 887.820400\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 199.53, NNZs: 12, Bias: -7.573275, T: 1292676, Avg. loss: 955.522594\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 2.62, NNZs: 12, Bias: -7.558061, T: 1338843, Avg. loss: 796.316823\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 210.87, NNZs: 13, Bias: -7.558087, T: 1385010, Avg. loss: 863.333311\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 52.78, NNZs: 13, Bias: -7.543950, T: 1431177, Avg. loss: 739.633082\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 226.55, NNZs: 13, Bias: -7.530154, T: 1477344, Avg. loss: 880.948785\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 1.70, NNZs: 13, Bias: -7.529892, T: 1523511, Avg. loss: 690.276900\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 174.12, NNZs: 13, Bias: -7.542870, T: 1569678, Avg. loss: 610.384634\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 114.59, NNZs: 13, Bias: -7.530515, T: 1615845, Avg. loss: 723.169123\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 214.64, NNZs: 13, Bias: -7.518274, T: 1662012, Avg. loss: 766.071075\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 218.57, NNZs: 13, Bias: -7.506452, T: 1708179, Avg. loss: 667.575955\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 8.18, NNZs: 13, Bias: -7.489019, T: 1754346, Avg. loss: 692.662528\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 137.96, NNZs: 13, Bias: -7.500320, T: 1800513, Avg. loss: 589.826374\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 49.51, NNZs: 13, Bias: -7.489379, T: 1846680, Avg. loss: 649.028920\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 24.20, NNZs: 13, Bias: -7.484033, T: 1892847, Avg. loss: 497.046848\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 151.59, NNZs: 13, Bias: -7.489195, T: 1939014, Avg. loss: 556.415422\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 121.43, NNZs: 13, Bias: -7.473964, T: 1985181, Avg. loss: 696.665821\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 1.54, NNZs: 13, Bias: -7.463960, T: 2031348, Avg. loss: 497.261603\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 5.03, NNZs: 13, Bias: -7.463984, T: 2077515, Avg. loss: 499.987042\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 52.00, NNZs: 13, Bias: -7.454471, T: 2123682, Avg. loss: 601.531369\n",
            "Total training time: 0.30 seconds.\n",
            "Convergence after 46 epochs took 0.30 seconds\n",
            "-- Epoch 1\n",
            "Norm: 9774.63, NNZs: 8, Bias: -8.924238, T: 46167, Avg. loss: 101580.766074\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 915.86, NNZs: 8, Bias: -9.026444, T: 92334, Avg. loss: 38773.696090\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 1950.54, NNZs: 8, Bias: -9.015529, T: 138501, Avg. loss: 24465.273713\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1027.81, NNZs: 9, Bias: -8.837598, T: 184668, Avg. loss: 20761.200565\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 226.94, NNZs: 9, Bias: -8.940872, T: 230835, Avg. loss: 12729.881394\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 361.41, NNZs: 9, Bias: -8.900245, T: 277002, Avg. loss: 11746.559843\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 594.29, NNZs: 12, Bias: -8.902176, T: 323169, Avg. loss: 10315.090305\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 95.96, NNZs: 12, Bias: -8.929412, T: 369336, Avg. loss: 8103.573595\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 547.88, NNZs: 12, Bias: -8.953002, T: 415503, Avg. loss: 7359.317623\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 855.69, NNZs: 14, Bias: -8.996837, T: 461670, Avg. loss: 5914.742731\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 153.11, NNZs: 14, Bias: -9.019369, T: 507837, Avg. loss: 5803.709929\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 508.31, NNZs: 14, Bias: -9.075695, T: 554004, Avg. loss: 5138.126327\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 750.66, NNZs: 14, Bias: -9.147037, T: 600171, Avg. loss: 4812.855709\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 809.13, NNZs: 14, Bias: -9.162007, T: 646338, Avg. loss: 4500.384455\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 324.96, NNZs: 14, Bias: -9.162977, T: 692505, Avg. loss: 4398.856350\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 321.93, NNZs: 14, Bias: -9.205048, T: 738672, Avg. loss: 3644.411345\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 201.36, NNZs: 14, Bias: -9.205407, T: 784839, Avg. loss: 4149.182900\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 86.85, NNZs: 14, Bias: -9.243087, T: 831006, Avg. loss: 3495.244629\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 179.17, NNZs: 14, Bias: -9.242729, T: 877173, Avg. loss: 3485.509825\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 321.29, NNZs: 14, Bias: -9.254128, T: 923340, Avg. loss: 3129.780438\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 256.12, NNZs: 14, Bias: -9.243686, T: 969507, Avg. loss: 3354.875097\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 153.00, NNZs: 14, Bias: -9.243542, T: 1015674, Avg. loss: 2821.729652\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 151.47, NNZs: 14, Bias: -9.262863, T: 1061841, Avg. loss: 2746.216603\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 223.57, NNZs: 14, Bias: -9.244607, T: 1108008, Avg. loss: 2897.770821\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 428.38, NNZs: 14, Bias: -9.288568, T: 1154175, Avg. loss: 2276.688300\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 197.20, NNZs: 14, Bias: -9.297089, T: 1200342, Avg. loss: 2392.921429\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 253.42, NNZs: 14, Bias: -9.304776, T: 1246509, Avg. loss: 2286.655509\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 233.34, NNZs: 14, Bias: -9.289048, T: 1292676, Avg. loss: 2509.434533\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 22.51, NNZs: 14, Bias: -9.311981, T: 1338843, Avg. loss: 1834.049090\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 307.20, NNZs: 14, Bias: -9.333948, T: 1385010, Avg. loss: 2138.630258\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 114.00, NNZs: 14, Bias: -9.319914, T: 1431177, Avg. loss: 2216.947906\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 133.06, NNZs: 15, Bias: -9.333392, T: 1477344, Avg. loss: 1977.409766\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 60.25, NNZs: 15, Bias: -9.340445, T: 1523511, Avg. loss: 1923.272596\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 181.19, NNZs: 15, Bias: -9.346643, T: 1569678, Avg. loss: 1780.739437\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 119.10, NNZs: 15, Bias: -9.359068, T: 1615845, Avg. loss: 1705.867836\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 189.07, NNZs: 15, Bias: -9.365105, T: 1662012, Avg. loss: 1710.746595\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 164.23, NNZs: 15, Bias: -9.370908, T: 1708179, Avg. loss: 1571.937480\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 165.09, NNZs: 15, Bias: -9.370802, T: 1754346, Avg. loss: 1638.677228\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 124.64, NNZs: 15, Bias: -9.387377, T: 1800513, Avg. loss: 1585.966665\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 7.53, NNZs: 15, Bias: -9.387385, T: 1846680, Avg. loss: 1582.486957\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 238.61, NNZs: 15, Bias: -9.398113, T: 1892847, Avg. loss: 1489.760356\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 262.68, NNZs: 15, Bias: -9.403106, T: 1939014, Avg. loss: 1533.290135\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 122.44, NNZs: 15, Bias: -9.397959, T: 1985181, Avg. loss: 1519.351168\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 125.16, NNZs: 15, Bias: -9.402821, T: 2031348, Avg. loss: 1339.831888\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 32.07, NNZs: 15, Bias: -9.402852, T: 2077515, Avg. loss: 1368.815533\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 39.12, NNZs: 15, Bias: -9.417271, T: 2123682, Avg. loss: 1343.401261\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 45.41, NNZs: 16, Bias: -9.426630, T: 2169849, Avg. loss: 1328.056890\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 11.07, NNZs: 16, Bias: -9.431098, T: 2216016, Avg. loss: 1269.210164\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 15.35, NNZs: 16, Bias: -9.444523, T: 2262183, Avg. loss: 1115.061204\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 34.08, NNZs: 16, Bias: -9.466471, T: 2308350, Avg. loss: 1065.258873\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 20.20, NNZs: 16, Bias: -9.462164, T: 2354517, Avg. loss: 1325.957237\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 153.36, NNZs: 16, Bias: -9.474809, T: 2400684, Avg. loss: 1151.394919\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 37.94, NNZs: 16, Bias: -9.483020, T: 2446851, Avg. loss: 1090.861148\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 130.53, NNZs: 16, Bias: -9.495184, T: 2493018, Avg. loss: 1085.489009\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 49.84, NNZs: 16, Bias: -9.510974, T: 2539185, Avg. loss: 963.746413\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 10.08, NNZs: 16, Bias: -9.510914, T: 2585352, Avg. loss: 1117.841660\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 15.02, NNZs: 16, Bias: -9.526245, T: 2631519, Avg. loss: 1016.058126\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 19.80, NNZs: 16, Bias: -9.529992, T: 2677686, Avg. loss: 1144.219669\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 114.50, NNZs: 16, Bias: -9.544814, T: 2723853, Avg. loss: 990.269112\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 34.48, NNZs: 17, Bias: -9.548401, T: 2770020, Avg. loss: 998.151831\n",
            "Total training time: 0.39 seconds.\n",
            "Convergence after 60 epochs took 0.39 seconds\n",
            "-- Epoch 1\n",
            "Norm: 9808.12, NNZs: 2, Bias: -9.999975, T: 46167, Avg. loss: 23379.885689\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 4853.91, NNZs: 2, Bias: -9.999971, T: 92334, Avg. loss: 9573.421957\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 1622.22, NNZs: 2, Bias: -9.999970, T: 138501, Avg. loss: 4292.546646\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 2424.23, NNZs: 2, Bias: -10.060654, T: 184668, Avg. loss: 2924.863031\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1470.62, NNZs: 2, Bias: -10.060654, T: 230835, Avg. loss: 2056.079674\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1359.35, NNZs: 2, Bias: -10.060654, T: 277002, Avg. loss: 1736.997960\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 108.37, NNZs: 2, Bias: -10.060654, T: 323169, Avg. loss: 1319.184255\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 484.67, NNZs: 2, Bias: -10.060654, T: 369336, Avg. loss: 1829.047310\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 183.62, NNZs: 2, Bias: -10.060654, T: 415503, Avg. loss: 1054.341396\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 20.40, NNZs: 2, Bias: -10.060654, T: 461670, Avg. loss: 907.708729\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 137.98, NNZs: 2, Bias: -10.080520, T: 507837, Avg. loss: 751.314905\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 441.84, NNZs: 2, Bias: -10.080520, T: 554004, Avg. loss: 1181.490661\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 106.76, NNZs: 2, Bias: -10.080520, T: 600171, Avg. loss: 633.937124\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 399.00, NNZs: 2, Bias: -10.096393, T: 646338, Avg. loss: 633.481812\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 163.52, NNZs: 2, Bias: -10.096393, T: 692505, Avg. loss: 613.984513\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 488.57, NNZs: 2, Bias: -10.110108, T: 738672, Avg. loss: 520.644069\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 241.93, NNZs: 2, Bias: -10.110108, T: 784839, Avg. loss: 572.393556\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 437.18, NNZs: 5, Bias: -10.110108, T: 831006, Avg. loss: 688.366887\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 452.03, NNZs: 6, Bias: -10.110108, T: 877173, Avg. loss: 552.313770\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 200.34, NNZs: 6, Bias: -10.110108, T: 923340, Avg. loss: 480.343382\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 333.17, NNZs: 6, Bias: -10.120742, T: 969507, Avg. loss: 520.365168\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 283.13, NNZs: 6, Bias: -10.120742, T: 1015674, Avg. loss: 456.794594\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 243.03, NNZs: 6, Bias: -10.130278, T: 1061841, Avg. loss: 344.680688\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 81.17, NNZs: 6, Bias: -10.130278, T: 1108008, Avg. loss: 353.716555\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 67.02, NNZs: 7, Bias: -10.156933, T: 1154175, Avg. loss: 289.267754\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 415.09, NNZs: 7, Bias: -10.165316, T: 1200342, Avg. loss: 373.657148\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 28.87, NNZs: 7, Bias: -10.157259, T: 1246509, Avg. loss: 402.632424\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 37.31, NNZs: 9, Bias: -10.173183, T: 1292676, Avg. loss: 263.393773\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 339.97, NNZs: 9, Bias: -10.180677, T: 1338843, Avg. loss: 336.385891\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 106.20, NNZs: 10, Bias: -10.180677, T: 1385010, Avg. loss: 333.551712\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 27.89, NNZs: 10, Bias: -10.187727, T: 1431177, Avg. loss: 272.447544\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 219.51, NNZs: 11, Bias: -10.194649, T: 1477344, Avg. loss: 345.293946\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 288.48, NNZs: 11, Bias: -10.194649, T: 1523511, Avg. loss: 334.083301\n",
            "Total training time: 0.22 seconds.\n",
            "Convergence after 33 epochs took 0.22 seconds\n",
            "-- Epoch 1\n",
            "Norm: 12621.43, NNZs: 15, Bias: 9.749977, T: 46167, Avg. loss: 8282534.220654\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 123.77, NNZs: 16, Bias: 13.821598, T: 92334, Avg. loss: 1728486.151922\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 2393.90, NNZs: 16, Bias: 12.535823, T: 138501, Avg. loss: 980213.583623\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1772.84, NNZs: 17, Bias: 12.189052, T: 184668, Avg. loss: 708228.176290\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1267.47, NNZs: 17, Bias: 11.750495, T: 230835, Avg. loss: 546440.220927\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1361.89, NNZs: 18, Bias: 13.008495, T: 277002, Avg. loss: 460815.332921\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 148.41, NNZs: 19, Bias: 12.578426, T: 323169, Avg. loss: 374925.461738\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 244.94, NNZs: 19, Bias: 13.079879, T: 369336, Avg. loss: 338414.568042\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 410.10, NNZs: 19, Bias: 13.626987, T: 415503, Avg. loss: 294831.612868\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 87.06, NNZs: 19, Bias: 13.776355, T: 461670, Avg. loss: 260139.291909\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 127.48, NNZs: 19, Bias: 13.941701, T: 507837, Avg. loss: 237723.841150\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 525.12, NNZs: 19, Bias: 13.877910, T: 554004, Avg. loss: 211366.364659\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 102.36, NNZs: 22, Bias: 14.089673, T: 600171, Avg. loss: 200629.473295\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 467.93, NNZs: 22, Bias: 14.138833, T: 646338, Avg. loss: 183154.340525\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 369.81, NNZs: 22, Bias: 14.353337, T: 692505, Avg. loss: 172657.389522\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 404.12, NNZs: 22, Bias: 14.412102, T: 738672, Avg. loss: 159751.012670\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 620.14, NNZs: 22, Bias: 14.233079, T: 784839, Avg. loss: 149271.717718\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 465.04, NNZs: 23, Bias: 14.308431, T: 831006, Avg. loss: 141457.950538\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 131.13, NNZs: 23, Bias: 14.201798, T: 877173, Avg. loss: 135035.020080\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 296.16, NNZs: 23, Bias: 14.282627, T: 923340, Avg. loss: 127783.292915\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 352.81, NNZs: 23, Bias: 14.451259, T: 969507, Avg. loss: 122753.569231\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 299.79, NNZs: 23, Bias: 14.552624, T: 1015674, Avg. loss: 115807.363137\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 464.71, NNZs: 23, Bias: 14.758195, T: 1061841, Avg. loss: 109490.556274\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 122.89, NNZs: 24, Bias: 14.797615, T: 1108008, Avg. loss: 104063.828585\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 208.70, NNZs: 24, Bias: 14.709168, T: 1154175, Avg. loss: 100484.172004\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 194.88, NNZs: 24, Bias: 14.676410, T: 1200342, Avg. loss: 95812.990842\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 337.19, NNZs: 25, Bias: 14.628014, T: 1246509, Avg. loss: 91614.165485\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 87.98, NNZs: 25, Bias: 14.456394, T: 1292676, Avg. loss: 89936.000747\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 231.10, NNZs: 25, Bias: 14.477509, T: 1338843, Avg. loss: 88173.374831\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 94.74, NNZs: 25, Bias: 14.456581, T: 1385010, Avg. loss: 84413.354597\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 143.98, NNZs: 26, Bias: 14.476859, T: 1431177, Avg. loss: 81001.858715\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 86.10, NNZs: 26, Bias: 14.491107, T: 1477344, Avg. loss: 79551.007036\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 300.88, NNZs: 26, Bias: 14.538881, T: 1523511, Avg. loss: 77285.190009\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 100.15, NNZs: 26, Bias: 14.524357, T: 1569678, Avg. loss: 74364.484756\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 281.39, NNZs: 26, Bias: 14.530322, T: 1615845, Avg. loss: 71446.306079\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 86.54, NNZs: 26, Bias: 14.487274, T: 1662012, Avg. loss: 69773.016752\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 315.88, NNZs: 26, Bias: 14.474495, T: 1708179, Avg. loss: 67014.555744\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 205.35, NNZs: 26, Bias: 14.543721, T: 1754346, Avg. loss: 66737.612056\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 88.80, NNZs: 26, Bias: 14.598948, T: 1800513, Avg. loss: 64295.467726\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 132.35, NNZs: 27, Bias: 14.632241, T: 1846680, Avg. loss: 62271.012312\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 162.21, NNZs: 29, Bias: 14.803131, T: 1892847, Avg. loss: 62465.439718\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 117.33, NNZs: 29, Bias: 14.844959, T: 1939014, Avg. loss: 60464.367453\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 110.40, NNZs: 29, Bias: 14.875101, T: 1985181, Avg. loss: 59000.084537\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 255.87, NNZs: 29, Bias: 14.915424, T: 2031348, Avg. loss: 56844.635407\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 99.17, NNZs: 29, Bias: 14.828224, T: 2077515, Avg. loss: 53693.014998\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 211.64, NNZs: 29, Bias: 14.894862, T: 2123682, Avg. loss: 55093.320438\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 157.44, NNZs: 29, Bias: 14.870687, T: 2169849, Avg. loss: 53210.105146\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 200.33, NNZs: 30, Bias: 14.983832, T: 2216016, Avg. loss: 53641.618977\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 94.29, NNZs: 30, Bias: 14.992460, T: 2262183, Avg. loss: 50635.273025\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 164.01, NNZs: 30, Bias: 15.106380, T: 2308350, Avg. loss: 51499.949174\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 85.73, NNZs: 30, Bias: 15.123565, T: 2354517, Avg. loss: 49767.197401\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 86.01, NNZs: 30, Bias: 15.216086, T: 2400684, Avg. loss: 48720.844690\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 110.35, NNZs: 30, Bias: 15.191429, T: 2446851, Avg. loss: 47340.930910\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 220.90, NNZs: 30, Bias: 15.288410, T: 2493018, Avg. loss: 46573.606630\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 136.15, NNZs: 30, Bias: 15.352254, T: 2539185, Avg. loss: 46230.785176\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 141.75, NNZs: 30, Bias: 15.426225, T: 2585352, Avg. loss: 45649.413133\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 197.17, NNZs: 30, Bias: 15.390684, T: 2631519, Avg. loss: 43849.740781\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 102.96, NNZs: 30, Bias: 15.495713, T: 2677686, Avg. loss: 43145.222311\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 186.88, NNZs: 30, Bias: 15.524797, T: 2723853, Avg. loss: 41536.159251\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 86.92, NNZs: 30, Bias: 15.590473, T: 2770020, Avg. loss: 41969.653507\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 203.89, NNZs: 30, Bias: 15.640557, T: 2816187, Avg. loss: 40879.890224\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 104.55, NNZs: 30, Bias: 15.732289, T: 2862354, Avg. loss: 41044.889007\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 117.42, NNZs: 30, Bias: 15.694270, T: 2908521, Avg. loss: 39939.589578\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 109.96, NNZs: 30, Bias: 15.731990, T: 2954688, Avg. loss: 39133.236026\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 114.31, NNZs: 31, Bias: 15.769092, T: 3000855, Avg. loss: 38706.423441\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 113.93, NNZs: 31, Bias: 15.759009, T: 3047022, Avg. loss: 38013.587838\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 120.51, NNZs: 31, Bias: 15.775256, T: 3093189, Avg. loss: 37426.241902\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 87.41, NNZs: 32, Bias: 15.781146, T: 3139356, Avg. loss: 36952.384361\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 85.68, NNZs: 32, Bias: 15.742968, T: 3185523, Avg. loss: 36360.014791\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 134.85, NNZs: 32, Bias: 15.805339, T: 3231690, Avg. loss: 36304.494483\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 86.20, NNZs: 32, Bias: 15.771135, T: 3277857, Avg. loss: 34887.261501\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 87.62, NNZs: 32, Bias: 15.804208, T: 3324024, Avg. loss: 34526.483312\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 159.48, NNZs: 32, Bias: 15.807173, T: 3370191, Avg. loss: 34371.522129\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 86.13, NNZs: 32, Bias: 15.765567, T: 3416358, Avg. loss: 33495.381439\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 136.79, NNZs: 32, Bias: 15.719030, T: 3462525, Avg. loss: 32993.250142\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 148.02, NNZs: 32, Bias: 15.807649, T: 3508692, Avg. loss: 33659.532722\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 111.36, NNZs: 32, Bias: 15.838814, T: 3554859, Avg. loss: 32963.602138\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 88.19, NNZs: 33, Bias: 15.899940, T: 3601026, Avg. loss: 32337.315710\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 93.58, NNZs: 33, Bias: 15.963597, T: 3647193, Avg. loss: 32781.127839\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 89.71, NNZs: 33, Bias: 15.993599, T: 3693360, Avg. loss: 31830.632337\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 113.39, NNZs: 34, Bias: 16.004374, T: 3739527, Avg. loss: 30914.251677\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 87.75, NNZs: 34, Bias: 16.044459, T: 3785694, Avg. loss: 31173.238327\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 85.82, NNZs: 34, Bias: 16.091831, T: 3831861, Avg. loss: 30827.355956\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 132.78, NNZs: 34, Bias: 16.112794, T: 3878028, Avg. loss: 29804.392967\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 88.21, NNZs: 34, Bias: 16.097399, T: 3924195, Avg. loss: 29572.775436\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 86.48, NNZs: 34, Bias: 16.127840, T: 3970362, Avg. loss: 29377.030691\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 114.22, NNZs: 34, Bias: 16.172798, T: 4016529, Avg. loss: 28557.458455\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 138.96, NNZs: 34, Bias: 16.244482, T: 4062696, Avg. loss: 29038.722169\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 94.24, NNZs: 34, Bias: 16.259392, T: 4108863, Avg. loss: 28142.532145\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 88.94, NNZs: 34, Bias: 16.259462, T: 4155030, Avg. loss: 27739.452198\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 140.28, NNZs: 34, Bias: 16.266792, T: 4201197, Avg. loss: 27723.562748\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 87.07, NNZs: 34, Bias: 16.295229, T: 4247364, Avg. loss: 27455.136387\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 149.25, NNZs: 34, Bias: 16.295058, T: 4293531, Avg. loss: 26868.863573\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 93.62, NNZs: 34, Bias: 16.258219, T: 4339698, Avg. loss: 26433.816582\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 85.98, NNZs: 34, Bias: 16.329508, T: 4385865, Avg. loss: 27036.904365\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 129.43, NNZs: 34, Bias: 16.356718, T: 4432032, Avg. loss: 26135.716940\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 86.42, NNZs: 34, Bias: 16.345405, T: 4478199, Avg. loss: 25593.960692\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 104.81, NNZs: 34, Bias: 16.323351, T: 4524366, Avg. loss: 25348.490977\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 100.65, NNZs: 34, Bias: 16.301326, T: 4570533, Avg. loss: 25277.930337\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 97.75, NNZs: 34, Bias: 16.325160, T: 4616700, Avg. loss: 25257.263928\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 119.12, NNZs: 34, Bias: 16.383210, T: 4662867, Avg. loss: 25543.749129\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 89.02, NNZs: 34, Bias: 16.389663, T: 4709034, Avg. loss: 24684.665459\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 86.05, NNZs: 34, Bias: 16.400267, T: 4755201, Avg. loss: 24240.266387\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 127.43, NNZs: 34, Bias: 16.393868, T: 4801368, Avg. loss: 24246.779721\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 88.57, NNZs: 34, Bias: 16.396074, T: 4847535, Avg. loss: 23958.208760\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 95.12, NNZs: 34, Bias: 16.424894, T: 4893702, Avg. loss: 23814.571418\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 135.36, NNZs: 34, Bias: 16.422985, T: 4939869, Avg. loss: 23381.014478\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 88.38, NNZs: 34, Bias: 16.455208, T: 4986036, Avg. loss: 23185.564216\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 103.06, NNZs: 34, Bias: 16.463001, T: 5032203, Avg. loss: 23103.389318\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 103.36, NNZs: 34, Bias: 16.459127, T: 5078370, Avg. loss: 22370.485716\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 103.79, NNZs: 34, Bias: 16.484488, T: 5124537, Avg. loss: 22789.952374\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 117.63, NNZs: 34, Bias: 16.511662, T: 5170704, Avg. loss: 22217.782129\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 151.35, NNZs: 34, Bias: 16.505819, T: 5216871, Avg. loss: 22173.288784\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 91.61, NNZs: 34, Bias: 16.519234, T: 5263038, Avg. loss: 22096.642118\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 95.91, NNZs: 34, Bias: 16.545710, T: 5309205, Avg. loss: 22027.307580\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 92.82, NNZs: 34, Bias: 16.609564, T: 5355372, Avg. loss: 22037.473171\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 95.75, NNZs: 34, Bias: 16.641185, T: 5401539, Avg. loss: 21324.330316\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 90.57, NNZs: 34, Bias: 16.676224, T: 5447706, Avg. loss: 21383.640134\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 117.32, NNZs: 34, Bias: 16.656204, T: 5493873, Avg. loss: 20964.787017\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 102.70, NNZs: 34, Bias: 16.665271, T: 5540040, Avg. loss: 20813.961363\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 90.75, NNZs: 34, Bias: 16.670598, T: 5586207, Avg. loss: 20913.215744\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 90.13, NNZs: 35, Bias: 16.661647, T: 5632374, Avg. loss: 20479.412691\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 89.53, NNZs: 35, Bias: 16.675828, T: 5678541, Avg. loss: 20062.437717\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 118.20, NNZs: 35, Bias: 16.686450, T: 5724708, Avg. loss: 20158.637906\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 131.24, NNZs: 35, Bias: 16.721352, T: 5770875, Avg. loss: 20549.835193\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 96.29, NNZs: 35, Bias: 16.719674, T: 5817042, Avg. loss: 19896.288489\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 89.71, NNZs: 35, Bias: 16.730008, T: 5863209, Avg. loss: 19813.420158\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 86.79, NNZs: 35, Bias: 16.752055, T: 5909376, Avg. loss: 19714.952273\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 129\n",
            "Norm: 88.11, NNZs: 35, Bias: 16.772235, T: 5955543, Avg. loss: 19669.474622\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 130\n",
            "Norm: 101.16, NNZs: 35, Bias: 16.783959, T: 6001710, Avg. loss: 19188.079831\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 131\n",
            "Norm: 96.45, NNZs: 35, Bias: 16.795606, T: 6047877, Avg. loss: 19581.956938\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 132\n",
            "Norm: 96.61, NNZs: 35, Bias: 16.805450, T: 6094044, Avg. loss: 19070.452436\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 133\n",
            "Norm: 85.81, NNZs: 35, Bias: 16.826619, T: 6140211, Avg. loss: 18760.800159\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 134\n",
            "Norm: 86.53, NNZs: 35, Bias: 16.795734, T: 6186378, Avg. loss: 18423.042161\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 135\n",
            "Norm: 89.54, NNZs: 35, Bias: 16.782824, T: 6232545, Avg. loss: 18161.939112\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 136\n",
            "Norm: 85.89, NNZs: 35, Bias: 16.813179, T: 6278712, Avg. loss: 18549.190852\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 137\n",
            "Norm: 95.99, NNZs: 35, Bias: 16.824263, T: 6324879, Avg. loss: 18406.616611\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 138\n",
            "Norm: 105.94, NNZs: 35, Bias: 16.860497, T: 6371046, Avg. loss: 18131.990093\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 139\n",
            "Norm: 88.08, NNZs: 35, Bias: 16.851171, T: 6417213, Avg. loss: 17988.175581\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 140\n",
            "Norm: 113.06, NNZs: 35, Bias: 16.879181, T: 6463380, Avg. loss: 18085.667982\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 141\n",
            "Norm: 106.75, NNZs: 35, Bias: 16.908478, T: 6509547, Avg. loss: 17914.204871\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 142\n",
            "Norm: 103.91, NNZs: 35, Bias: 16.925357, T: 6555714, Avg. loss: 17723.086212\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 143\n",
            "Norm: 108.79, NNZs: 35, Bias: 16.923840, T: 6601881, Avg. loss: 17657.092556\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 144\n",
            "Norm: 86.79, NNZs: 35, Bias: 16.932940, T: 6648048, Avg. loss: 17781.234125\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 145\n",
            "Norm: 86.50, NNZs: 35, Bias: 16.916538, T: 6694215, Avg. loss: 17154.321877\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 146\n",
            "Norm: 115.45, NNZs: 35, Bias: 16.955244, T: 6740382, Avg. loss: 17231.602107\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 147\n",
            "Norm: 88.29, NNZs: 35, Bias: 16.978904, T: 6786549, Avg. loss: 17335.621552\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 148\n",
            "Norm: 87.02, NNZs: 35, Bias: 17.022974, T: 6832716, Avg. loss: 17224.205751\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 149\n",
            "Norm: 89.09, NNZs: 35, Bias: 17.005375, T: 6878883, Avg. loss: 16598.733491\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 150\n",
            "Norm: 100.92, NNZs: 35, Bias: 17.009712, T: 6925050, Avg. loss: 16610.528164\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 151\n",
            "Norm: 85.78, NNZs: 35, Bias: 17.009753, T: 6971217, Avg. loss: 16293.628230\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 152\n",
            "Norm: 89.00, NNZs: 35, Bias: 17.046937, T: 7017384, Avg. loss: 16666.241001\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 153\n",
            "Norm: 90.28, NNZs: 35, Bias: 17.055364, T: 7063551, Avg. loss: 16324.604975\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 154\n",
            "Norm: 86.45, NNZs: 35, Bias: 17.055402, T: 7109718, Avg. loss: 16119.986803\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 155\n",
            "Norm: 87.43, NNZs: 35, Bias: 17.089068, T: 7155885, Avg. loss: 16116.789205\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 156\n",
            "Norm: 100.50, NNZs: 35, Bias: 17.105799, T: 7202052, Avg. loss: 16066.305353\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 157\n",
            "Norm: 86.04, NNZs: 35, Bias: 17.118248, T: 7248219, Avg. loss: 15878.023704\n",
            "Total training time: 1.06 seconds.\n",
            "-- Epoch 158\n",
            "Norm: 104.92, NNZs: 35, Bias: 17.149825, T: 7294386, Avg. loss: 15969.713803\n",
            "Total training time: 1.06 seconds.\n",
            "-- Epoch 159\n",
            "Norm: 86.44, NNZs: 35, Bias: 17.141591, T: 7340553, Avg. loss: 15752.546304\n",
            "Total training time: 1.07 seconds.\n",
            "-- Epoch 160\n",
            "Norm: 86.79, NNZs: 35, Bias: 17.134850, T: 7386720, Avg. loss: 15483.792089\n",
            "Total training time: 1.07 seconds.\n",
            "-- Epoch 161\n",
            "Norm: 95.67, NNZs: 35, Bias: 17.155066, T: 7432887, Avg. loss: 15593.382859\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 162\n",
            "Norm: 85.82, NNZs: 35, Bias: 17.165810, T: 7479054, Avg. loss: 15759.358252\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 163\n",
            "Norm: 93.17, NNZs: 35, Bias: 17.200459, T: 7525221, Avg. loss: 15363.493795\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 164\n",
            "Norm: 100.08, NNZs: 35, Bias: 17.184571, T: 7571388, Avg. loss: 15044.263426\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 165\n",
            "Norm: 92.49, NNZs: 35, Bias: 17.197692, T: 7617555, Avg. loss: 15290.672378\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 166\n",
            "Norm: 92.52, NNZs: 35, Bias: 17.202894, T: 7663722, Avg. loss: 14993.947011\n",
            "Total training time: 1.11 seconds.\n",
            "-- Epoch 167\n",
            "Norm: 85.78, NNZs: 35, Bias: 17.192528, T: 7709889, Avg. loss: 14820.017370\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 168\n",
            "Norm: 86.89, NNZs: 35, Bias: 17.208019, T: 7756056, Avg. loss: 14780.597524\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 169\n",
            "Norm: 92.28, NNZs: 35, Bias: 17.247817, T: 7802223, Avg. loss: 15092.414668\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 170\n",
            "Norm: 85.94, NNZs: 35, Bias: 17.247809, T: 7848390, Avg. loss: 14518.353951\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 171\n",
            "Norm: 86.61, NNZs: 35, Bias: 17.224928, T: 7894557, Avg. loss: 14430.312817\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 172\n",
            "Norm: 88.18, NNZs: 35, Bias: 17.240105, T: 7940724, Avg. loss: 14555.011829\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 173\n",
            "Norm: 86.16, NNZs: 35, Bias: 17.236392, T: 7986891, Avg. loss: 14548.043163\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 174\n",
            "Norm: 91.19, NNZs: 35, Bias: 17.242713, T: 8033058, Avg. loss: 14569.004473\n",
            "Total training time: 1.17 seconds.\n",
            "-- Epoch 175\n",
            "Norm: 94.68, NNZs: 35, Bias: 17.252638, T: 8079225, Avg. loss: 14477.382802\n",
            "Total training time: 1.18 seconds.\n",
            "-- Epoch 176\n",
            "Norm: 90.02, NNZs: 35, Bias: 17.257513, T: 8125392, Avg. loss: 14053.924701\n",
            "Total training time: 1.18 seconds.\n",
            "-- Epoch 177\n",
            "Norm: 87.77, NNZs: 35, Bias: 17.274683, T: 8171559, Avg. loss: 14183.002255\n",
            "Total training time: 1.19 seconds.\n",
            "-- Epoch 178\n",
            "Norm: 95.81, NNZs: 35, Bias: 17.299150, T: 8217726, Avg. loss: 14259.387755\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 179\n",
            "Norm: 85.75, NNZs: 35, Bias: 17.305215, T: 8263893, Avg. loss: 14153.491035\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 180\n",
            "Norm: 87.91, NNZs: 35, Bias: 17.283559, T: 8310060, Avg. loss: 13708.544043\n",
            "Total training time: 1.21 seconds.\n",
            "-- Epoch 181\n",
            "Norm: 105.64, NNZs: 35, Bias: 17.278702, T: 8356227, Avg. loss: 13904.531900\n",
            "Total training time: 1.21 seconds.\n",
            "-- Epoch 182\n",
            "Norm: 95.41, NNZs: 35, Bias: 17.309737, T: 8402394, Avg. loss: 14027.331867\n",
            "Total training time: 1.22 seconds.\n",
            "-- Epoch 183\n",
            "Norm: 101.72, NNZs: 35, Bias: 17.332302, T: 8448561, Avg. loss: 13600.585473\n",
            "Total training time: 1.23 seconds.\n",
            "-- Epoch 184\n",
            "Norm: 101.59, NNZs: 35, Bias: 17.318161, T: 8494728, Avg. loss: 13564.439112\n",
            "Total training time: 1.23 seconds.\n",
            "-- Epoch 185\n",
            "Norm: 90.50, NNZs: 35, Bias: 17.334602, T: 8540895, Avg. loss: 13725.150854\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 186\n",
            "Norm: 91.40, NNZs: 35, Bias: 17.346292, T: 8587062, Avg. loss: 13567.139541\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 187\n",
            "Norm: 97.37, NNZs: 35, Bias: 17.345143, T: 8633229, Avg. loss: 13331.368718\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 188\n",
            "Norm: 85.84, NNZs: 35, Bias: 17.345170, T: 8679396, Avg. loss: 13086.288373\n",
            "Total training time: 1.26 seconds.\n",
            "-- Epoch 189\n",
            "Norm: 89.11, NNZs: 35, Bias: 17.332525, T: 8725563, Avg. loss: 13127.905805\n",
            "Total training time: 1.26 seconds.\n",
            "-- Epoch 190\n",
            "Norm: 93.01, NNZs: 35, Bias: 17.329133, T: 8771730, Avg. loss: 13294.124435\n",
            "Total training time: 1.27 seconds.\n",
            "-- Epoch 191\n",
            "Norm: 101.13, NNZs: 35, Bias: 17.340475, T: 8817897, Avg. loss: 13096.873635\n",
            "Total training time: 1.27 seconds.\n",
            "-- Epoch 192\n",
            "Norm: 105.85, NNZs: 35, Bias: 17.330335, T: 8864064, Avg. loss: 12959.949511\n",
            "Total training time: 1.28 seconds.\n",
            "-- Epoch 193\n",
            "Norm: 96.83, NNZs: 35, Bias: 17.326978, T: 8910231, Avg. loss: 12922.043510\n",
            "Total training time: 1.29 seconds.\n",
            "-- Epoch 194\n",
            "Norm: 93.08, NNZs: 35, Bias: 17.318052, T: 8956398, Avg. loss: 12912.361498\n",
            "Total training time: 1.29 seconds.\n",
            "-- Epoch 195\n",
            "Norm: 89.49, NNZs: 35, Bias: 17.320302, T: 9002565, Avg. loss: 12864.176120\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 196\n",
            "Norm: 87.66, NNZs: 35, Bias: 17.321384, T: 9048732, Avg. loss: 12699.968859\n",
            "Total training time: 1.31 seconds.\n",
            "-- Epoch 197\n",
            "Norm: 88.01, NNZs: 35, Bias: 17.313690, T: 9094899, Avg. loss: 12531.556780\n",
            "Total training time: 1.31 seconds.\n",
            "-- Epoch 198\n",
            "Norm: 85.79, NNZs: 35, Bias: 17.322432, T: 9141066, Avg. loss: 12696.081993\n",
            "Total training time: 1.32 seconds.\n",
            "-- Epoch 199\n",
            "Norm: 85.83, NNZs: 35, Bias: 17.355142, T: 9187233, Avg. loss: 12770.102782\n",
            "Total training time: 1.33 seconds.\n",
            "-- Epoch 200\n",
            "Norm: 86.42, NNZs: 35, Bias: 17.358381, T: 9233400, Avg. loss: 12327.245750\n",
            "Total training time: 1.33 seconds.\n",
            "-- Epoch 201\n",
            "Norm: 90.07, NNZs: 35, Bias: 17.376761, T: 9279567, Avg. loss: 12566.720252\n",
            "Total training time: 1.34 seconds.\n",
            "-- Epoch 202\n",
            "Norm: 86.58, NNZs: 35, Bias: 17.381056, T: 9325734, Avg. loss: 12482.858516\n",
            "Total training time: 1.35 seconds.\n",
            "-- Epoch 203\n",
            "Norm: 90.12, NNZs: 35, Bias: 17.390673, T: 9371901, Avg. loss: 12274.338989\n",
            "Total training time: 1.35 seconds.\n",
            "-- Epoch 204\n",
            "Norm: 85.78, NNZs: 35, Bias: 17.401330, T: 9418068, Avg. loss: 12245.316401\n",
            "Total training time: 1.36 seconds.\n",
            "-- Epoch 205\n",
            "Norm: 94.03, NNZs: 35, Bias: 17.405523, T: 9464235, Avg. loss: 12111.212075\n",
            "Total training time: 1.36 seconds.\n",
            "-- Epoch 206\n",
            "Norm: 86.69, NNZs: 35, Bias: 17.418186, T: 9510402, Avg. loss: 12257.539347\n",
            "Total training time: 1.37 seconds.\n",
            "-- Epoch 207\n",
            "Norm: 90.62, NNZs: 35, Bias: 17.393053, T: 9556569, Avg. loss: 11675.918977\n",
            "Total training time: 1.38 seconds.\n",
            "-- Epoch 208\n",
            "Norm: 87.50, NNZs: 35, Bias: 17.401416, T: 9602736, Avg. loss: 11885.431522\n",
            "Total training time: 1.38 seconds.\n",
            "-- Epoch 209\n",
            "Norm: 85.93, NNZs: 35, Bias: 17.401453, T: 9648903, Avg. loss: 11780.897646\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 210\n",
            "Norm: 91.03, NNZs: 35, Bias: 17.413894, T: 9695070, Avg. loss: 11951.534227\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 211\n",
            "Norm: 87.43, NNZs: 35, Bias: 17.419072, T: 9741237, Avg. loss: 11793.800727\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 212\n",
            "Norm: 91.12, NNZs: 35, Bias: 17.446759, T: 9787404, Avg. loss: 12027.514953\n",
            "Total training time: 1.41 seconds.\n",
            "Convergence after 212 epochs took 1.41 seconds\n",
            "-- Epoch 1\n",
            "Norm: 2863.35, NNZs: 14, Bias: -25.355863, T: 46167, Avg. loss: 6069656.597590\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1875.73, NNZs: 14, Bias: -28.997833, T: 92334, Avg. loss: 1420975.759269\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 3348.07, NNZs: 15, Bias: -31.416432, T: 138501, Avg. loss: 841572.991730\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 406.35, NNZs: 17, Bias: -32.594546, T: 184668, Avg. loss: 585253.895279\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 372.08, NNZs: 18, Bias: -33.467752, T: 230835, Avg. loss: 455238.658198\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 22.42, NNZs: 19, Bias: -34.427664, T: 277002, Avg. loss: 379361.416280\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 910.42, NNZs: 19, Bias: -35.022977, T: 323169, Avg. loss: 320351.757846\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 80.79, NNZs: 19, Bias: -35.465830, T: 369336, Avg. loss: 275341.888444\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 823.69, NNZs: 19, Bias: -36.136660, T: 415503, Avg. loss: 244019.976785\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 101.56, NNZs: 19, Bias: -36.321188, T: 461670, Avg. loss: 220452.789591\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 459.09, NNZs: 20, Bias: -36.795442, T: 507837, Avg. loss: 196948.657975\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 490.40, NNZs: 21, Bias: -37.127195, T: 554004, Avg. loss: 179309.194833\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 799.18, NNZs: 22, Bias: -37.829214, T: 600171, Avg. loss: 161310.807574\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 125.43, NNZs: 22, Bias: -37.731176, T: 646338, Avg. loss: 154082.324916\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 370.14, NNZs: 22, Bias: -37.756500, T: 692505, Avg. loss: 143440.013607\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 118.37, NNZs: 22, Bias: -37.955200, T: 738672, Avg. loss: 132729.841084\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 35.09, NNZs: 23, Bias: -38.447774, T: 784839, Avg. loss: 122145.680022\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 181.97, NNZs: 25, Bias: -38.551549, T: 831006, Avg. loss: 119535.968874\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 603.24, NNZs: 25, Bias: -38.796327, T: 877173, Avg. loss: 112948.725511\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 284.25, NNZs: 26, Bias: -38.951140, T: 923340, Avg. loss: 106327.897693\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 335.29, NNZs: 26, Bias: -39.145016, T: 969507, Avg. loss: 100461.345022\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 122.44, NNZs: 26, Bias: -39.204564, T: 1015674, Avg. loss: 96677.737126\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 25.09, NNZs: 26, Bias: -39.405091, T: 1061841, Avg. loss: 91614.020866\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 139.08, NNZs: 27, Bias: -39.608724, T: 1108008, Avg. loss: 88984.006660\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 34.00, NNZs: 27, Bias: -39.759054, T: 1154175, Avg. loss: 84400.681759\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 31.96, NNZs: 27, Bias: -40.045124, T: 1200342, Avg. loss: 79779.052079\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 320.65, NNZs: 27, Bias: -40.215394, T: 1246509, Avg. loss: 77947.670603\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 282.33, NNZs: 27, Bias: -40.325814, T: 1292676, Avg. loss: 74365.021666\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 377.47, NNZs: 27, Bias: -40.500237, T: 1338843, Avg. loss: 72219.207509\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 24.49, NNZs: 28, Bias: -40.668721, T: 1385010, Avg. loss: 69837.269594\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 146.89, NNZs: 28, Bias: -40.759871, T: 1431177, Avg. loss: 67515.302034\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 95.81, NNZs: 28, Bias: -41.007539, T: 1477344, Avg. loss: 64265.752753\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 25.45, NNZs: 28, Bias: -41.114020, T: 1523511, Avg. loss: 63073.455532\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 49.41, NNZs: 28, Bias: -41.158362, T: 1569678, Avg. loss: 61686.338102\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 31.58, NNZs: 28, Bias: -41.245864, T: 1615845, Avg. loss: 60913.118899\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 18.51, NNZs: 28, Bias: -41.294814, T: 1662012, Avg. loss: 58659.036433\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 107.55, NNZs: 28, Bias: -41.408920, T: 1708179, Avg. loss: 57364.128505\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 123.80, NNZs: 28, Bias: -41.512933, T: 1754346, Avg. loss: 54821.110959\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 18.57, NNZs: 28, Bias: -41.660234, T: 1800513, Avg. loss: 53305.593157\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 62.21, NNZs: 28, Bias: -41.742897, T: 1846680, Avg. loss: 52365.382756\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 39.73, NNZs: 28, Bias: -41.860246, T: 1892847, Avg. loss: 50482.472299\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 38.22, NNZs: 28, Bias: -41.922711, T: 1939014, Avg. loss: 50093.378673\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 93.47, NNZs: 28, Bias: -42.070506, T: 1985181, Avg. loss: 48565.774239\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 78.40, NNZs: 29, Bias: -42.150507, T: 2031348, Avg. loss: 46883.081122\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 57.97, NNZs: 29, Bias: -42.331204, T: 2077515, Avg. loss: 46202.310805\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 57.16, NNZs: 29, Bias: -42.340332, T: 2123682, Avg. loss: 45586.899513\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 198.07, NNZs: 29, Bias: -42.499250, T: 2169849, Avg. loss: 43954.259604\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 33.07, NNZs: 29, Bias: -42.640473, T: 2216016, Avg. loss: 43557.970453\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 33.40, NNZs: 29, Bias: -42.725497, T: 2262183, Avg. loss: 42879.405849\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 70.52, NNZs: 29, Bias: -42.716483, T: 2308350, Avg. loss: 42262.065859\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 78.39, NNZs: 29, Bias: -42.827984, T: 2354517, Avg. loss: 41100.530580\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 36.32, NNZs: 30, Bias: -42.853231, T: 2400684, Avg. loss: 40776.179855\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 122.21, NNZs: 30, Bias: -42.914213, T: 2446851, Avg. loss: 39502.249520\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 35.92, NNZs: 30, Bias: -43.023240, T: 2493018, Avg. loss: 38035.373813\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 111.39, NNZs: 30, Bias: -43.074297, T: 2539185, Avg. loss: 38208.027941\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 97.62, NNZs: 30, Bias: -43.105077, T: 2585352, Avg. loss: 37451.183721\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 160.38, NNZs: 30, Bias: -43.147114, T: 2631519, Avg. loss: 36877.621345\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 39.54, NNZs: 30, Bias: -43.184773, T: 2677686, Avg. loss: 36079.119289\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 167.87, NNZs: 30, Bias: -43.258992, T: 2723853, Avg. loss: 34447.347027\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 104.70, NNZs: 31, Bias: -43.371367, T: 2770020, Avg. loss: 34567.455067\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 52.78, NNZs: 31, Bias: -43.443151, T: 2816187, Avg. loss: 34382.857976\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 43.59, NNZs: 31, Bias: -43.502983, T: 2862354, Avg. loss: 33777.444266\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 57.61, NNZs: 31, Bias: -43.582396, T: 2908521, Avg. loss: 32922.901720\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 21.17, NNZs: 32, Bias: -43.664321, T: 2954688, Avg. loss: 32368.780570\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 27.43, NNZs: 32, Bias: -43.751806, T: 3000855, Avg. loss: 31569.998775\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 75.85, NNZs: 32, Bias: -43.834164, T: 3047022, Avg. loss: 31009.831423\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 53.88, NNZs: 32, Bias: -43.938443, T: 3093189, Avg. loss: 30662.607385\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 40.72, NNZs: 32, Bias: -44.012065, T: 3139356, Avg. loss: 30102.338620\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 61.81, NNZs: 32, Bias: -44.078090, T: 3185523, Avg. loss: 30115.120444\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 42.76, NNZs: 32, Bias: -44.156050, T: 3231690, Avg. loss: 29491.413554\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 42.50, NNZs: 32, Bias: -44.245059, T: 3277857, Avg. loss: 28875.295790\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 18.47, NNZs: 32, Bias: -44.224297, T: 3324024, Avg. loss: 29017.969789\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 18.76, NNZs: 32, Bias: -44.331707, T: 3370191, Avg. loss: 27709.389714\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 21.51, NNZs: 32, Bias: -44.384427, T: 3416358, Avg. loss: 28578.040131\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 58.03, NNZs: 32, Bias: -44.433763, T: 3462525, Avg. loss: 27637.024450\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 22.56, NNZs: 32, Bias: -44.537249, T: 3508692, Avg. loss: 27102.206091\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 36.66, NNZs: 32, Bias: -44.582744, T: 3554859, Avg. loss: 26793.264570\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 141.00, NNZs: 33, Bias: -44.669326, T: 3601026, Avg. loss: 26527.944344\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 44.75, NNZs: 33, Bias: -44.735522, T: 3647193, Avg. loss: 26176.705658\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 34.90, NNZs: 33, Bias: -44.770743, T: 3693360, Avg. loss: 25969.681702\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 55.87, NNZs: 33, Bias: -44.805709, T: 3739527, Avg. loss: 25616.337563\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 38.13, NNZs: 33, Bias: -44.824125, T: 3785694, Avg. loss: 25595.831470\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 91.18, NNZs: 33, Bias: -44.832057, T: 3831861, Avg. loss: 25280.908957\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 36.02, NNZs: 33, Bias: -44.907302, T: 3878028, Avg. loss: 24182.240700\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 40.99, NNZs: 33, Bias: -44.960967, T: 3924195, Avg. loss: 24139.291194\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 18.45, NNZs: 33, Bias: -45.026806, T: 3970362, Avg. loss: 23532.988220\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 64.60, NNZs: 33, Bias: -45.061836, T: 4016529, Avg. loss: 24285.168356\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 46.51, NNZs: 33, Bias: -45.106262, T: 4062696, Avg. loss: 23649.479743\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 19.18, NNZs: 33, Bias: -45.157597, T: 4108863, Avg. loss: 23137.133962\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 56.32, NNZs: 33, Bias: -45.172031, T: 4155030, Avg. loss: 23075.711262\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 47.61, NNZs: 33, Bias: -45.191251, T: 4201197, Avg. loss: 23010.591713\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 26.49, NNZs: 33, Bias: -45.184246, T: 4247364, Avg. loss: 22801.389956\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 42.67, NNZs: 33, Bias: -45.221854, T: 4293531, Avg. loss: 22458.870864\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 18.77, NNZs: 33, Bias: -45.277451, T: 4339698, Avg. loss: 21843.577398\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 46.31, NNZs: 33, Bias: -45.286446, T: 4385865, Avg. loss: 21771.825838\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 86.86, NNZs: 33, Bias: -45.297961, T: 4432032, Avg. loss: 21988.893016\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 40.52, NNZs: 34, Bias: -45.356347, T: 4478199, Avg. loss: 21350.291378\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 41.80, NNZs: 34, Bias: -45.407299, T: 4524366, Avg. loss: 20874.878609\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 81.31, NNZs: 34, Bias: -45.431348, T: 4570533, Avg. loss: 20644.152327\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 25.15, NNZs: 34, Bias: -45.474854, T: 4616700, Avg. loss: 20699.581454\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 80.02, NNZs: 34, Bias: -45.517962, T: 4662867, Avg. loss: 20149.443282\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 38.99, NNZs: 34, Bias: -45.526506, T: 4709034, Avg. loss: 20588.928069\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 58.84, NNZs: 34, Bias: -45.556130, T: 4755201, Avg. loss: 20028.833124\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 42.05, NNZs: 34, Bias: -45.577112, T: 4801368, Avg. loss: 20134.565083\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 65.80, NNZs: 34, Bias: -45.620661, T: 4847535, Avg. loss: 19859.409409\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 47.65, NNZs: 34, Bias: -45.686466, T: 4893702, Avg. loss: 19240.182531\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 18.86, NNZs: 34, Bias: -45.714828, T: 4939869, Avg. loss: 19429.097344\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 34.60, NNZs: 34, Bias: -45.720902, T: 4986036, Avg. loss: 19380.962391\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 31.24, NNZs: 34, Bias: -45.768822, T: 5032203, Avg. loss: 18902.264007\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 48.87, NNZs: 34, Bias: -45.840131, T: 5078370, Avg. loss: 18315.945199\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 22.58, NNZs: 34, Bias: -45.857802, T: 5124537, Avg. loss: 18795.349240\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 29.68, NNZs: 34, Bias: -45.857567, T: 5170704, Avg. loss: 18923.914689\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 58.17, NNZs: 34, Bias: -45.899828, T: 5216871, Avg. loss: 18352.903987\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 21.20, NNZs: 34, Bias: -45.957102, T: 5263038, Avg. loss: 18115.554300\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 18.63, NNZs: 34, Bias: -45.998738, T: 5309205, Avg. loss: 18263.269146\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 51.36, NNZs: 34, Bias: -46.023186, T: 5355372, Avg. loss: 18176.102656\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 28.29, NNZs: 34, Bias: -46.062232, T: 5401539, Avg. loss: 17738.362339\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 21.02, NNZs: 34, Bias: -46.132252, T: 5447706, Avg. loss: 17418.874398\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 36.14, NNZs: 34, Bias: -46.185117, T: 5493873, Avg. loss: 17245.609730\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 35.72, NNZs: 34, Bias: -46.239503, T: 5540040, Avg. loss: 17066.785941\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 18.98, NNZs: 34, Bias: -46.250313, T: 5586207, Avg. loss: 16802.280307\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 55.03, NNZs: 34, Bias: -46.303797, T: 5632374, Avg. loss: 16952.305820\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 23.41, NNZs: 34, Bias: -46.340936, T: 5678541, Avg. loss: 16848.412621\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 19.61, NNZs: 34, Bias: -46.404044, T: 5724708, Avg. loss: 16463.750061\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 72.42, NNZs: 34, Bias: -46.468495, T: 5770875, Avg. loss: 16213.566639\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 76.54, NNZs: 34, Bias: -46.489206, T: 5817042, Avg. loss: 16679.796584\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 25.46, NNZs: 34, Bias: -46.550829, T: 5863209, Avg. loss: 16083.577348\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 18.56, NNZs: 34, Bias: -46.566143, T: 5909376, Avg. loss: 16372.673734\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 129\n",
            "Norm: 21.82, NNZs: 34, Bias: -46.615085, T: 5955543, Avg. loss: 16031.465988\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 130\n",
            "Norm: 33.14, NNZs: 34, Bias: -46.646894, T: 6001710, Avg. loss: 15920.789954\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 131\n",
            "Norm: 66.95, NNZs: 34, Bias: -46.683452, T: 6047877, Avg. loss: 15840.662812\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 132\n",
            "Norm: 37.88, NNZs: 34, Bias: -46.724656, T: 6094044, Avg. loss: 15553.210105\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 133\n",
            "Norm: 24.14, NNZs: 34, Bias: -46.754084, T: 6140211, Avg. loss: 15510.208930\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 134\n",
            "Norm: 83.91, NNZs: 34, Bias: -46.810817, T: 6186378, Avg. loss: 15453.267077\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 135\n",
            "Norm: 25.36, NNZs: 34, Bias: -46.802731, T: 6232545, Avg. loss: 15650.576128\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 136\n",
            "Norm: 58.42, NNZs: 34, Bias: -46.837889, T: 6278712, Avg. loss: 15459.300493\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 137\n",
            "Norm: 22.43, NNZs: 35, Bias: -46.866401, T: 6324879, Avg. loss: 15054.562599\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 138\n",
            "Norm: 30.13, NNZs: 35, Bias: -46.915300, T: 6371046, Avg. loss: 14906.514550\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 139\n",
            "Norm: 63.58, NNZs: 35, Bias: -46.960615, T: 6417213, Avg. loss: 14821.790188\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 140\n",
            "Norm: 19.99, NNZs: 35, Bias: -46.977761, T: 6463380, Avg. loss: 14844.316094\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 141\n",
            "Norm: 69.73, NNZs: 35, Bias: -46.990054, T: 6509547, Avg. loss: 14936.775933\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 142\n",
            "Norm: 30.96, NNZs: 35, Bias: -47.017677, T: 6555714, Avg. loss: 14570.500126\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 143\n",
            "Norm: 19.58, NNZs: 35, Bias: -47.043576, T: 6601881, Avg. loss: 14581.061364\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 144\n",
            "Norm: 26.31, NNZs: 35, Bias: -47.078220, T: 6648048, Avg. loss: 14455.955332\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 145\n",
            "Norm: 88.01, NNZs: 35, Bias: -47.114207, T: 6694215, Avg. loss: 14165.161478\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 146\n",
            "Norm: 30.52, NNZs: 35, Bias: -47.155869, T: 6740382, Avg. loss: 14180.378728\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 147\n",
            "Norm: 25.63, NNZs: 35, Bias: -47.191351, T: 6786549, Avg. loss: 14213.265531\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 148\n",
            "Norm: 24.82, NNZs: 35, Bias: -47.223637, T: 6832716, Avg. loss: 13673.937944\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 149\n",
            "Norm: 18.95, NNZs: 35, Bias: -47.260007, T: 6878883, Avg. loss: 13608.631277\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 150\n",
            "Norm: 45.15, NNZs: 35, Bias: -47.291904, T: 6925050, Avg. loss: 13679.112405\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 151\n",
            "Norm: 33.36, NNZs: 36, Bias: -47.301943, T: 6971217, Avg. loss: 13845.814301\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 152\n",
            "Norm: 26.77, NNZs: 36, Bias: -47.329159, T: 7017384, Avg. loss: 13934.915100\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 153\n",
            "Norm: 27.57, NNZs: 36, Bias: -47.368863, T: 7063551, Avg. loss: 13290.234821\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 154\n",
            "Norm: 23.36, NNZs: 36, Bias: -47.392752, T: 7109718, Avg. loss: 13428.446520\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 155\n",
            "Norm: 48.73, NNZs: 36, Bias: -47.394172, T: 7155885, Avg. loss: 13599.337827\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 156\n",
            "Norm: 31.13, NNZs: 36, Bias: -47.403892, T: 7202052, Avg. loss: 13562.665548\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 157\n",
            "Norm: 29.98, NNZs: 36, Bias: -47.427412, T: 7248219, Avg. loss: 13147.408569\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 158\n",
            "Norm: 19.24, NNZs: 36, Bias: -47.459020, T: 7294386, Avg. loss: 12970.471972\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 159\n",
            "Norm: 38.58, NNZs: 36, Bias: -47.464455, T: 7340553, Avg. loss: 13261.768753\n",
            "Total training time: 1.06 seconds.\n",
            "-- Epoch 160\n",
            "Norm: 22.67, NNZs: 36, Bias: -47.514663, T: 7386720, Avg. loss: 12662.890640\n",
            "Total training time: 1.07 seconds.\n",
            "-- Epoch 161\n",
            "Norm: 21.98, NNZs: 36, Bias: -47.552461, T: 7432887, Avg. loss: 12845.812639\n",
            "Total training time: 1.07 seconds.\n",
            "-- Epoch 162\n",
            "Norm: 22.09, NNZs: 36, Bias: -47.575303, T: 7479054, Avg. loss: 12757.044762\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 163\n",
            "Norm: 21.23, NNZs: 36, Bias: -47.594048, T: 7525221, Avg. loss: 12708.649534\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 164\n",
            "Norm: 22.85, NNZs: 36, Bias: -47.608650, T: 7571388, Avg. loss: 12777.360458\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 165\n",
            "Norm: 22.29, NNZs: 36, Bias: -47.638935, T: 7617555, Avg. loss: 12500.305378\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 166\n",
            "Norm: 20.67, NNZs: 36, Bias: -47.661149, T: 7663722, Avg. loss: 12416.804705\n",
            "Total training time: 1.11 seconds.\n",
            "-- Epoch 167\n",
            "Norm: 31.52, NNZs: 36, Bias: -47.709283, T: 7709889, Avg. loss: 11952.510616\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 168\n",
            "Norm: 30.06, NNZs: 36, Bias: -47.753270, T: 7756056, Avg. loss: 12172.936690\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 169\n",
            "Norm: 48.50, NNZs: 36, Bias: -47.767366, T: 7802223, Avg. loss: 12539.462957\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 170\n",
            "Norm: 42.65, NNZs: 36, Bias: -47.773767, T: 7848390, Avg. loss: 12291.842024\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 171\n",
            "Norm: 18.32, NNZs: 36, Bias: -47.814408, T: 7894557, Avg. loss: 11996.158436\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 172\n",
            "Norm: 45.78, NNZs: 36, Bias: -47.869992, T: 7940724, Avg. loss: 11825.677900\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 173\n",
            "Norm: 24.96, NNZs: 36, Bias: -47.917676, T: 7986891, Avg. loss: 11731.084852\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 174\n",
            "Norm: 70.98, NNZs: 36, Bias: -47.937648, T: 8033058, Avg. loss: 11781.575875\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 175\n",
            "Norm: 55.47, NNZs: 36, Bias: -47.943875, T: 8079225, Avg. loss: 12100.630867\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 176\n",
            "Norm: 28.51, NNZs: 36, Bias: -47.933998, T: 8125392, Avg. loss: 12092.960768\n",
            "Total training time: 1.17 seconds.\n",
            "-- Epoch 177\n",
            "Norm: 19.92, NNZs: 36, Bias: -47.978167, T: 8171559, Avg. loss: 11541.154843\n",
            "Total training time: 1.18 seconds.\n",
            "-- Epoch 178\n",
            "Norm: 30.11, NNZs: 36, Bias: -48.006224, T: 8217726, Avg. loss: 11627.925362\n",
            "Total training time: 1.19 seconds.\n",
            "-- Epoch 179\n",
            "Norm: 22.66, NNZs: 36, Bias: -48.007454, T: 8263893, Avg. loss: 11776.780109\n",
            "Total training time: 1.19 seconds.\n",
            "-- Epoch 180\n",
            "Norm: 36.19, NNZs: 36, Bias: -48.037630, T: 8310060, Avg. loss: 11458.026659\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 181\n",
            "Norm: 18.96, NNZs: 36, Bias: -48.067629, T: 8356227, Avg. loss: 11396.212679\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 182\n",
            "Norm: 42.33, NNZs: 36, Bias: -48.077172, T: 8402394, Avg. loss: 11581.162306\n",
            "Total training time: 1.21 seconds.\n",
            "-- Epoch 183\n",
            "Norm: 22.35, NNZs: 36, Bias: -48.103222, T: 8448561, Avg. loss: 11165.702328\n",
            "Total training time: 1.21 seconds.\n",
            "-- Epoch 184\n",
            "Norm: 47.77, NNZs: 36, Bias: -48.105565, T: 8494728, Avg. loss: 11342.804325\n",
            "Total training time: 1.22 seconds.\n",
            "-- Epoch 185\n",
            "Norm: 25.12, NNZs: 36, Bias: -48.136080, T: 8540895, Avg. loss: 11143.759151\n",
            "Total training time: 1.23 seconds.\n",
            "-- Epoch 186\n",
            "Norm: 23.50, NNZs: 36, Bias: -48.159398, T: 8587062, Avg. loss: 11051.036919\n",
            "Total training time: 1.23 seconds.\n",
            "-- Epoch 187\n",
            "Norm: 75.12, NNZs: 36, Bias: -48.180320, T: 8633229, Avg. loss: 11010.480973\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 188\n",
            "Norm: 52.81, NNZs: 36, Bias: -48.190694, T: 8679396, Avg. loss: 11209.816690\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 189\n",
            "Norm: 46.58, NNZs: 36, Bias: -48.213716, T: 8725563, Avg. loss: 10875.749960\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 190\n",
            "Norm: 24.21, NNZs: 36, Bias: -48.236586, T: 8771730, Avg. loss: 10943.223810\n",
            "Total training time: 1.26 seconds.\n",
            "-- Epoch 191\n",
            "Norm: 21.97, NNZs: 36, Bias: -48.263849, T: 8817897, Avg. loss: 10793.290995\n",
            "Total training time: 1.26 seconds.\n",
            "-- Epoch 192\n",
            "Norm: 18.62, NNZs: 36, Bias: -48.275129, T: 8864064, Avg. loss: 10896.209163\n",
            "Total training time: 1.27 seconds.\n",
            "-- Epoch 193\n",
            "Norm: 18.29, NNZs: 36, Bias: -48.280715, T: 8910231, Avg. loss: 11010.445239\n",
            "Total training time: 1.28 seconds.\n",
            "-- Epoch 194\n",
            "Norm: 36.77, NNZs: 36, Bias: -48.303110, T: 8956398, Avg. loss: 10708.370168\n",
            "Total training time: 1.28 seconds.\n",
            "-- Epoch 195\n",
            "Norm: 21.29, NNZs: 36, Bias: -48.315359, T: 9002565, Avg. loss: 10679.380177\n",
            "Total training time: 1.29 seconds.\n",
            "-- Epoch 196\n",
            "Norm: 18.28, NNZs: 36, Bias: -48.353025, T: 9048732, Avg. loss: 10438.092329\n",
            "Total training time: 1.29 seconds.\n",
            "-- Epoch 197\n",
            "Norm: 58.30, NNZs: 36, Bias: -48.382835, T: 9094899, Avg. loss: 10581.243424\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 198\n",
            "Norm: 19.72, NNZs: 36, Bias: -48.419034, T: 9141066, Avg. loss: 10270.540189\n",
            "Total training time: 1.31 seconds.\n",
            "-- Epoch 199\n",
            "Norm: 19.38, NNZs: 36, Bias: -48.445225, T: 9187233, Avg. loss: 10355.515706\n",
            "Total training time: 1.31 seconds.\n",
            "-- Epoch 200\n",
            "Norm: 19.24, NNZs: 36, Bias: -48.472361, T: 9233400, Avg. loss: 10318.988010\n",
            "Total training time: 1.32 seconds.\n",
            "-- Epoch 201\n",
            "Norm: 32.42, NNZs: 36, Bias: -48.503641, T: 9279567, Avg. loss: 10225.031191\n",
            "Total training time: 1.32 seconds.\n",
            "-- Epoch 202\n",
            "Norm: 68.76, NNZs: 36, Bias: -48.525145, T: 9325734, Avg. loss: 10193.938999\n",
            "Total training time: 1.33 seconds.\n",
            "-- Epoch 203\n",
            "Norm: 49.81, NNZs: 36, Bias: -48.565750, T: 9371901, Avg. loss: 10186.855095\n",
            "Total training time: 1.34 seconds.\n",
            "-- Epoch 204\n",
            "Norm: 23.74, NNZs: 36, Bias: -48.599784, T: 9418068, Avg. loss: 10195.201058\n",
            "Total training time: 1.34 seconds.\n",
            "-- Epoch 205\n",
            "Norm: 18.45, NNZs: 36, Bias: -48.616710, T: 9464235, Avg. loss: 10176.862088\n",
            "Total training time: 1.35 seconds.\n",
            "-- Epoch 206\n",
            "Norm: 25.41, NNZs: 36, Bias: -48.650406, T: 9510402, Avg. loss: 10004.063745\n",
            "Total training time: 1.35 seconds.\n",
            "-- Epoch 207\n",
            "Norm: 43.81, NNZs: 36, Bias: -48.677651, T: 9556569, Avg. loss: 9910.381868\n",
            "Total training time: 1.36 seconds.\n",
            "-- Epoch 208\n",
            "Norm: 24.63, NNZs: 36, Bias: -48.687063, T: 9602736, Avg. loss: 10141.642762\n",
            "Total training time: 1.37 seconds.\n",
            "-- Epoch 209\n",
            "Norm: 19.00, NNZs: 36, Bias: -48.724440, T: 9648903, Avg. loss: 9671.059815\n",
            "Total training time: 1.37 seconds.\n",
            "-- Epoch 210\n",
            "Norm: 19.74, NNZs: 36, Bias: -48.746126, T: 9695070, Avg. loss: 9926.061746\n",
            "Total training time: 1.38 seconds.\n",
            "-- Epoch 211\n",
            "Norm: 52.15, NNZs: 36, Bias: -48.773889, T: 9741237, Avg. loss: 9822.449344\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 212\n",
            "Norm: 18.53, NNZs: 36, Bias: -48.773870, T: 9787404, Avg. loss: 10029.384843\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 213\n",
            "Norm: 27.02, NNZs: 36, Bias: -48.792192, T: 9833571, Avg. loss: 9765.783466\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 214\n",
            "Norm: 52.36, NNZs: 36, Bias: -48.799260, T: 9879738, Avg. loss: 9718.337328\n",
            "Total training time: 1.41 seconds.\n",
            "Convergence after 214 epochs took 1.41 seconds\n",
            "-- Epoch 1\n",
            "Norm: 6166.52, NNZs: 3, Bias: -9.786667, T: 46167, Avg. loss: 2239.116920\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1847.69, NNZs: 3, Bias: -9.624231, T: 92334, Avg. loss: 1211.648027\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 387.46, NNZs: 3, Bias: -9.531244, T: 138501, Avg. loss: 411.257144\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1883.70, NNZs: 3, Bias: -9.531243, T: 184668, Avg. loss: 468.343993\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 997.87, NNZs: 3, Bias: -9.479227, T: 230835, Avg. loss: 466.563232\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 406.26, NNZs: 3, Bias: -9.437376, T: 277002, Avg. loss: 248.295923\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1770.81, NNZs: 5, Bias: -9.437376, T: 323169, Avg. loss: 126.695247\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1230.35, NNZs: 5, Bias: -9.409760, T: 369336, Avg. loss: 406.569801\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 809.70, NNZs: 5, Bias: -9.383354, T: 415503, Avg. loss: 308.560279\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 472.99, NNZs: 5, Bias: -9.361691, T: 461670, Avg. loss: 187.363666\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 197.39, NNZs: 5, Bias: -9.341951, T: 507837, Avg. loss: 110.785814\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 539.22, NNZs: 6, Bias: -9.341951, T: 554004, Avg. loss: 72.268666\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 300.86, NNZs: 6, Bias: -9.325058, T: 600171, Avg. loss: 129.653946\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 96.50, NNZs: 6, Bias: -9.308725, T: 646338, Avg. loss: 75.764815\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 867.58, NNZs: 6, Bias: -9.308725, T: 692505, Avg. loss: 144.627138\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 653.36, NNZs: 6, Bias: -9.294825, T: 738672, Avg. loss: 214.486204\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 464.31, NNZs: 6, Bias: -9.281372, T: 784839, Avg. loss: 166.731514\n",
            "Total training time: 0.11 seconds.\n",
            "Convergence after 17 epochs took 0.11 seconds\n",
            "-- Epoch 1\n",
            "Norm: 9645.36, NNZs: 15, Bias: -39.677247, T: 46167, Avg. loss: 16501885.526149\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 3495.40, NNZs: 17, Bias: -42.501014, T: 92334, Avg. loss: 2795267.191544\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 299.46, NNZs: 19, Bias: -43.646648, T: 138501, Avg. loss: 1619348.294608\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 2535.36, NNZs: 20, Bias: -43.725285, T: 184668, Avg. loss: 1207183.572320\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 2416.42, NNZs: 20, Bias: -44.402710, T: 230835, Avg. loss: 916065.661198\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1404.32, NNZs: 20, Bias: -45.122098, T: 277002, Avg. loss: 750643.120243\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 71.61, NNZs: 22, Bias: -46.336676, T: 323169, Avg. loss: 625170.839471\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 805.95, NNZs: 22, Bias: -46.396386, T: 369336, Avg. loss: 553258.495324\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 162.61, NNZs: 23, Bias: -46.994894, T: 415503, Avg. loss: 482256.602899\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 139.09, NNZs: 23, Bias: -47.135190, T: 461670, Avg. loss: 436017.161354\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 1108.53, NNZs: 23, Bias: -47.424855, T: 507837, Avg. loss: 395055.476920\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 487.72, NNZs: 23, Bias: -47.668293, T: 554004, Avg. loss: 357825.280656\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 189.07, NNZs: 23, Bias: -47.804121, T: 600171, Avg. loss: 330175.390261\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 367.62, NNZs: 23, Bias: -47.926930, T: 646338, Avg. loss: 305933.638350\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 254.16, NNZs: 24, Bias: -48.150730, T: 692505, Avg. loss: 285911.358370\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 79.76, NNZs: 25, Bias: -48.157766, T: 738672, Avg. loss: 270444.373161\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 67.42, NNZs: 27, Bias: -48.326660, T: 784839, Avg. loss: 252429.793067\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 412.89, NNZs: 27, Bias: -48.359412, T: 831006, Avg. loss: 237629.844181\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 576.27, NNZs: 27, Bias: -48.593880, T: 877173, Avg. loss: 221714.142781\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 542.91, NNZs: 27, Bias: -48.984682, T: 923340, Avg. loss: 209113.176981\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 301.99, NNZs: 27, Bias: -49.289913, T: 969507, Avg. loss: 198078.092410\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 137.54, NNZs: 27, Bias: -49.287111, T: 1015674, Avg. loss: 191811.338551\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 238.08, NNZs: 27, Bias: -49.317286, T: 1061841, Avg. loss: 184288.609149\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 487.73, NNZs: 27, Bias: -49.434769, T: 1108008, Avg. loss: 177031.695437\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 138.35, NNZs: 30, Bias: -49.527266, T: 1154175, Avg. loss: 168134.146776\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 196.47, NNZs: 30, Bias: -49.704583, T: 1200342, Avg. loss: 161740.284454\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 42.45, NNZs: 30, Bias: -49.948199, T: 1246509, Avg. loss: 156435.037574\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 191.10, NNZs: 30, Bias: -50.182270, T: 1292676, Avg. loss: 150414.773101\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 157.74, NNZs: 30, Bias: -50.196538, T: 1338843, Avg. loss: 145288.780953\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 143.39, NNZs: 30, Bias: -50.252065, T: 1385010, Avg. loss: 139685.980208\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 172.44, NNZs: 30, Bias: -50.407485, T: 1431177, Avg. loss: 134265.875941\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 358.83, NNZs: 30, Bias: -50.426843, T: 1477344, Avg. loss: 132841.870981\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 49.76, NNZs: 30, Bias: -50.486263, T: 1523511, Avg. loss: 127131.872809\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 57.13, NNZs: 30, Bias: -50.574879, T: 1569678, Avg. loss: 122093.971536\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 65.63, NNZs: 30, Bias: -50.736950, T: 1615845, Avg. loss: 119245.891040\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 193.30, NNZs: 30, Bias: -50.888208, T: 1662012, Avg. loss: 115145.241903\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 80.76, NNZs: 30, Bias: -50.846215, T: 1708179, Avg. loss: 112958.462153\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 184.84, NNZs: 30, Bias: -50.909152, T: 1754346, Avg. loss: 110225.708131\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 237.82, NNZs: 30, Bias: -50.920384, T: 1800513, Avg. loss: 108137.738824\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 132.13, NNZs: 30, Bias: -50.898120, T: 1846680, Avg. loss: 106032.651860\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 52.30, NNZs: 31, Bias: -51.063999, T: 1892847, Avg. loss: 101512.144923\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 121.97, NNZs: 31, Bias: -51.236849, T: 1939014, Avg. loss: 97942.246759\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 41.23, NNZs: 33, Bias: -51.364127, T: 1985181, Avg. loss: 97096.888896\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 114.88, NNZs: 33, Bias: -51.419555, T: 2031348, Avg. loss: 95080.579578\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 255.15, NNZs: 33, Bias: -51.485110, T: 2077515, Avg. loss: 92669.947254\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 38.77, NNZs: 33, Bias: -51.550817, T: 2123682, Avg. loss: 91216.136742\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 51.96, NNZs: 33, Bias: -51.547092, T: 2169849, Avg. loss: 89097.793564\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 42.21, NNZs: 33, Bias: -51.692752, T: 2216016, Avg. loss: 87014.460123\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 95.84, NNZs: 33, Bias: -51.741854, T: 2262183, Avg. loss: 85162.739224\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 52.97, NNZs: 33, Bias: -51.826488, T: 2308350, Avg. loss: 83628.058076\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 120.86, NNZs: 33, Bias: -51.912484, T: 2354517, Avg. loss: 81572.344459\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 223.52, NNZs: 34, Bias: -51.903831, T: 2400684, Avg. loss: 80481.859357\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 84.60, NNZs: 34, Bias: -51.891554, T: 2446851, Avg. loss: 79430.954071\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 190.74, NNZs: 34, Bias: -51.903357, T: 2493018, Avg. loss: 78043.339774\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 163.32, NNZs: 34, Bias: -51.923622, T: 2539185, Avg. loss: 76188.581049\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 48.67, NNZs: 34, Bias: -51.865997, T: 2585352, Avg. loss: 76026.494342\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 86.93, NNZs: 34, Bias: -51.949908, T: 2631519, Avg. loss: 73188.815022\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 121.03, NNZs: 34, Bias: -51.930995, T: 2677686, Avg. loss: 72521.582990\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 45.88, NNZs: 34, Bias: -51.987142, T: 2723853, Avg. loss: 69905.558246\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 69.28, NNZs: 34, Bias: -52.017118, T: 2770020, Avg. loss: 69375.529347\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 100.15, NNZs: 34, Bias: -52.131617, T: 2816187, Avg. loss: 68136.682982\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 40.15, NNZs: 34, Bias: -52.261985, T: 2862354, Avg. loss: 67420.546164\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 66.18, NNZs: 34, Bias: -52.328048, T: 2908521, Avg. loss: 66426.881120\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 40.05, NNZs: 34, Bias: -52.317444, T: 2954688, Avg. loss: 65232.199440\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 39.12, NNZs: 34, Bias: -52.318288, T: 3000855, Avg. loss: 64717.031282\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 139.01, NNZs: 34, Bias: -52.380818, T: 3047022, Avg. loss: 63330.495376\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 54.80, NNZs: 34, Bias: -52.505030, T: 3093189, Avg. loss: 61960.274288\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 94.67, NNZs: 34, Bias: -52.517724, T: 3139356, Avg. loss: 62653.453222\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 87.29, NNZs: 34, Bias: -52.552537, T: 3185523, Avg. loss: 59584.852687\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 62.45, NNZs: 34, Bias: -52.537290, T: 3231690, Avg. loss: 59604.080405\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 71.50, NNZs: 34, Bias: -52.617176, T: 3277857, Avg. loss: 58413.065914\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 131.03, NNZs: 34, Bias: -52.574460, T: 3324024, Avg. loss: 58332.425226\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 46.75, NNZs: 34, Bias: -52.592002, T: 3370191, Avg. loss: 56779.276519\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 39.47, NNZs: 34, Bias: -52.662674, T: 3416358, Avg. loss: 56167.114781\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 45.25, NNZs: 34, Bias: -52.680221, T: 3462525, Avg. loss: 55639.496393\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 122.59, NNZs: 34, Bias: -52.723172, T: 3508692, Avg. loss: 54781.435373\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 166.34, NNZs: 34, Bias: -52.785457, T: 3554859, Avg. loss: 53788.010740\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 62.58, NNZs: 34, Bias: -52.844229, T: 3601026, Avg. loss: 52746.378223\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 121.57, NNZs: 34, Bias: -52.857911, T: 3647193, Avg. loss: 52503.078157\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 77.35, NNZs: 34, Bias: -52.901190, T: 3693360, Avg. loss: 51466.831188\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 88.24, NNZs: 34, Bias: -52.957519, T: 3739527, Avg. loss: 51469.025324\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 125.32, NNZs: 34, Bias: -52.992069, T: 3785694, Avg. loss: 51068.910053\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 40.89, NNZs: 34, Bias: -53.033919, T: 3831861, Avg. loss: 49713.386251\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 71.83, NNZs: 34, Bias: -53.142961, T: 3878028, Avg. loss: 48068.728596\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 98.35, NNZs: 34, Bias: -53.135265, T: 3924195, Avg. loss: 48806.229084\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 85.60, NNZs: 34, Bias: -53.200849, T: 3970362, Avg. loss: 48202.376115\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 60.08, NNZs: 34, Bias: -53.195729, T: 4016529, Avg. loss: 48103.580783\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 66.68, NNZs: 34, Bias: -53.249986, T: 4062696, Avg. loss: 46887.724186\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 55.44, NNZs: 34, Bias: -53.264696, T: 4108863, Avg. loss: 46732.102026\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 38.70, NNZs: 34, Bias: -53.267082, T: 4155030, Avg. loss: 46872.293508\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 43.98, NNZs: 34, Bias: -53.324478, T: 4201197, Avg. loss: 45043.038500\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 107.57, NNZs: 34, Bias: -53.395716, T: 4247364, Avg. loss: 44791.954766\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 38.49, NNZs: 34, Bias: -53.442456, T: 4293531, Avg. loss: 44388.970911\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 68.66, NNZs: 34, Bias: -53.479250, T: 4339698, Avg. loss: 44263.571504\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 49.05, NNZs: 35, Bias: -53.440227, T: 4385865, Avg. loss: 44834.107289\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 47.62, NNZs: 35, Bias: -53.460562, T: 4432032, Avg. loss: 43658.132560\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 42.72, NNZs: 35, Bias: -53.491919, T: 4478199, Avg. loss: 42182.395209\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 96.11, NNZs: 35, Bias: -53.454095, T: 4524366, Avg. loss: 42821.078175\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 50.04, NNZs: 35, Bias: -53.473757, T: 4570533, Avg. loss: 42316.089806\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 77.59, NNZs: 35, Bias: -53.445337, T: 4616700, Avg. loss: 42002.904579\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 97.92, NNZs: 35, Bias: -53.451862, T: 4662867, Avg. loss: 41592.128473\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 66.98, NNZs: 35, Bias: -53.458195, T: 4709034, Avg. loss: 40530.008969\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 107.45, NNZs: 35, Bias: -53.464596, T: 4755201, Avg. loss: 40401.583746\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 38.81, NNZs: 35, Bias: -53.493815, T: 4801368, Avg. loss: 39618.141858\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 47.68, NNZs: 35, Bias: -53.510650, T: 4847535, Avg. loss: 39425.076487\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 66.36, NNZs: 35, Bias: -53.520910, T: 4893702, Avg. loss: 39281.268263\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 122.62, NNZs: 35, Bias: -53.557659, T: 4939869, Avg. loss: 38862.457356\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 61.11, NNZs: 36, Bias: -53.589985, T: 4986036, Avg. loss: 38609.035696\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 79.67, NNZs: 36, Bias: -53.608035, T: 5032203, Avg. loss: 38257.846690\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 83.47, NNZs: 36, Bias: -53.611972, T: 5078370, Avg. loss: 38093.143127\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 51.34, NNZs: 36, Bias: -53.653025, T: 5124537, Avg. loss: 37422.737109\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 80.71, NNZs: 36, Bias: -53.709320, T: 5170704, Avg. loss: 36967.237291\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 45.34, NNZs: 36, Bias: -53.759320, T: 5216871, Avg. loss: 36328.811455\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 71.19, NNZs: 36, Bias: -53.822257, T: 5263038, Avg. loss: 36375.529582\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 81.67, NNZs: 36, Bias: -53.877148, T: 5309205, Avg. loss: 35949.558465\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 49.62, NNZs: 36, Bias: -53.882882, T: 5355372, Avg. loss: 35607.308852\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 76.59, NNZs: 36, Bias: -53.927419, T: 5401539, Avg. loss: 35211.231300\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 42.67, NNZs: 36, Bias: -53.942206, T: 5447706, Avg. loss: 35383.966847\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 87.02, NNZs: 36, Bias: -53.998988, T: 5493873, Avg. loss: 34560.976051\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 45.24, NNZs: 36, Bias: -54.042494, T: 5540040, Avg. loss: 34166.517759\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 38.14, NNZs: 36, Bias: -54.074837, T: 5586207, Avg. loss: 34414.365226\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 92.36, NNZs: 36, Bias: -54.083714, T: 5632374, Avg. loss: 34189.506782\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 52.77, NNZs: 36, Bias: -54.110124, T: 5678541, Avg. loss: 33739.642330\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 38.47, NNZs: 36, Bias: -54.103231, T: 5724708, Avg. loss: 33437.077251\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 40.83, NNZs: 36, Bias: -54.098190, T: 5770875, Avg. loss: 33468.846942\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 71.52, NNZs: 36, Bias: -54.131109, T: 5817042, Avg. loss: 32905.310304\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 91.91, NNZs: 36, Bias: -54.151569, T: 5863209, Avg. loss: 32716.458162\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 41.72, NNZs: 36, Bias: -54.185438, T: 5909376, Avg. loss: 32094.409835\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 129\n",
            "Norm: 74.54, NNZs: 36, Bias: -54.178650, T: 5955543, Avg. loss: 32379.676949\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 130\n",
            "Norm: 91.12, NNZs: 36, Bias: -54.195491, T: 6001710, Avg. loss: 31617.623553\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 131\n",
            "Norm: 50.00, NNZs: 36, Bias: -54.170559, T: 6047877, Avg. loss: 32431.303309\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 132\n",
            "Norm: 41.12, NNZs: 36, Bias: -54.213313, T: 6094044, Avg. loss: 31296.030103\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 133\n",
            "Norm: 55.87, NNZs: 36, Bias: -54.213382, T: 6140211, Avg. loss: 31033.028686\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 134\n",
            "Norm: 52.97, NNZs: 36, Bias: -54.283151, T: 6186378, Avg. loss: 30389.945857\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 135\n",
            "Norm: 77.46, NNZs: 36, Bias: -54.273583, T: 6232545, Avg. loss: 30870.322116\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 136\n",
            "Norm: 91.07, NNZs: 36, Bias: -54.284791, T: 6278712, Avg. loss: 30609.618925\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 137\n",
            "Norm: 63.66, NNZs: 36, Bias: -54.330770, T: 6324879, Avg. loss: 29956.851167\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 138\n",
            "Norm: 103.58, NNZs: 36, Bias: -54.322860, T: 6371046, Avg. loss: 30179.215314\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 139\n",
            "Norm: 72.41, NNZs: 36, Bias: -54.316546, T: 6417213, Avg. loss: 29876.943559\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 140\n",
            "Norm: 51.01, NNZs: 36, Bias: -54.344495, T: 6463380, Avg. loss: 29478.480450\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 141\n",
            "Norm: 71.45, NNZs: 36, Bias: -54.383131, T: 6509547, Avg. loss: 29391.118426\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 142\n",
            "Norm: 80.93, NNZs: 36, Bias: -54.393822, T: 6555714, Avg. loss: 29285.054770\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 143\n",
            "Norm: 96.31, NNZs: 36, Bias: -54.421046, T: 6601881, Avg. loss: 28820.638352\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 144\n",
            "Norm: 46.64, NNZs: 36, Bias: -54.458731, T: 6648048, Avg. loss: 28993.566320\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 145\n",
            "Norm: 43.33, NNZs: 36, Bias: -54.470813, T: 6694215, Avg. loss: 29029.137371\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 146\n",
            "Norm: 38.83, NNZs: 36, Bias: -54.469311, T: 6740382, Avg. loss: 28415.964196\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 147\n",
            "Norm: 69.03, NNZs: 36, Bias: -54.482605, T: 6786549, Avg. loss: 28462.885051\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 148\n",
            "Norm: 39.10, NNZs: 36, Bias: -54.525164, T: 6832716, Avg. loss: 27518.332797\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 149\n",
            "Norm: 52.45, NNZs: 36, Bias: -54.510575, T: 6878883, Avg. loss: 28119.850389\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 150\n",
            "Norm: 39.03, NNZs: 36, Bias: -54.533705, T: 6925050, Avg. loss: 27121.919653\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 151\n",
            "Norm: 39.22, NNZs: 36, Bias: -54.604194, T: 6971217, Avg. loss: 26874.277188\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 152\n",
            "Norm: 55.78, NNZs: 36, Bias: -54.618481, T: 7017384, Avg. loss: 27513.183323\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 153\n",
            "Norm: 46.37, NNZs: 36, Bias: -54.634020, T: 7063551, Avg. loss: 27074.507549\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 154\n",
            "Norm: 42.97, NNZs: 36, Bias: -54.670710, T: 7109718, Avg. loss: 26770.842551\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 155\n",
            "Norm: 45.94, NNZs: 36, Bias: -54.700134, T: 7155885, Avg. loss: 26502.110103\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 156\n",
            "Norm: 43.84, NNZs: 36, Bias: -54.697281, T: 7202052, Avg. loss: 26838.028755\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 157\n",
            "Norm: 58.59, NNZs: 36, Bias: -54.733229, T: 7248219, Avg. loss: 25851.830160\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 158\n",
            "Norm: 38.03, NNZs: 36, Bias: -54.685065, T: 7294386, Avg. loss: 26858.090564\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 159\n",
            "Norm: 72.39, NNZs: 36, Bias: -54.697363, T: 7340553, Avg. loss: 26011.695064\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 160\n",
            "Norm: 38.39, NNZs: 36, Bias: -54.716430, T: 7386720, Avg. loss: 25837.840792\n",
            "Total training time: 1.06 seconds.\n",
            "-- Epoch 161\n",
            "Norm: 38.84, NNZs: 36, Bias: -54.743384, T: 7432887, Avg. loss: 25700.144458\n",
            "Total training time: 1.06 seconds.\n",
            "-- Epoch 162\n",
            "Norm: 39.00, NNZs: 36, Bias: -54.750001, T: 7479054, Avg. loss: 25568.860430\n",
            "Total training time: 1.07 seconds.\n",
            "-- Epoch 163\n",
            "Norm: 72.10, NNZs: 36, Bias: -54.796592, T: 7525221, Avg. loss: 25283.841073\n",
            "Total training time: 1.07 seconds.\n",
            "-- Epoch 164\n",
            "Norm: 40.93, NNZs: 36, Bias: -54.768766, T: 7571388, Avg. loss: 25405.923038\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 165\n",
            "Norm: 42.94, NNZs: 36, Bias: -54.801732, T: 7617555, Avg. loss: 24945.873056\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 166\n",
            "Norm: 60.14, NNZs: 36, Bias: -54.830577, T: 7663722, Avg. loss: 25040.835221\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 167\n",
            "Norm: 40.70, NNZs: 36, Bias: -54.851414, T: 7709889, Avg. loss: 24745.552345\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 168\n",
            "Norm: 45.22, NNZs: 36, Bias: -54.883754, T: 7756056, Avg. loss: 24530.830450\n",
            "Total training time: 1.11 seconds.\n",
            "-- Epoch 169\n",
            "Norm: 68.56, NNZs: 36, Bias: -54.878577, T: 7802223, Avg. loss: 24552.971379\n",
            "Total training time: 1.11 seconds.\n",
            "-- Epoch 170\n",
            "Norm: 52.63, NNZs: 36, Bias: -54.868299, T: 7848390, Avg. loss: 24853.191493\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 171\n",
            "Norm: 41.37, NNZs: 36, Bias: -54.872141, T: 7894557, Avg. loss: 24316.394673\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 172\n",
            "Norm: 67.23, NNZs: 36, Bias: -54.886045, T: 7940724, Avg. loss: 24415.670406\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 173\n",
            "Norm: 39.50, NNZs: 36, Bias: -54.883586, T: 7986891, Avg. loss: 24201.403625\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 174\n",
            "Norm: 52.29, NNZs: 36, Bias: -54.899747, T: 8033058, Avg. loss: 23709.833504\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 175\n",
            "Norm: 51.02, NNZs: 36, Bias: -54.915861, T: 8079225, Avg. loss: 23750.822437\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 176\n",
            "Norm: 65.54, NNZs: 36, Bias: -54.913377, T: 8125392, Avg. loss: 23921.781602\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 177\n",
            "Norm: 46.59, NNZs: 36, Bias: -54.929379, T: 8171559, Avg. loss: 23334.356725\n",
            "Total training time: 1.17 seconds.\n",
            "-- Epoch 178\n",
            "Norm: 48.37, NNZs: 37, Bias: -54.974495, T: 8217726, Avg. loss: 23016.762270\n",
            "Total training time: 1.18 seconds.\n",
            "-- Epoch 179\n",
            "Norm: 48.26, NNZs: 37, Bias: -54.987811, T: 8263893, Avg. loss: 23081.191270\n",
            "Total training time: 1.18 seconds.\n",
            "-- Epoch 180\n",
            "Norm: 42.48, NNZs: 37, Bias: -55.009517, T: 8310060, Avg. loss: 23046.980564\n",
            "Total training time: 1.19 seconds.\n",
            "-- Epoch 181\n",
            "Norm: 73.39, NNZs: 37, Bias: -55.031113, T: 8356227, Avg. loss: 22971.353974\n",
            "Total training time: 1.19 seconds.\n",
            "-- Epoch 182\n",
            "Norm: 58.31, NNZs: 37, Bias: -55.016787, T: 8402394, Avg. loss: 22918.710107\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 183\n",
            "Norm: 40.14, NNZs: 37, Bias: -55.046482, T: 8448561, Avg. loss: 22405.862198\n",
            "Total training time: 1.21 seconds.\n",
            "-- Epoch 184\n",
            "Norm: 38.15, NNZs: 37, Bias: -55.066498, T: 8494728, Avg. loss: 22645.313142\n",
            "Total training time: 1.22 seconds.\n",
            "-- Epoch 185\n",
            "Norm: 51.54, NNZs: 37, Bias: -55.093491, T: 8540895, Avg. loss: 22162.257322\n",
            "Total training time: 1.22 seconds.\n",
            "-- Epoch 186\n",
            "Norm: 70.07, NNZs: 37, Bias: -55.085297, T: 8587062, Avg. loss: 22355.725766\n",
            "Total training time: 1.23 seconds.\n",
            "-- Epoch 187\n",
            "Norm: 58.73, NNZs: 37, Bias: -55.094568, T: 8633229, Avg. loss: 22268.453350\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 188\n",
            "Norm: 38.29, NNZs: 37, Bias: -55.133781, T: 8679396, Avg. loss: 21784.587281\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 189\n",
            "Norm: 40.00, NNZs: 37, Bias: -55.155663, T: 8725563, Avg. loss: 21995.709729\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 190\n",
            "Norm: 64.67, NNZs: 37, Bias: -55.172750, T: 8771730, Avg. loss: 21712.056914\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 191\n",
            "Norm: 38.06, NNZs: 37, Bias: -55.192033, T: 8817897, Avg. loss: 21460.412800\n",
            "Total training time: 1.26 seconds.\n",
            "-- Epoch 192\n",
            "Norm: 38.09, NNZs: 37, Bias: -55.227138, T: 8864064, Avg. loss: 21600.382252\n",
            "Total training time: 1.27 seconds.\n",
            "-- Epoch 193\n",
            "Norm: 38.53, NNZs: 37, Bias: -55.257470, T: 8910231, Avg. loss: 21293.718995\n",
            "Total training time: 1.27 seconds.\n",
            "-- Epoch 194\n",
            "Norm: 56.90, NNZs: 37, Bias: -55.283268, T: 8956398, Avg. loss: 20949.910383\n",
            "Total training time: 1.28 seconds.\n",
            "-- Epoch 195\n",
            "Norm: 57.66, NNZs: 37, Bias: -55.279941, T: 9002565, Avg. loss: 21200.201007\n",
            "Total training time: 1.29 seconds.\n",
            "-- Epoch 196\n",
            "Norm: 39.64, NNZs: 37, Bias: -55.289932, T: 9048732, Avg. loss: 21091.267508\n",
            "Total training time: 1.29 seconds.\n",
            "-- Epoch 197\n",
            "Norm: 38.29, NNZs: 37, Bias: -55.328521, T: 9094899, Avg. loss: 21023.047509\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 198\n",
            "Norm: 40.31, NNZs: 37, Bias: -55.340591, T: 9141066, Avg. loss: 20884.931961\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 199\n",
            "Norm: 42.78, NNZs: 37, Bias: -55.331817, T: 9187233, Avg. loss: 20969.029978\n",
            "Total training time: 1.31 seconds.\n",
            "-- Epoch 200\n",
            "Norm: 50.59, NNZs: 37, Bias: -55.319869, T: 9233400, Avg. loss: 20971.633151\n",
            "Total training time: 1.32 seconds.\n",
            "-- Epoch 201\n",
            "Norm: 39.64, NNZs: 37, Bias: -55.351212, T: 9279567, Avg. loss: 20814.904624\n",
            "Total training time: 1.32 seconds.\n",
            "-- Epoch 202\n",
            "Norm: 40.31, NNZs: 37, Bias: -55.327535, T: 9325734, Avg. loss: 20750.310890\n",
            "Total training time: 1.33 seconds.\n",
            "-- Epoch 203\n",
            "Norm: 38.24, NNZs: 37, Bias: -55.337199, T: 9371901, Avg. loss: 20633.450560\n",
            "Total training time: 1.34 seconds.\n",
            "-- Epoch 204\n",
            "Norm: 66.49, NNZs: 37, Bias: -55.347818, T: 9418068, Avg. loss: 20259.465185\n",
            "Total training time: 1.34 seconds.\n",
            "-- Epoch 205\n",
            "Norm: 41.40, NNZs: 37, Bias: -55.356326, T: 9464235, Avg. loss: 20544.527127\n",
            "Total training time: 1.35 seconds.\n",
            "-- Epoch 206\n",
            "Norm: 73.12, NNZs: 37, Bias: -55.381633, T: 9510402, Avg. loss: 20225.260036\n",
            "Total training time: 1.36 seconds.\n",
            "-- Epoch 207\n",
            "Norm: 43.71, NNZs: 37, Bias: -55.384817, T: 9556569, Avg. loss: 20064.229643\n",
            "Total training time: 1.37 seconds.\n",
            "-- Epoch 208\n",
            "Norm: 50.30, NNZs: 37, Bias: -55.394221, T: 9602736, Avg. loss: 19907.619461\n",
            "Total training time: 1.37 seconds.\n",
            "-- Epoch 209\n",
            "Norm: 60.04, NNZs: 37, Bias: -55.401484, T: 9648903, Avg. loss: 19869.939005\n",
            "Total training time: 1.38 seconds.\n",
            "-- Epoch 210\n",
            "Norm: 61.60, NNZs: 37, Bias: -55.437649, T: 9695070, Avg. loss: 19460.625179\n",
            "Total training time: 1.38 seconds.\n",
            "-- Epoch 211\n",
            "Norm: 38.45, NNZs: 37, Bias: -55.464448, T: 9741237, Avg. loss: 19527.437724\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 212\n",
            "Norm: 39.62, NNZs: 37, Bias: -55.473690, T: 9787404, Avg. loss: 19686.946974\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 213\n",
            "Norm: 54.80, NNZs: 37, Bias: -55.459431, T: 9833571, Avg. loss: 19313.541608\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 214\n",
            "Norm: 38.88, NNZs: 37, Bias: -55.476683, T: 9879738, Avg. loss: 19327.059466\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 215\n",
            "Norm: 41.17, NNZs: 37, Bias: -55.467574, T: 9925905, Avg. loss: 19413.154938\n",
            "Total training time: 1.42 seconds.\n",
            "-- Epoch 216\n",
            "Norm: 59.85, NNZs: 37, Bias: -55.492748, T: 9972072, Avg. loss: 19228.354720\n",
            "Total training time: 1.42 seconds.\n",
            "-- Epoch 217\n",
            "Norm: 49.58, NNZs: 37, Bias: -55.496769, T: 10018239, Avg. loss: 18999.462557\n",
            "Total training time: 1.43 seconds.\n",
            "-- Epoch 218\n",
            "Norm: 45.28, NNZs: 37, Bias: -55.507797, T: 10064406, Avg. loss: 19097.458512\n",
            "Total training time: 1.44 seconds.\n",
            "-- Epoch 219\n",
            "Norm: 41.47, NNZs: 37, Bias: -55.507839, T: 10110573, Avg. loss: 19081.247590\n",
            "Total training time: 1.44 seconds.\n",
            "-- Epoch 220\n",
            "Norm: 42.14, NNZs: 37, Bias: -55.514712, T: 10156740, Avg. loss: 18537.928254\n",
            "Total training time: 1.45 seconds.\n",
            "-- Epoch 221\n",
            "Norm: 62.91, NNZs: 37, Bias: -55.549983, T: 10202907, Avg. loss: 18341.363244\n",
            "Total training time: 1.45 seconds.\n",
            "-- Epoch 222\n",
            "Norm: 48.97, NNZs: 37, Bias: -55.560754, T: 10249074, Avg. loss: 18599.852838\n",
            "Total training time: 1.46 seconds.\n",
            "-- Epoch 223\n",
            "Norm: 53.28, NNZs: 37, Bias: -55.561714, T: 10295241, Avg. loss: 18477.352148\n",
            "Total training time: 1.47 seconds.\n",
            "-- Epoch 224\n",
            "Norm: 39.58, NNZs: 37, Bias: -55.579157, T: 10341408, Avg. loss: 18310.935482\n",
            "Total training time: 1.47 seconds.\n",
            "-- Epoch 225\n",
            "Norm: 38.11, NNZs: 38, Bias: -55.567610, T: 10387575, Avg. loss: 18468.927693\n",
            "Total training time: 1.48 seconds.\n",
            "-- Epoch 226\n",
            "Norm: 39.76, NNZs: 38, Bias: -55.579127, T: 10433742, Avg. loss: 18161.821144\n",
            "Total training time: 1.48 seconds.\n",
            "-- Epoch 227\n",
            "Norm: 38.11, NNZs: 38, Bias: -55.593464, T: 10479909, Avg. loss: 18333.832644\n",
            "Total training time: 1.49 seconds.\n",
            "-- Epoch 228\n",
            "Norm: 52.34, NNZs: 38, Bias: -55.602987, T: 10526076, Avg. loss: 18209.532449\n",
            "Total training time: 1.50 seconds.\n",
            "-- Epoch 229\n",
            "Norm: 38.53, NNZs: 38, Bias: -55.631443, T: 10572243, Avg. loss: 17678.242480\n",
            "Total training time: 1.50 seconds.\n",
            "-- Epoch 230\n",
            "Norm: 53.81, NNZs: 38, Bias: -55.645617, T: 10618410, Avg. loss: 18103.698539\n",
            "Total training time: 1.51 seconds.\n",
            "-- Epoch 231\n",
            "Norm: 45.25, NNZs: 38, Bias: -55.669067, T: 10664577, Avg. loss: 18032.120808\n",
            "Total training time: 1.51 seconds.\n",
            "-- Epoch 232\n",
            "Norm: 43.57, NNZs: 38, Bias: -55.679331, T: 10710744, Avg. loss: 17913.682889\n",
            "Total training time: 1.52 seconds.\n",
            "-- Epoch 233\n",
            "Norm: 38.43, NNZs: 38, Bias: -55.695157, T: 10756911, Avg. loss: 17907.558330\n",
            "Total training time: 1.53 seconds.\n",
            "-- Epoch 234\n",
            "Norm: 38.17, NNZs: 38, Bias: -55.697910, T: 10803078, Avg. loss: 17798.699059\n",
            "Total training time: 1.53 seconds.\n",
            "Convergence after 234 epochs took 1.53 seconds\n",
            "-- Epoch 1\n",
            "Norm: 732.31, NNZs: 7, Bias: -10.787617, T: 46167, Avg. loss: 234621.669312\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 2165.58, NNZs: 9, Bias: -11.275942, T: 92334, Avg. loss: 61124.879525\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 110.72, NNZs: 9, Bias: -11.512615, T: 138501, Avg. loss: 35842.401293\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 152.34, NNZs: 9, Bias: -11.891704, T: 184668, Avg. loss: 24207.369948\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1102.13, NNZs: 9, Bias: -12.277430, T: 230835, Avg. loss: 18728.237204\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1195.47, NNZs: 9, Bias: -12.277047, T: 277002, Avg. loss: 16956.379313\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1046.62, NNZs: 12, Bias: -12.415940, T: 323169, Avg. loss: 13589.578438\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1139.59, NNZs: 12, Bias: -12.331769, T: 369336, Avg. loss: 13458.617141\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 599.55, NNZs: 12, Bias: -12.379901, T: 415503, Avg. loss: 10861.599094\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 746.61, NNZs: 13, Bias: -12.427842, T: 461670, Avg. loss: 9192.633359\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 314.63, NNZs: 13, Bias: -12.409256, T: 507837, Avg. loss: 8774.154830\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 175.25, NNZs: 13, Bias: -12.427799, T: 554004, Avg. loss: 7973.966441\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 91.17, NNZs: 14, Bias: -12.514625, T: 600171, Avg. loss: 6612.392204\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 136.23, NNZs: 14, Bias: -12.561297, T: 646338, Avg. loss: 6326.115067\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 820.87, NNZs: 15, Bias: -12.606348, T: 692505, Avg. loss: 5886.909640\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 66.84, NNZs: 15, Bias: -12.564315, T: 738672, Avg. loss: 6611.639750\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 562.44, NNZs: 15, Bias: -12.577511, T: 784839, Avg. loss: 6118.105844\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 75.68, NNZs: 15, Bias: -12.552441, T: 831006, Avg. loss: 5393.131973\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 177.62, NNZs: 15, Bias: -12.553209, T: 877173, Avg. loss: 5273.954730\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 184.70, NNZs: 15, Bias: -12.620312, T: 923340, Avg. loss: 4194.679893\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 363.06, NNZs: 15, Bias: -12.651529, T: 969507, Avg. loss: 4341.504206\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 30.41, NNZs: 16, Bias: -12.641980, T: 1015674, Avg. loss: 4336.399502\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 350.84, NNZs: 16, Bias: -12.680709, T: 1061841, Avg. loss: 3907.385112\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 204.44, NNZs: 16, Bias: -12.726828, T: 1108008, Avg. loss: 3709.097029\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 52.66, NNZs: 16, Bias: -12.744599, T: 1154175, Avg. loss: 3618.058084\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 59.83, NNZs: 16, Bias: -12.727867, T: 1200342, Avg. loss: 3764.097439\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 76.76, NNZs: 16, Bias: -12.775937, T: 1246509, Avg. loss: 3043.417346\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 119.35, NNZs: 16, Bias: -12.768087, T: 1292676, Avg. loss: 3479.827077\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 195.05, NNZs: 16, Bias: -12.775381, T: 1338843, Avg. loss: 3106.046922\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 66.63, NNZs: 16, Bias: -12.783026, T: 1385010, Avg. loss: 3088.089268\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 51.50, NNZs: 16, Bias: -12.782763, T: 1431177, Avg. loss: 3136.202540\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 8.84, NNZs: 16, Bias: -12.796120, T: 1477344, Avg. loss: 3024.344731\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 232.25, NNZs: 16, Bias: -12.789416, T: 1523511, Avg. loss: 3042.581483\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 20.07, NNZs: 16, Bias: -12.789542, T: 1569678, Avg. loss: 2669.274730\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 326.46, NNZs: 16, Bias: -12.796040, T: 1615845, Avg. loss: 2608.671144\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 39.29, NNZs: 16, Bias: -12.789734, T: 1662012, Avg. loss: 2692.651677\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 82.03, NNZs: 16, Bias: -12.777821, T: 1708179, Avg. loss: 2741.118457\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 18.41, NNZs: 16, Bias: -12.771810, T: 1754346, Avg. loss: 2458.353575\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 178.89, NNZs: 16, Bias: -12.782990, T: 1800513, Avg. loss: 2442.457400\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 151.02, NNZs: 17, Bias: -12.766654, T: 1846680, Avg. loss: 2580.690094\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 155.79, NNZs: 17, Bias: -12.761187, T: 1892847, Avg. loss: 2353.517108\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 133.83, NNZs: 17, Bias: -12.771763, T: 1939014, Avg. loss: 2146.758293\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 65.54, NNZs: 17, Bias: -12.782045, T: 1985181, Avg. loss: 2091.780472\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 2.75, NNZs: 17, Bias: -12.782101, T: 2031348, Avg. loss: 2088.072382\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 22.85, NNZs: 17, Bias: -12.791832, T: 2077515, Avg. loss: 1929.150942\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 169.30, NNZs: 17, Bias: -12.820462, T: 2123682, Avg. loss: 1836.642947\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 15.99, NNZs: 17, Bias: -12.825003, T: 2169849, Avg. loss: 1922.629945\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 239.43, NNZs: 17, Bias: -12.829475, T: 2216016, Avg. loss: 2037.745761\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 170.04, NNZs: 17, Bias: -12.833961, T: 2262183, Avg. loss: 1874.067134\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 201.82, NNZs: 17, Bias: -12.825163, T: 2308350, Avg. loss: 1917.392284\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 70.94, NNZs: 17, Bias: -12.820731, T: 2354517, Avg. loss: 1821.655814\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 69.05, NNZs: 17, Bias: -12.816579, T: 2400684, Avg. loss: 1905.717903\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 157.06, NNZs: 17, Bias: -12.824862, T: 2446851, Avg. loss: 1737.578369\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 186.39, NNZs: 17, Bias: -12.816722, T: 2493018, Avg. loss: 1739.459688\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 98.97, NNZs: 17, Bias: -12.816881, T: 2539185, Avg. loss: 1736.419413\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 39.75, NNZs: 17, Bias: -12.820858, T: 2585352, Avg. loss: 1624.869968\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 69.64, NNZs: 17, Bias: -12.820791, T: 2631519, Avg. loss: 1589.143777\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 122.30, NNZs: 17, Bias: -12.820859, T: 2677686, Avg. loss: 1594.645002\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 125.15, NNZs: 17, Bias: -12.820783, T: 2723853, Avg. loss: 1614.118432\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 2.74, NNZs: 17, Bias: -12.817091, T: 2770020, Avg. loss: 1570.238380\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 15.42, NNZs: 17, Bias: -12.824326, T: 2816187, Avg. loss: 1532.287418\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 66.50, NNZs: 17, Bias: -12.824367, T: 2862354, Avg. loss: 1516.639793\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 67.81, NNZs: 17, Bias: -12.834805, T: 2908521, Avg. loss: 1400.827738\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 97.84, NNZs: 17, Bias: -12.838190, T: 2954688, Avg. loss: 1392.344113\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 49.34, NNZs: 17, Bias: -12.831506, T: 3000855, Avg. loss: 1511.502413\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 8.14, NNZs: 17, Bias: -12.851298, T: 3047022, Avg. loss: 1346.734026\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 25.53, NNZs: 17, Bias: -12.861038, T: 3093189, Avg. loss: 1416.729355\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 22.53, NNZs: 17, Bias: -12.860980, T: 3139356, Avg. loss: 1357.002651\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 181.44, NNZs: 17, Bias: -12.870402, T: 3185523, Avg. loss: 1306.009837\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 15.12, NNZs: 17, Bias: -12.867261, T: 3231690, Avg. loss: 1334.167610\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 134.75, NNZs: 17, Bias: -12.870305, T: 3277857, Avg. loss: 1366.023923\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 87.62, NNZs: 17, Bias: -12.873355, T: 3324024, Avg. loss: 1257.046036\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 183.91, NNZs: 17, Bias: -12.879262, T: 3370191, Avg. loss: 1199.353212\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 38.79, NNZs: 17, Bias: -12.882169, T: 3416358, Avg. loss: 1361.059941\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 43.40, NNZs: 17, Bias: -12.887983, T: 3462525, Avg. loss: 1294.840261\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 81.67, NNZs: 17, Bias: -12.885140, T: 3508692, Avg. loss: 1248.977024\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 158.24, NNZs: 17, Bias: -12.887978, T: 3554859, Avg. loss: 1155.698461\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 44.44, NNZs: 17, Bias: -12.876834, T: 3601026, Avg. loss: 1292.073938\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 39.34, NNZs: 17, Bias: -12.874095, T: 3647193, Avg. loss: 1201.596729\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 43.00, NNZs: 17, Bias: -12.882240, T: 3693360, Avg. loss: 1054.972018\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 120.90, NNZs: 17, Bias: -12.882227, T: 3739527, Avg. loss: 1184.592823\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 123.47, NNZs: 17, Bias: -12.884878, T: 3785694, Avg. loss: 1126.355490\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 26.17, NNZs: 17, Bias: -12.879625, T: 3831861, Avg. loss: 1160.567854\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 8.70, NNZs: 17, Bias: -12.887445, T: 3878028, Avg. loss: 999.125052\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 85.71, NNZs: 17, Bias: -12.887435, T: 3924195, Avg. loss: 1041.272964\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 63.83, NNZs: 17, Bias: -12.895046, T: 3970362, Avg. loss: 1002.634464\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 67.29, NNZs: 17, Bias: -12.897585, T: 4016529, Avg. loss: 1009.327446\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 53.53, NNZs: 17, Bias: -12.900049, T: 4062696, Avg. loss: 1042.864809\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 10.18, NNZs: 17, Bias: -12.909826, T: 4108863, Avg. loss: 977.517094\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 89.13, NNZs: 17, Bias: -12.914663, T: 4155030, Avg. loss: 989.886371\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 122.45, NNZs: 18, Bias: -12.919407, T: 4201197, Avg. loss: 960.935567\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 92.63, NNZs: 18, Bias: -12.919398, T: 4247364, Avg. loss: 1082.863378\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 30.90, NNZs: 18, Bias: -12.917052, T: 4293531, Avg. loss: 1007.451224\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 45.42, NNZs: 18, Bias: -12.917065, T: 4339698, Avg. loss: 994.580574\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 22.03, NNZs: 18, Bias: -12.921634, T: 4385865, Avg. loss: 986.268961\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 2.92, NNZs: 18, Bias: -12.923896, T: 4432032, Avg. loss: 897.964661\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 22.15, NNZs: 18, Bias: -12.926176, T: 4478199, Avg. loss: 961.726707\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 93.49, NNZs: 18, Bias: -12.930595, T: 4524366, Avg. loss: 956.937334\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 20.90, NNZs: 18, Bias: -12.926189, T: 4570533, Avg. loss: 936.413037\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 13.66, NNZs: 18, Bias: -12.930557, T: 4616700, Avg. loss: 873.702784\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 97.26, NNZs: 18, Bias: -12.932697, T: 4662867, Avg. loss: 881.812505\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 42.85, NNZs: 18, Bias: -12.928428, T: 4709034, Avg. loss: 954.346669\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 107.92, NNZs: 18, Bias: -12.930533, T: 4755201, Avg. loss: 904.450354\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 58.22, NNZs: 18, Bias: -12.934759, T: 4801368, Avg. loss: 878.255461\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 3.69, NNZs: 18, Bias: -12.936851, T: 4847535, Avg. loss: 868.996617\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 26.37, NNZs: 18, Bias: -12.934802, T: 4893702, Avg. loss: 893.145653\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 64.62, NNZs: 18, Bias: -12.940898, T: 4939869, Avg. loss: 826.019983\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 87.23, NNZs: 18, Bias: -12.942925, T: 4986036, Avg. loss: 797.856725\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 37.17, NNZs: 18, Bias: -12.946896, T: 5032203, Avg. loss: 787.145659\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 21.58, NNZs: 18, Bias: -12.944923, T: 5078370, Avg. loss: 823.686434\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 4.01, NNZs: 18, Bias: -12.952749, T: 5124537, Avg. loss: 752.717765\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 39.39, NNZs: 18, Bias: -12.956657, T: 5170704, Avg. loss: 762.860243\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 39.08, NNZs: 18, Bias: -12.960497, T: 5216871, Avg. loss: 825.195982\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 53.71, NNZs: 18, Bias: -12.956705, T: 5263038, Avg. loss: 848.783641\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 2.82, NNZs: 18, Bias: -12.952932, T: 5309205, Avg. loss: 819.294191\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 69.33, NNZs: 18, Bias: -12.958573, T: 5355372, Avg. loss: 736.720525\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 22.12, NNZs: 18, Bias: -12.958562, T: 5401539, Avg. loss: 724.430600\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 99.67, NNZs: 18, Bias: -12.964110, T: 5447706, Avg. loss: 725.449934\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 27.59, NNZs: 18, Bias: -12.962294, T: 5493873, Avg. loss: 771.314111\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 38.25, NNZs: 18, Bias: -12.964084, T: 5540040, Avg. loss: 802.190084\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 26.05, NNZs: 18, Bias: -12.962288, T: 5586207, Avg. loss: 805.038528\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 4.96, NNZs: 18, Bias: -12.962274, T: 5632374, Avg. loss: 754.306896\n",
            "Total training time: 0.77 seconds.\n",
            "Convergence after 122 epochs took 0.77 seconds\n",
            "-- Epoch 1\n",
            "Norm: 261.83, NNZs: 28, Bias: 136.711333, T: 46167, Avg. loss: 92871770.192488\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1743.47, NNZs: 31, Bias: 153.679342, T: 92334, Avg. loss: 16924967.390527\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 848.90, NNZs: 31, Bias: 148.928612, T: 138501, Avg. loss: 9879625.137050\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1001.74, NNZs: 33, Bias: 152.529111, T: 184668, Avg. loss: 7082680.463838\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 644.17, NNZs: 33, Bias: 151.869711, T: 230835, Avg. loss: 5434593.630552\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 315.38, NNZs: 33, Bias: 153.036965, T: 277002, Avg. loss: 4493488.614052\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 364.31, NNZs: 34, Bias: 153.456537, T: 323169, Avg. loss: 3808635.956715\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 919.58, NNZs: 34, Bias: 153.957963, T: 369336, Avg. loss: 3272695.705558\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1034.72, NNZs: 34, Bias: 154.874505, T: 415503, Avg. loss: 2897096.118994\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 659.66, NNZs: 35, Bias: 155.685436, T: 461670, Avg. loss: 2589082.574452\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 873.33, NNZs: 35, Bias: 155.894973, T: 507837, Avg. loss: 2329428.695536\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 245.11, NNZs: 35, Bias: 157.528583, T: 554004, Avg. loss: 2186434.636089\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 536.09, NNZs: 35, Bias: 157.830429, T: 600171, Avg. loss: 1948160.379872\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 487.70, NNZs: 35, Bias: 158.827504, T: 646338, Avg. loss: 1849877.973534\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 381.84, NNZs: 35, Bias: 160.647316, T: 692505, Avg. loss: 1733598.618123\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 294.45, NNZs: 35, Bias: 161.112443, T: 738672, Avg. loss: 1581878.251678\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 404.87, NNZs: 36, Bias: 162.104801, T: 784839, Avg. loss: 1505584.573733\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 481.36, NNZs: 36, Bias: 162.924549, T: 831006, Avg. loss: 1410852.107907\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 592.97, NNZs: 37, Bias: 163.917992, T: 877173, Avg. loss: 1342169.210168\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 284.83, NNZs: 37, Bias: 164.866537, T: 923340, Avg. loss: 1269680.233921\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 389.61, NNZs: 37, Bias: 164.989106, T: 969507, Avg. loss: 1194836.113620\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 356.16, NNZs: 37, Bias: 165.116730, T: 1015674, Avg. loss: 1138944.158202\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 287.73, NNZs: 37, Bias: 165.129398, T: 1061841, Avg. loss: 1097964.795031\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 561.12, NNZs: 38, Bias: 165.484303, T: 1108008, Avg. loss: 1037863.133136\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 235.50, NNZs: 38, Bias: 165.739357, T: 1154175, Avg. loss: 1001810.481458\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 239.30, NNZs: 38, Bias: 166.275818, T: 1200342, Avg. loss: 967588.203188\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 235.86, NNZs: 38, Bias: 166.421827, T: 1246509, Avg. loss: 936399.835223\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 453.98, NNZs: 38, Bias: 166.716244, T: 1292676, Avg. loss: 888886.968375\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 235.29, NNZs: 38, Bias: 166.997871, T: 1338843, Avg. loss: 878476.291801\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 300.56, NNZs: 38, Bias: 166.933710, T: 1385010, Avg. loss: 824992.378506\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 256.64, NNZs: 38, Bias: 167.361195, T: 1431177, Avg. loss: 809662.235167\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 315.67, NNZs: 38, Bias: 167.850728, T: 1477344, Avg. loss: 780120.984031\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 448.61, NNZs: 38, Bias: 168.256533, T: 1523511, Avg. loss: 753776.818929\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 385.38, NNZs: 38, Bias: 168.314275, T: 1569678, Avg. loss: 725293.259159\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 236.14, NNZs: 38, Bias: 168.515727, T: 1615845, Avg. loss: 713972.528938\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 358.66, NNZs: 38, Bias: 168.633305, T: 1662012, Avg. loss: 696522.985468\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 237.53, NNZs: 38, Bias: 168.600896, T: 1708179, Avg. loss: 674288.008559\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 246.41, NNZs: 38, Bias: 168.835913, T: 1754346, Avg. loss: 661694.358346\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 357.46, NNZs: 38, Bias: 168.858533, T: 1800513, Avg. loss: 643571.120960\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 266.30, NNZs: 38, Bias: 169.145287, T: 1846680, Avg. loss: 621909.304575\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 297.73, NNZs: 38, Bias: 169.340740, T: 1892847, Avg. loss: 612714.807610\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 318.23, NNZs: 38, Bias: 169.468597, T: 1939014, Avg. loss: 589388.882605\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 240.30, NNZs: 38, Bias: 169.570175, T: 1985181, Avg. loss: 582760.483627\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 250.47, NNZs: 38, Bias: 169.737400, T: 2031348, Avg. loss: 570331.876362\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 258.16, NNZs: 38, Bias: 169.705990, T: 2077515, Avg. loss: 550523.805956\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 326.21, NNZs: 38, Bias: 169.711002, T: 2123682, Avg. loss: 538629.732074\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 241.07, NNZs: 38, Bias: 170.189521, T: 2169849, Avg. loss: 536678.750820\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 246.23, NNZs: 38, Bias: 170.110484, T: 2216016, Avg. loss: 523395.480357\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 279.56, NNZs: 38, Bias: 170.293746, T: 2262183, Avg. loss: 508077.069037\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 240.83, NNZs: 38, Bias: 170.544357, T: 2308350, Avg. loss: 500277.176984\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 320.46, NNZs: 38, Bias: 170.621042, T: 2354517, Avg. loss: 486544.186148\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 241.12, NNZs: 38, Bias: 170.901795, T: 2400684, Avg. loss: 479767.030159\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 242.82, NNZs: 38, Bias: 171.120900, T: 2446851, Avg. loss: 471739.803008\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 259.59, NNZs: 38, Bias: 171.424660, T: 2493018, Avg. loss: 462940.975204\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 244.01, NNZs: 38, Bias: 171.611830, T: 2539185, Avg. loss: 452467.045984\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 239.18, NNZs: 38, Bias: 171.673249, T: 2585352, Avg. loss: 443410.467501\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 295.03, NNZs: 38, Bias: 171.602180, T: 2631519, Avg. loss: 436070.353830\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 234.91, NNZs: 38, Bias: 171.718186, T: 2677686, Avg. loss: 428760.680215\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 233.25, NNZs: 38, Bias: 171.579944, T: 2723853, Avg. loss: 421287.419788\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 235.02, NNZs: 38, Bias: 171.747878, T: 2770020, Avg. loss: 408794.254753\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 300.41, NNZs: 38, Bias: 172.019361, T: 2816187, Avg. loss: 408975.978520\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 233.60, NNZs: 38, Bias: 172.159086, T: 2862354, Avg. loss: 404518.854946\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 245.74, NNZs: 38, Bias: 172.272846, T: 2908521, Avg. loss: 393865.459757\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 246.84, NNZs: 38, Bias: 172.328731, T: 2954688, Avg. loss: 386222.678002\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 249.77, NNZs: 38, Bias: 172.401640, T: 3000855, Avg. loss: 384051.250450\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 261.53, NNZs: 38, Bias: 172.479732, T: 3047022, Avg. loss: 378036.663643\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 233.67, NNZs: 38, Bias: 172.483309, T: 3093189, Avg. loss: 372371.489600\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 275.54, NNZs: 38, Bias: 172.586659, T: 3139356, Avg. loss: 369407.252382\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 243.62, NNZs: 38, Bias: 172.633071, T: 3185523, Avg. loss: 362652.658135\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 250.79, NNZs: 38, Bias: 172.649199, T: 3231690, Avg. loss: 357564.359925\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 242.52, NNZs: 38, Bias: 172.537837, T: 3277857, Avg. loss: 352615.521821\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 242.73, NNZs: 38, Bias: 172.662641, T: 3324024, Avg. loss: 346835.611405\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 278.18, NNZs: 38, Bias: 172.716085, T: 3370191, Avg. loss: 341867.532427\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 234.18, NNZs: 38, Bias: 172.748137, T: 3416358, Avg. loss: 338821.899431\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 233.29, NNZs: 38, Bias: 172.800036, T: 3462525, Avg. loss: 333468.416668\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 257.56, NNZs: 38, Bias: 172.956897, T: 3508692, Avg. loss: 328553.199274\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 233.97, NNZs: 38, Bias: 173.123679, T: 3554859, Avg. loss: 322290.034089\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 242.19, NNZs: 38, Bias: 173.168383, T: 3601026, Avg. loss: 317851.587822\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 253.57, NNZs: 38, Bias: 173.151477, T: 3647193, Avg. loss: 314688.858788\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 233.53, NNZs: 38, Bias: 173.049640, T: 3693360, Avg. loss: 308533.851773\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 242.46, NNZs: 38, Bias: 173.121007, T: 3739527, Avg. loss: 303855.159733\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 269.11, NNZs: 38, Bias: 173.224964, T: 3785694, Avg. loss: 304369.884912\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 239.79, NNZs: 38, Bias: 173.308450, T: 3831861, Avg. loss: 297271.014768\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 243.79, NNZs: 38, Bias: 173.347622, T: 3878028, Avg. loss: 295495.768937\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 236.91, NNZs: 38, Bias: 173.534972, T: 3924195, Avg. loss: 292437.541897\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 233.85, NNZs: 38, Bias: 173.616595, T: 3970362, Avg. loss: 288149.529330\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 237.12, NNZs: 38, Bias: 173.670937, T: 4016529, Avg. loss: 285716.444523\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 233.64, NNZs: 38, Bias: 173.792123, T: 4062696, Avg. loss: 280564.625156\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 238.79, NNZs: 38, Bias: 173.711439, T: 4108863, Avg. loss: 278080.076914\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 233.96, NNZs: 38, Bias: 173.940470, T: 4155030, Avg. loss: 277444.011181\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 267.53, NNZs: 38, Bias: 173.962049, T: 4201197, Avg. loss: 270788.550687\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 233.30, NNZs: 38, Bias: 174.236188, T: 4247364, Avg. loss: 274985.371373\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 243.59, NNZs: 38, Bias: 174.170693, T: 4293531, Avg. loss: 265913.357902\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 235.00, NNZs: 38, Bias: 174.131574, T: 4339698, Avg. loss: 264210.622030\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 254.97, NNZs: 38, Bias: 174.165480, T: 4385865, Avg. loss: 262036.615097\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 235.85, NNZs: 38, Bias: 174.081842, T: 4432032, Avg. loss: 257753.995696\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 261.43, NNZs: 38, Bias: 174.021418, T: 4478199, Avg. loss: 254989.906721\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 234.17, NNZs: 38, Bias: 174.149928, T: 4524366, Avg. loss: 248585.356754\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 237.51, NNZs: 38, Bias: 174.279109, T: 4570533, Avg. loss: 251145.447102\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 246.53, NNZs: 38, Bias: 174.311925, T: 4616700, Avg. loss: 250564.058941\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 233.49, NNZs: 38, Bias: 174.486016, T: 4662867, Avg. loss: 250643.550225\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 233.73, NNZs: 38, Bias: 174.492413, T: 4709034, Avg. loss: 243306.514552\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 247.58, NNZs: 38, Bias: 174.462853, T: 4755201, Avg. loss: 239490.665644\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 234.92, NNZs: 38, Bias: 174.535971, T: 4801368, Avg. loss: 239472.827522\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 236.48, NNZs: 38, Bias: 174.598031, T: 4847535, Avg. loss: 236992.281164\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 233.04, NNZs: 38, Bias: 174.635547, T: 4893702, Avg. loss: 232625.534044\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 233.08, NNZs: 38, Bias: 174.661948, T: 4939869, Avg. loss: 232271.797648\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 247.72, NNZs: 38, Bias: 174.797256, T: 4986036, Avg. loss: 231229.629518\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 234.48, NNZs: 38, Bias: 174.911019, T: 5032203, Avg. loss: 225708.084439\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 236.32, NNZs: 38, Bias: 174.810509, T: 5078370, Avg. loss: 225274.793785\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 241.12, NNZs: 38, Bias: 174.916437, T: 5124537, Avg. loss: 221834.666086\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 241.15, NNZs: 38, Bias: 174.858251, T: 5170704, Avg. loss: 221667.448475\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 258.98, NNZs: 38, Bias: 174.814315, T: 5216871, Avg. loss: 216646.050764\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 233.12, NNZs: 38, Bias: 174.925015, T: 5263038, Avg. loss: 219872.636672\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 245.95, NNZs: 38, Bias: 174.879475, T: 5309205, Avg. loss: 214563.468490\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 233.44, NNZs: 38, Bias: 174.916726, T: 5355372, Avg. loss: 212206.925284\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 233.40, NNZs: 38, Bias: 175.013611, T: 5401539, Avg. loss: 212008.129413\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 232.77, NNZs: 38, Bias: 175.017094, T: 5447706, Avg. loss: 210084.548242\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 235.30, NNZs: 38, Bias: 175.048304, T: 5493873, Avg. loss: 206169.437841\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 233.02, NNZs: 38, Bias: 175.019504, T: 5540040, Avg. loss: 206006.339358\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 233.10, NNZs: 38, Bias: 175.010602, T: 5586207, Avg. loss: 206308.769539\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 235.40, NNZs: 38, Bias: 175.067919, T: 5632374, Avg. loss: 203366.665393\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 240.32, NNZs: 38, Bias: 175.138725, T: 5678541, Avg. loss: 203332.227006\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 234.14, NNZs: 38, Bias: 175.154610, T: 5724708, Avg. loss: 200893.909878\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 234.30, NNZs: 38, Bias: 175.191175, T: 5770875, Avg. loss: 200033.766923\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 245.43, NNZs: 38, Bias: 175.227604, T: 5817042, Avg. loss: 196815.667957\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 233.42, NNZs: 38, Bias: 175.355949, T: 5863209, Avg. loss: 195789.624357\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 233.84, NNZs: 38, Bias: 175.330419, T: 5909376, Avg. loss: 193101.403853\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 129\n",
            "Norm: 236.88, NNZs: 38, Bias: 175.502538, T: 5955543, Avg. loss: 192984.710221\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 130\n",
            "Norm: 233.37, NNZs: 38, Bias: 175.552299, T: 6001710, Avg. loss: 192578.776392\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 131\n",
            "Norm: 235.07, NNZs: 38, Bias: 175.555764, T: 6047877, Avg. loss: 190320.035417\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 132\n",
            "Norm: 233.10, NNZs: 38, Bias: 175.525945, T: 6094044, Avg. loss: 188204.850389\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 133\n",
            "Norm: 235.46, NNZs: 38, Bias: 175.579829, T: 6140211, Avg. loss: 187701.487152\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 134\n",
            "Norm: 242.52, NNZs: 38, Bias: 175.607484, T: 6186378, Avg. loss: 184514.862376\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 135\n",
            "Norm: 233.34, NNZs: 38, Bias: 175.625107, T: 6232545, Avg. loss: 183470.745509\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 136\n",
            "Norm: 234.06, NNZs: 38, Bias: 175.653870, T: 6278712, Avg. loss: 182080.670442\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 137\n",
            "Norm: 234.45, NNZs: 38, Bias: 175.742557, T: 6324879, Avg. loss: 181578.722965\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 138\n",
            "Norm: 236.76, NNZs: 38, Bias: 175.838601, T: 6371046, Avg. loss: 178651.800435\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 139\n",
            "Norm: 236.25, NNZs: 38, Bias: 175.843755, T: 6417213, Avg. loss: 178930.684078\n",
            "Total training time: 0.97 seconds.\n",
            "-- Epoch 140\n",
            "Norm: 236.10, NNZs: 38, Bias: 175.879400, T: 6463380, Avg. loss: 174114.778660\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 141\n",
            "Norm: 235.07, NNZs: 38, Bias: 175.791437, T: 6509547, Avg. loss: 177534.540132\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 142\n",
            "Norm: 232.74, NNZs: 38, Bias: 175.794320, T: 6555714, Avg. loss: 173850.058065\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 143\n",
            "Norm: 239.49, NNZs: 38, Bias: 175.884053, T: 6601881, Avg. loss: 172611.239053\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 144\n",
            "Norm: 235.65, NNZs: 38, Bias: 175.961065, T: 6648048, Avg. loss: 173450.490154\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 145\n",
            "Norm: 244.09, NNZs: 38, Bias: 175.985177, T: 6694215, Avg. loss: 171656.710927\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 146\n",
            "Norm: 244.53, NNZs: 38, Bias: 176.144465, T: 6740382, Avg. loss: 169430.923115\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 147\n",
            "Norm: 238.65, NNZs: 38, Bias: 176.236127, T: 6786549, Avg. loss: 170245.387969\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 148\n",
            "Norm: 237.33, NNZs: 38, Bias: 176.240465, T: 6832716, Avg. loss: 167944.904915\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 149\n",
            "Norm: 232.82, NNZs: 38, Bias: 176.272384, T: 6878883, Avg. loss: 166530.386268\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 150\n",
            "Norm: 232.66, NNZs: 38, Bias: 176.344796, T: 6925050, Avg. loss: 166231.251878\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 151\n",
            "Norm: 245.49, NNZs: 38, Bias: 176.329044, T: 6971217, Avg. loss: 164330.369150\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 152\n",
            "Norm: 236.77, NNZs: 38, Bias: 176.351982, T: 7017384, Avg. loss: 162499.792372\n",
            "Total training time: 1.06 seconds.\n",
            "-- Epoch 153\n",
            "Norm: 233.59, NNZs: 38, Bias: 176.401575, T: 7063551, Avg. loss: 160359.973761\n",
            "Total training time: 1.07 seconds.\n",
            "-- Epoch 154\n",
            "Norm: 243.30, NNZs: 38, Bias: 176.421321, T: 7109718, Avg. loss: 161323.757055\n",
            "Total training time: 1.07 seconds.\n",
            "-- Epoch 155\n",
            "Norm: 235.25, NNZs: 38, Bias: 176.408629, T: 7155885, Avg. loss: 159501.608283\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 156\n",
            "Norm: 232.91, NNZs: 38, Bias: 176.422493, T: 7202052, Avg. loss: 156437.903188\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 157\n",
            "Norm: 232.75, NNZs: 38, Bias: 176.469530, T: 7248219, Avg. loss: 157003.272070\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 158\n",
            "Norm: 232.67, NNZs: 38, Bias: 176.467896, T: 7294386, Avg. loss: 155602.262220\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 159\n",
            "Norm: 245.29, NNZs: 38, Bias: 176.507331, T: 7340553, Avg. loss: 155716.210429\n",
            "Total training time: 1.11 seconds.\n",
            "-- Epoch 160\n",
            "Norm: 233.60, NNZs: 38, Bias: 176.558833, T: 7386720, Avg. loss: 155588.006444\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 161\n",
            "Norm: 239.13, NNZs: 38, Bias: 176.537375, T: 7432887, Avg. loss: 153940.408164\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 162\n",
            "Norm: 235.28, NNZs: 38, Bias: 176.511809, T: 7479054, Avg. loss: 152763.519801\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 163\n",
            "Norm: 232.93, NNZs: 38, Bias: 176.519869, T: 7525221, Avg. loss: 154244.171474\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 164\n",
            "Norm: 234.82, NNZs: 38, Bias: 176.592563, T: 7571388, Avg. loss: 152574.309884\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 165\n",
            "Norm: 233.02, NNZs: 38, Bias: 176.634756, T: 7617555, Avg. loss: 149406.499765\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 166\n",
            "Norm: 238.63, NNZs: 38, Bias: 176.702767, T: 7663722, Avg. loss: 150665.322037\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 167\n",
            "Norm: 236.63, NNZs: 38, Bias: 176.731469, T: 7709889, Avg. loss: 148714.648430\n",
            "Total training time: 1.17 seconds.\n",
            "-- Epoch 168\n",
            "Norm: 235.04, NNZs: 38, Bias: 176.855679, T: 7756056, Avg. loss: 147949.657805\n",
            "Total training time: 1.18 seconds.\n",
            "-- Epoch 169\n",
            "Norm: 232.78, NNZs: 38, Bias: 176.860861, T: 7802223, Avg. loss: 148522.885020\n",
            "Total training time: 1.19 seconds.\n",
            "-- Epoch 170\n",
            "Norm: 233.69, NNZs: 38, Bias: 176.805917, T: 7848390, Avg. loss: 144389.687681\n",
            "Total training time: 1.19 seconds.\n",
            "-- Epoch 171\n",
            "Norm: 237.38, NNZs: 38, Bias: 176.854230, T: 7894557, Avg. loss: 145173.110797\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 172\n",
            "Norm: 232.80, NNZs: 38, Bias: 176.919851, T: 7940724, Avg. loss: 143397.294939\n",
            "Total training time: 1.21 seconds.\n",
            "-- Epoch 173\n",
            "Norm: 235.15, NNZs: 38, Bias: 176.972381, T: 7986891, Avg. loss: 143411.322253\n",
            "Total training time: 1.21 seconds.\n",
            "-- Epoch 174\n",
            "Norm: 233.22, NNZs: 38, Bias: 177.004914, T: 8033058, Avg. loss: 142421.840712\n",
            "Total training time: 1.22 seconds.\n",
            "-- Epoch 175\n",
            "Norm: 233.27, NNZs: 38, Bias: 176.958942, T: 8079225, Avg. loss: 140377.037361\n",
            "Total training time: 1.23 seconds.\n",
            "-- Epoch 176\n",
            "Norm: 232.93, NNZs: 38, Bias: 176.931793, T: 8125392, Avg. loss: 141574.733980\n",
            "Total training time: 1.23 seconds.\n",
            "-- Epoch 177\n",
            "Norm: 237.79, NNZs: 38, Bias: 176.956246, T: 8171559, Avg. loss: 140886.780207\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 178\n",
            "Norm: 232.82, NNZs: 38, Bias: 176.923432, T: 8217726, Avg. loss: 137311.467238\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 179\n",
            "Norm: 235.28, NNZs: 38, Bias: 176.887176, T: 8263893, Avg. loss: 137774.440608\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 180\n",
            "Norm: 232.80, NNZs: 38, Bias: 176.910032, T: 8310060, Avg. loss: 136053.912697\n",
            "Total training time: 1.26 seconds.\n",
            "-- Epoch 181\n",
            "Norm: 234.11, NNZs: 38, Bias: 176.920878, T: 8356227, Avg. loss: 135738.033921\n",
            "Total training time: 1.27 seconds.\n",
            "-- Epoch 182\n",
            "Norm: 235.51, NNZs: 38, Bias: 176.913737, T: 8402394, Avg. loss: 135675.391927\n",
            "Total training time: 1.27 seconds.\n",
            "-- Epoch 183\n",
            "Norm: 236.27, NNZs: 38, Bias: 176.942162, T: 8448561, Avg. loss: 135182.544953\n",
            "Total training time: 1.28 seconds.\n",
            "-- Epoch 184\n",
            "Norm: 232.80, NNZs: 38, Bias: 176.997657, T: 8494728, Avg. loss: 136039.738371\n",
            "Total training time: 1.28 seconds.\n",
            "-- Epoch 185\n",
            "Norm: 238.27, NNZs: 38, Bias: 176.977751, T: 8540895, Avg. loss: 135196.977075\n",
            "Total training time: 1.29 seconds.\n",
            "-- Epoch 186\n",
            "Norm: 241.48, NNZs: 38, Bias: 176.973176, T: 8587062, Avg. loss: 132775.878377\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 187\n",
            "Norm: 234.62, NNZs: 38, Bias: 176.995231, T: 8633229, Avg. loss: 132601.995821\n",
            "Total training time: 1.30 seconds.\n",
            "-- Epoch 188\n",
            "Norm: 239.05, NNZs: 38, Bias: 177.055255, T: 8679396, Avg. loss: 132630.868179\n",
            "Total training time: 1.31 seconds.\n",
            "-- Epoch 189\n",
            "Norm: 235.60, NNZs: 38, Bias: 177.062143, T: 8725563, Avg. loss: 130896.281403\n",
            "Total training time: 1.33 seconds.\n",
            "-- Epoch 190\n",
            "Norm: 232.67, NNZs: 38, Bias: 177.072536, T: 8771730, Avg. loss: 130529.833448\n",
            "Total training time: 1.33 seconds.\n",
            "-- Epoch 191\n",
            "Norm: 235.98, NNZs: 38, Bias: 177.088455, T: 8817897, Avg. loss: 128798.976040\n",
            "Total training time: 1.34 seconds.\n",
            "-- Epoch 192\n",
            "Norm: 232.66, NNZs: 38, Bias: 177.147332, T: 8864064, Avg. loss: 130480.592185\n",
            "Total training time: 1.35 seconds.\n",
            "-- Epoch 193\n",
            "Norm: 234.32, NNZs: 38, Bias: 177.164018, T: 8910231, Avg. loss: 128410.252098\n",
            "Total training time: 1.35 seconds.\n",
            "-- Epoch 194\n",
            "Norm: 232.97, NNZs: 38, Bias: 177.198811, T: 8956398, Avg. loss: 128528.457653\n",
            "Total training time: 1.36 seconds.\n",
            "-- Epoch 195\n",
            "Norm: 235.86, NNZs: 38, Bias: 177.206611, T: 9002565, Avg. loss: 126639.483843\n",
            "Total training time: 1.37 seconds.\n",
            "-- Epoch 196\n",
            "Norm: 236.80, NNZs: 38, Bias: 177.266372, T: 9048732, Avg. loss: 126147.480691\n",
            "Total training time: 1.38 seconds.\n",
            "-- Epoch 197\n",
            "Norm: 235.14, NNZs: 38, Bias: 177.249895, T: 9094899, Avg. loss: 124250.481840\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 198\n",
            "Norm: 232.56, NNZs: 38, Bias: 177.279637, T: 9141066, Avg. loss: 127161.070128\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 199\n",
            "Norm: 234.28, NNZs: 38, Bias: 177.245872, T: 9187233, Avg. loss: 122660.820858\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 200\n",
            "Norm: 233.22, NNZs: 38, Bias: 177.316447, T: 9233400, Avg. loss: 123461.642213\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 201\n",
            "Norm: 232.73, NNZs: 38, Bias: 177.331589, T: 9279567, Avg. loss: 124268.535704\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 202\n",
            "Norm: 239.35, NNZs: 38, Bias: 177.375698, T: 9325734, Avg. loss: 122780.588387\n",
            "Total training time: 1.42 seconds.\n",
            "-- Epoch 203\n",
            "Norm: 235.88, NNZs: 38, Bias: 177.301923, T: 9371901, Avg. loss: 121685.674766\n",
            "Total training time: 1.43 seconds.\n",
            "-- Epoch 204\n",
            "Norm: 232.46, NNZs: 38, Bias: 177.328570, T: 9418068, Avg. loss: 120558.622672\n",
            "Total training time: 1.44 seconds.\n",
            "-- Epoch 205\n",
            "Norm: 235.28, NNZs: 38, Bias: 177.375156, T: 9464235, Avg. loss: 121933.890866\n",
            "Total training time: 1.44 seconds.\n",
            "-- Epoch 206\n",
            "Norm: 236.12, NNZs: 38, Bias: 177.427874, T: 9510402, Avg. loss: 120849.211125\n",
            "Total training time: 1.45 seconds.\n",
            "-- Epoch 207\n",
            "Norm: 234.83, NNZs: 38, Bias: 177.495084, T: 9556569, Avg. loss: 119800.873689\n",
            "Total training time: 1.46 seconds.\n",
            "-- Epoch 208\n",
            "Norm: 232.52, NNZs: 38, Bias: 177.477360, T: 9602736, Avg. loss: 118546.901528\n",
            "Total training time: 1.47 seconds.\n",
            "-- Epoch 209\n",
            "Norm: 232.68, NNZs: 38, Bias: 177.502203, T: 9648903, Avg. loss: 119712.467602\n",
            "Total training time: 1.47 seconds.\n",
            "-- Epoch 210\n",
            "Norm: 232.56, NNZs: 38, Bias: 177.509499, T: 9695070, Avg. loss: 117332.739363\n",
            "Total training time: 1.48 seconds.\n",
            "-- Epoch 211\n",
            "Norm: 236.87, NNZs: 38, Bias: 177.505376, T: 9741237, Avg. loss: 117810.626067\n",
            "Total training time: 1.48 seconds.\n",
            "-- Epoch 212\n",
            "Norm: 232.59, NNZs: 38, Bias: 177.544344, T: 9787404, Avg. loss: 117532.141635\n",
            "Total training time: 1.49 seconds.\n",
            "-- Epoch 213\n",
            "Norm: 232.66, NNZs: 38, Bias: 177.489445, T: 9833571, Avg. loss: 116460.349774\n",
            "Total training time: 1.50 seconds.\n",
            "-- Epoch 214\n",
            "Norm: 232.53, NNZs: 38, Bias: 177.523917, T: 9879738, Avg. loss: 115163.885946\n",
            "Total training time: 1.50 seconds.\n",
            "-- Epoch 215\n",
            "Norm: 232.55, NNZs: 38, Bias: 177.612690, T: 9925905, Avg. loss: 116173.849596\n",
            "Total training time: 1.51 seconds.\n",
            "-- Epoch 216\n",
            "Norm: 235.59, NNZs: 38, Bias: 177.580563, T: 9972072, Avg. loss: 113771.733227\n",
            "Total training time: 1.52 seconds.\n",
            "-- Epoch 217\n",
            "Norm: 232.65, NNZs: 38, Bias: 177.574662, T: 10018239, Avg. loss: 113447.539602\n",
            "Total training time: 1.53 seconds.\n",
            "-- Epoch 218\n",
            "Norm: 233.10, NNZs: 38, Bias: 177.589643, T: 10064406, Avg. loss: 114298.329450\n",
            "Total training time: 1.53 seconds.\n",
            "-- Epoch 219\n",
            "Norm: 232.76, NNZs: 38, Bias: 177.609455, T: 10110573, Avg. loss: 114089.654464\n",
            "Total training time: 1.54 seconds.\n",
            "-- Epoch 220\n",
            "Norm: 232.58, NNZs: 38, Bias: 177.604417, T: 10156740, Avg. loss: 112219.985926\n",
            "Total training time: 1.55 seconds.\n",
            "-- Epoch 221\n",
            "Norm: 232.69, NNZs: 38, Bias: 177.631910, T: 10202907, Avg. loss: 110129.990655\n",
            "Total training time: 1.55 seconds.\n",
            "-- Epoch 222\n",
            "Norm: 233.80, NNZs: 38, Bias: 177.599646, T: 10249074, Avg. loss: 112248.938970\n",
            "Total training time: 1.56 seconds.\n",
            "-- Epoch 223\n",
            "Norm: 237.97, NNZs: 38, Bias: 177.641487, T: 10295241, Avg. loss: 110378.442281\n",
            "Total training time: 1.57 seconds.\n",
            "-- Epoch 224\n",
            "Norm: 232.47, NNZs: 38, Bias: 177.667806, T: 10341408, Avg. loss: 110266.406301\n",
            "Total training time: 1.58 seconds.\n",
            "-- Epoch 225\n",
            "Norm: 232.64, NNZs: 38, Bias: 177.774940, T: 10387575, Avg. loss: 110066.272289\n",
            "Total training time: 1.59 seconds.\n",
            "-- Epoch 226\n",
            "Norm: 232.71, NNZs: 38, Bias: 177.850895, T: 10433742, Avg. loss: 109431.685836\n",
            "Total training time: 1.59 seconds.\n",
            "-- Epoch 227\n",
            "Norm: 232.77, NNZs: 38, Bias: 177.887238, T: 10479909, Avg. loss: 109327.181019\n",
            "Total training time: 1.60 seconds.\n",
            "-- Epoch 228\n",
            "Norm: 233.04, NNZs: 38, Bias: 177.864426, T: 10526076, Avg. loss: 107871.887679\n",
            "Total training time: 1.61 seconds.\n",
            "-- Epoch 229\n",
            "Norm: 236.67, NNZs: 38, Bias: 177.880436, T: 10572243, Avg. loss: 106775.838052\n",
            "Total training time: 1.61 seconds.\n",
            "-- Epoch 230\n",
            "Norm: 232.38, NNZs: 38, Bias: 177.853080, T: 10618410, Avg. loss: 107070.187578\n",
            "Total training time: 1.62 seconds.\n",
            "-- Epoch 231\n",
            "Norm: 232.50, NNZs: 38, Bias: 177.906580, T: 10664577, Avg. loss: 106288.460515\n",
            "Total training time: 1.63 seconds.\n",
            "-- Epoch 232\n",
            "Norm: 233.38, NNZs: 38, Bias: 177.890694, T: 10710744, Avg. loss: 106876.858554\n",
            "Total training time: 1.63 seconds.\n",
            "-- Epoch 233\n",
            "Norm: 233.72, NNZs: 38, Bias: 177.897079, T: 10756911, Avg. loss: 105842.192205\n",
            "Total training time: 1.64 seconds.\n",
            "-- Epoch 234\n",
            "Norm: 232.35, NNZs: 38, Bias: 177.903614, T: 10803078, Avg. loss: 106106.719051\n",
            "Total training time: 1.65 seconds.\n",
            "-- Epoch 235\n",
            "Norm: 232.46, NNZs: 38, Bias: 177.874990, T: 10849245, Avg. loss: 104730.042952\n",
            "Total training time: 1.66 seconds.\n",
            "-- Epoch 236\n",
            "Norm: 232.95, NNZs: 38, Bias: 177.954081, T: 10895412, Avg. loss: 105672.232435\n",
            "Total training time: 1.66 seconds.\n",
            "-- Epoch 237\n",
            "Norm: 233.44, NNZs: 38, Bias: 177.917401, T: 10941579, Avg. loss: 104950.224431\n",
            "Total training time: 1.67 seconds.\n",
            "-- Epoch 238\n",
            "Norm: 232.83, NNZs: 38, Bias: 177.920182, T: 10987746, Avg. loss: 104183.859690\n",
            "Total training time: 1.68 seconds.\n",
            "-- Epoch 239\n",
            "Norm: 235.33, NNZs: 38, Bias: 177.910218, T: 11033913, Avg. loss: 103471.534428\n",
            "Total training time: 1.68 seconds.\n",
            "-- Epoch 240\n",
            "Norm: 237.58, NNZs: 38, Bias: 177.880351, T: 11080080, Avg. loss: 102037.934276\n",
            "Total training time: 1.69 seconds.\n",
            "-- Epoch 241\n",
            "Norm: 233.79, NNZs: 38, Bias: 177.871385, T: 11126247, Avg. loss: 102100.993103\n",
            "Total training time: 1.70 seconds.\n",
            "-- Epoch 242\n",
            "Norm: 232.28, NNZs: 38, Bias: 177.893759, T: 11172414, Avg. loss: 102617.849056\n",
            "Total training time: 1.70 seconds.\n",
            "-- Epoch 243\n",
            "Norm: 236.47, NNZs: 38, Bias: 177.855414, T: 11218581, Avg. loss: 101018.245667\n",
            "Total training time: 1.71 seconds.\n",
            "-- Epoch 244\n",
            "Norm: 232.28, NNZs: 38, Bias: 177.880339, T: 11264748, Avg. loss: 101551.527693\n",
            "Total training time: 1.71 seconds.\n",
            "-- Epoch 245\n",
            "Norm: 233.59, NNZs: 38, Bias: 177.913157, T: 11310915, Avg. loss: 100182.437843\n",
            "Total training time: 1.72 seconds.\n",
            "-- Epoch 246\n",
            "Norm: 235.24, NNZs: 38, Bias: 177.934503, T: 11357082, Avg. loss: 100484.185261\n",
            "Total training time: 1.73 seconds.\n",
            "-- Epoch 247\n",
            "Norm: 232.17, NNZs: 38, Bias: 177.978475, T: 11403249, Avg. loss: 100141.934703\n",
            "Total training time: 1.73 seconds.\n",
            "-- Epoch 248\n",
            "Norm: 233.83, NNZs: 38, Bias: 177.963598, T: 11449416, Avg. loss: 99555.258147\n",
            "Total training time: 1.74 seconds.\n",
            "-- Epoch 249\n",
            "Norm: 232.22, NNZs: 38, Bias: 177.929655, T: 11495583, Avg. loss: 99947.544880\n",
            "Total training time: 1.75 seconds.\n",
            "-- Epoch 250\n",
            "Norm: 233.28, NNZs: 38, Bias: 177.958250, T: 11541750, Avg. loss: 99230.777900\n",
            "Total training time: 1.75 seconds.\n",
            "-- Epoch 251\n",
            "Norm: 234.22, NNZs: 38, Bias: 177.914175, T: 11587917, Avg. loss: 98240.398971\n",
            "Total training time: 1.76 seconds.\n",
            "-- Epoch 252\n",
            "Norm: 232.12, NNZs: 38, Bias: 177.938287, T: 11634084, Avg. loss: 98207.398731\n",
            "Total training time: 1.76 seconds.\n",
            "-- Epoch 253\n",
            "Norm: 232.26, NNZs: 38, Bias: 177.942684, T: 11680251, Avg. loss: 97412.782233\n",
            "Total training time: 1.77 seconds.\n",
            "-- Epoch 254\n",
            "Norm: 235.05, NNZs: 38, Bias: 177.929835, T: 11726418, Avg. loss: 96821.427260\n",
            "Total training time: 1.78 seconds.\n",
            "-- Epoch 255\n",
            "Norm: 232.09, NNZs: 38, Bias: 177.980009, T: 11772585, Avg. loss: 96946.995640\n",
            "Total training time: 1.79 seconds.\n",
            "-- Epoch 256\n",
            "Norm: 232.37, NNZs: 38, Bias: 178.010486, T: 11818752, Avg. loss: 95681.446487\n",
            "Total training time: 1.80 seconds.\n",
            "-- Epoch 257\n",
            "Norm: 233.02, NNZs: 38, Bias: 177.997868, T: 11864919, Avg. loss: 96265.548486\n",
            "Total training time: 1.80 seconds.\n",
            "-- Epoch 258\n",
            "Norm: 232.99, NNZs: 38, Bias: 178.013880, T: 11911086, Avg. loss: 95636.304941\n",
            "Total training time: 1.81 seconds.\n",
            "-- Epoch 259\n",
            "Norm: 235.93, NNZs: 38, Bias: 178.014713, T: 11957253, Avg. loss: 95771.387615\n",
            "Total training time: 1.82 seconds.\n",
            "-- Epoch 260\n",
            "Norm: 232.28, NNZs: 38, Bias: 178.015549, T: 12003420, Avg. loss: 95011.990946\n",
            "Total training time: 1.82 seconds.\n",
            "-- Epoch 261\n",
            "Norm: 235.54, NNZs: 38, Bias: 178.071191, T: 12049587, Avg. loss: 95267.570684\n",
            "Total training time: 1.83 seconds.\n",
            "-- Epoch 262\n",
            "Norm: 237.15, NNZs: 38, Bias: 178.039748, T: 12095754, Avg. loss: 94101.548528\n",
            "Total training time: 1.84 seconds.\n",
            "-- Epoch 263\n",
            "Norm: 232.09, NNZs: 38, Bias: 178.024859, T: 12141921, Avg. loss: 93327.984229\n",
            "Total training time: 1.84 seconds.\n",
            "-- Epoch 264\n",
            "Norm: 233.62, NNZs: 38, Bias: 178.034728, T: 12188088, Avg. loss: 94161.781980\n",
            "Total training time: 1.85 seconds.\n",
            "-- Epoch 265\n",
            "Norm: 233.14, NNZs: 38, Bias: 178.063278, T: 12234255, Avg. loss: 93144.491944\n",
            "Total training time: 1.86 seconds.\n",
            "-- Epoch 266\n",
            "Norm: 233.00, NNZs: 38, Bias: 178.091745, T: 12280422, Avg. loss: 92395.899930\n",
            "Total training time: 1.86 seconds.\n",
            "-- Epoch 267\n",
            "Norm: 232.54, NNZs: 38, Bias: 178.141293, T: 12326589, Avg. loss: 93736.254278\n",
            "Total training time: 1.87 seconds.\n",
            "-- Epoch 268\n",
            "Norm: 234.00, NNZs: 38, Bias: 178.179353, T: 12372756, Avg. loss: 92989.430872\n",
            "Total training time: 1.88 seconds.\n",
            "-- Epoch 269\n",
            "Norm: 233.06, NNZs: 38, Bias: 178.220504, T: 12418923, Avg. loss: 92069.221439\n",
            "Total training time: 1.88 seconds.\n",
            "-- Epoch 270\n",
            "Norm: 235.12, NNZs: 38, Bias: 178.166646, T: 12465090, Avg. loss: 92318.556452\n",
            "Total training time: 1.89 seconds.\n",
            "-- Epoch 271\n",
            "Norm: 236.75, NNZs: 38, Bias: 178.148259, T: 12511257, Avg. loss: 90923.149452\n",
            "Total training time: 1.90 seconds.\n",
            "-- Epoch 272\n",
            "Norm: 232.96, NNZs: 38, Bias: 178.176222, T: 12557424, Avg. loss: 90808.746551\n",
            "Total training time: 1.91 seconds.\n",
            "-- Epoch 273\n",
            "Norm: 232.56, NNZs: 38, Bias: 178.227896, T: 12603591, Avg. loss: 91505.770019\n",
            "Total training time: 1.91 seconds.\n",
            "-- Epoch 274\n",
            "Norm: 234.27, NNZs: 38, Bias: 178.200994, T: 12649758, Avg. loss: 89185.470477\n",
            "Total training time: 1.92 seconds.\n",
            "-- Epoch 275\n",
            "Norm: 232.68, NNZs: 38, Bias: 178.234877, T: 12695925, Avg. loss: 89918.044158\n",
            "Total training time: 1.93 seconds.\n",
            "-- Epoch 276\n",
            "Norm: 232.53, NNZs: 38, Bias: 178.245845, T: 12742092, Avg. loss: 90143.993186\n",
            "Total training time: 1.94 seconds.\n",
            "-- Epoch 277\n",
            "Norm: 234.14, NNZs: 38, Bias: 178.227779, T: 12788259, Avg. loss: 89091.968224\n",
            "Total training time: 1.94 seconds.\n",
            "-- Epoch 278\n",
            "Norm: 233.13, NNZs: 38, Bias: 178.220028, T: 12834426, Avg. loss: 89559.947106\n",
            "Total training time: 1.95 seconds.\n",
            "-- Epoch 279\n",
            "Norm: 233.52, NNZs: 38, Bias: 178.211433, T: 12880593, Avg. loss: 88515.770282\n",
            "Total training time: 1.96 seconds.\n",
            "-- Epoch 280\n",
            "Norm: 232.59, NNZs: 38, Bias: 178.205237, T: 12926760, Avg. loss: 88850.766134\n",
            "Total training time: 1.96 seconds.\n",
            "-- Epoch 281\n",
            "Norm: 234.68, NNZs: 38, Bias: 178.223055, T: 12972927, Avg. loss: 88277.221795\n",
            "Total training time: 1.97 seconds.\n",
            "-- Epoch 282\n",
            "Norm: 232.55, NNZs: 38, Bias: 178.211513, T: 13019094, Avg. loss: 87670.477457\n",
            "Total training time: 1.98 seconds.\n",
            "-- Epoch 283\n",
            "Norm: 233.61, NNZs: 38, Bias: 178.169315, T: 13065261, Avg. loss: 87883.279675\n",
            "Total training time: 1.99 seconds.\n",
            "-- Epoch 284\n",
            "Norm: 232.61, NNZs: 38, Bias: 178.232693, T: 13111428, Avg. loss: 87356.316213\n",
            "Total training time: 2.00 seconds.\n",
            "-- Epoch 285\n",
            "Norm: 232.94, NNZs: 38, Bias: 178.269969, T: 13157595, Avg. loss: 87963.962132\n",
            "Total training time: 2.01 seconds.\n",
            "-- Epoch 286\n",
            "Norm: 232.27, NNZs: 38, Bias: 178.303361, T: 13203762, Avg. loss: 86382.967374\n",
            "Total training time: 2.01 seconds.\n",
            "-- Epoch 287\n",
            "Norm: 232.25, NNZs: 38, Bias: 178.317682, T: 13249929, Avg. loss: 86280.258552\n",
            "Total training time: 2.02 seconds.\n",
            "-- Epoch 288\n",
            "Norm: 233.17, NNZs: 38, Bias: 178.290595, T: 13296096, Avg. loss: 84747.484205\n",
            "Total training time: 2.03 seconds.\n",
            "-- Epoch 289\n",
            "Norm: 234.73, NNZs: 38, Bias: 178.322146, T: 13342263, Avg. loss: 85054.555042\n",
            "Total training time: 2.04 seconds.\n",
            "-- Epoch 290\n",
            "Norm: 233.69, NNZs: 38, Bias: 178.339328, T: 13388430, Avg. loss: 84812.656677\n",
            "Total training time: 2.04 seconds.\n",
            "-- Epoch 291\n",
            "Norm: 233.23, NNZs: 38, Bias: 178.353457, T: 13434597, Avg. loss: 84910.581682\n",
            "Total training time: 2.05 seconds.\n",
            "-- Epoch 292\n",
            "Norm: 233.13, NNZs: 38, Bias: 178.358675, T: 13480764, Avg. loss: 84685.014260\n",
            "Total training time: 2.06 seconds.\n",
            "-- Epoch 293\n",
            "Norm: 232.25, NNZs: 38, Bias: 178.339350, T: 13526931, Avg. loss: 84829.882225\n",
            "Total training time: 2.07 seconds.\n",
            "-- Epoch 294\n",
            "Norm: 233.85, NNZs: 38, Bias: 178.285530, T: 13573098, Avg. loss: 84222.978760\n",
            "Total training time: 2.07 seconds.\n",
            "-- Epoch 295\n",
            "Norm: 236.37, NNZs: 38, Bias: 178.273048, T: 13619265, Avg. loss: 83142.457106\n",
            "Total training time: 2.08 seconds.\n",
            "-- Epoch 296\n",
            "Norm: 233.02, NNZs: 38, Bias: 178.271591, T: 13665432, Avg. loss: 83103.601984\n",
            "Total training time: 2.09 seconds.\n",
            "-- Epoch 297\n",
            "Norm: 234.53, NNZs: 38, Bias: 178.262812, T: 13711599, Avg. loss: 83442.556813\n",
            "Total training time: 2.09 seconds.\n",
            "-- Epoch 298\n",
            "Norm: 232.61, NNZs: 38, Bias: 178.272265, T: 13757766, Avg. loss: 82957.720214\n",
            "Total training time: 2.10 seconds.\n",
            "-- Epoch 299\n",
            "Norm: 232.25, NNZs: 38, Bias: 178.239634, T: 13803933, Avg. loss: 82670.256097\n",
            "Total training time: 2.11 seconds.\n",
            "-- Epoch 300\n",
            "Norm: 235.02, NNZs: 38, Bias: 178.207811, T: 13850100, Avg. loss: 83374.306569\n",
            "Total training time: 2.12 seconds.\n",
            "-- Epoch 301\n",
            "Norm: 234.71, NNZs: 38, Bias: 178.207093, T: 13896267, Avg. loss: 82376.226787\n",
            "Total training time: 2.12 seconds.\n",
            "-- Epoch 302\n",
            "Norm: 234.83, NNZs: 38, Bias: 178.206385, T: 13942434, Avg. loss: 82415.482975\n",
            "Total training time: 2.13 seconds.\n",
            "-- Epoch 303\n",
            "Norm: 235.33, NNZs: 38, Bias: 178.171327, T: 13988601, Avg. loss: 80971.974384\n",
            "Total training time: 2.14 seconds.\n",
            "-- Epoch 304\n",
            "Norm: 232.52, NNZs: 38, Bias: 178.201281, T: 14034768, Avg. loss: 81559.606354\n",
            "Total training time: 2.14 seconds.\n",
            "-- Epoch 305\n",
            "Norm: 233.51, NNZs: 38, Bias: 178.220486, T: 14080935, Avg. loss: 79924.506206\n",
            "Total training time: 2.15 seconds.\n",
            "-- Epoch 306\n",
            "Norm: 233.48, NNZs: 38, Bias: 178.258779, T: 14127102, Avg. loss: 79778.753238\n",
            "Total training time: 2.16 seconds.\n",
            "-- Epoch 307\n",
            "Norm: 233.87, NNZs: 38, Bias: 178.231217, T: 14173269, Avg. loss: 80640.371984\n",
            "Total training time: 2.17 seconds.\n",
            "-- Epoch 308\n",
            "Norm: 232.00, NNZs: 38, Bias: 178.250917, T: 14219436, Avg. loss: 80843.029880\n",
            "Total training time: 2.18 seconds.\n",
            "-- Epoch 309\n",
            "Norm: 233.02, NNZs: 38, Bias: 178.304273, T: 14265603, Avg. loss: 81334.565588\n",
            "Total training time: 2.19 seconds.\n",
            "-- Epoch 310\n",
            "Norm: 231.93, NNZs: 38, Bias: 178.271378, T: 14311770, Avg. loss: 79509.846569\n",
            "Total training time: 2.19 seconds.\n",
            "-- Epoch 311\n",
            "Norm: 232.03, NNZs: 38, Bias: 178.305527, T: 14357937, Avg. loss: 79259.497817\n",
            "Total training time: 2.21 seconds.\n",
            "-- Epoch 312\n",
            "Norm: 233.24, NNZs: 38, Bias: 178.302009, T: 14404104, Avg. loss: 78371.254618\n",
            "Total training time: 2.21 seconds.\n",
            "-- Epoch 313\n",
            "Norm: 232.07, NNZs: 38, Bias: 178.317897, T: 14450271, Avg. loss: 79134.029664\n",
            "Total training time: 2.22 seconds.\n",
            "-- Epoch 314\n",
            "Norm: 232.08, NNZs: 38, Bias: 178.310238, T: 14496438, Avg. loss: 78452.624702\n",
            "Total training time: 2.23 seconds.\n",
            "-- Epoch 315\n",
            "Norm: 231.82, NNZs: 38, Bias: 178.369483, T: 14542605, Avg. loss: 79210.267403\n",
            "Total training time: 2.23 seconds.\n",
            "-- Epoch 316\n",
            "Norm: 231.78, NNZs: 38, Bias: 178.416137, T: 14588772, Avg. loss: 78369.385760\n",
            "Total training time: 2.24 seconds.\n",
            "-- Epoch 317\n",
            "Norm: 231.73, NNZs: 38, Bias: 178.427768, T: 14634939, Avg. loss: 77674.523096\n",
            "Total training time: 2.25 seconds.\n",
            "-- Epoch 318\n",
            "Norm: 231.74, NNZs: 38, Bias: 178.428496, T: 14681106, Avg. loss: 77145.180605\n",
            "Total training time: 2.26 seconds.\n",
            "-- Epoch 319\n",
            "Norm: 231.68, NNZs: 38, Bias: 178.459075, T: 14727273, Avg. loss: 77531.492373\n",
            "Total training time: 2.26 seconds.\n",
            "-- Epoch 320\n",
            "Norm: 231.72, NNZs: 38, Bias: 178.453008, T: 14773440, Avg. loss: 77413.203527\n",
            "Total training time: 2.27 seconds.\n",
            "-- Epoch 321\n",
            "Norm: 232.07, NNZs: 38, Bias: 178.470569, T: 14819607, Avg. loss: 76789.872883\n",
            "Total training time: 2.28 seconds.\n",
            "-- Epoch 322\n",
            "Norm: 231.79, NNZs: 38, Bias: 178.465828, T: 14865774, Avg. loss: 76809.321973\n",
            "Total training time: 2.29 seconds.\n",
            "-- Epoch 323\n",
            "Norm: 233.30, NNZs: 38, Bias: 178.490006, T: 14911941, Avg. loss: 76739.140078\n",
            "Total training time: 2.29 seconds.\n",
            "-- Epoch 324\n",
            "Norm: 232.44, NNZs: 38, Bias: 178.519479, T: 14958108, Avg. loss: 75978.971866\n",
            "Total training time: 2.30 seconds.\n",
            "-- Epoch 325\n",
            "Norm: 232.30, NNZs: 38, Bias: 178.467435, T: 15004275, Avg. loss: 75366.920438\n",
            "Total training time: 2.31 seconds.\n",
            "-- Epoch 326\n",
            "Norm: 234.16, NNZs: 38, Bias: 178.474162, T: 15050442, Avg. loss: 76162.262490\n",
            "Total training time: 2.31 seconds.\n",
            "-- Epoch 327\n",
            "Norm: 232.92, NNZs: 38, Bias: 178.476194, T: 15096609, Avg. loss: 75722.072457\n",
            "Total training time: 2.32 seconds.\n",
            "-- Epoch 328\n",
            "Norm: 232.51, NNZs: 38, Bias: 178.455095, T: 15142776, Avg. loss: 75592.931671\n",
            "Total training time: 2.33 seconds.\n",
            "-- Epoch 329\n",
            "Norm: 231.98, NNZs: 38, Bias: 178.408316, T: 15188943, Avg. loss: 74836.839832\n",
            "Total training time: 2.34 seconds.\n",
            "-- Epoch 330\n",
            "Norm: 232.88, NNZs: 38, Bias: 178.412284, T: 15235110, Avg. loss: 75027.429041\n",
            "Total training time: 2.35 seconds.\n",
            "-- Epoch 331\n",
            "Norm: 232.64, NNZs: 38, Bias: 178.429321, T: 15281277, Avg. loss: 74941.500467\n",
            "Total training time: 2.36 seconds.\n",
            "-- Epoch 332\n",
            "Norm: 231.85, NNZs: 38, Bias: 178.439111, T: 15327444, Avg. loss: 74292.069224\n",
            "Total training time: 2.36 seconds.\n",
            "-- Epoch 333\n",
            "Norm: 231.79, NNZs: 38, Bias: 178.462584, T: 15373611, Avg. loss: 73893.558236\n",
            "Total training time: 2.37 seconds.\n",
            "-- Epoch 334\n",
            "Norm: 231.80, NNZs: 38, Bias: 178.471034, T: 15419778, Avg. loss: 73677.675895\n",
            "Total training time: 2.38 seconds.\n",
            "-- Epoch 335\n",
            "Norm: 231.91, NNZs: 38, Bias: 178.460678, T: 15465945, Avg. loss: 73914.630211\n",
            "Total training time: 2.39 seconds.\n",
            "-- Epoch 336\n",
            "Norm: 233.07, NNZs: 38, Bias: 178.473611, T: 15512112, Avg. loss: 73975.378863\n",
            "Total training time: 2.39 seconds.\n",
            "-- Epoch 337\n",
            "Norm: 231.92, NNZs: 38, Bias: 178.487125, T: 15558279, Avg. loss: 73334.197556\n",
            "Total training time: 2.41 seconds.\n",
            "-- Epoch 338\n",
            "Norm: 232.04, NNZs: 38, Bias: 178.481348, T: 15604446, Avg. loss: 73686.686404\n",
            "Total training time: 2.41 seconds.\n",
            "-- Epoch 339\n",
            "Norm: 232.08, NNZs: 38, Bias: 178.505054, T: 15650613, Avg. loss: 73038.167217\n",
            "Total training time: 2.42 seconds.\n",
            "-- Epoch 340\n",
            "Norm: 231.88, NNZs: 38, Bias: 178.526751, T: 15696780, Avg. loss: 72751.384607\n",
            "Total training time: 2.43 seconds.\n",
            "-- Epoch 341\n",
            "Norm: 232.64, NNZs: 38, Bias: 178.585951, T: 15742947, Avg. loss: 72992.590981\n",
            "Total training time: 2.44 seconds.\n",
            "-- Epoch 342\n",
            "Norm: 231.92, NNZs: 38, Bias: 178.592898, T: 15789114, Avg. loss: 71573.745331\n",
            "Total training time: 2.45 seconds.\n",
            "-- Epoch 343\n",
            "Norm: 235.16, NNZs: 38, Bias: 178.615666, T: 15835281, Avg. loss: 72418.326520\n",
            "Total training time: 2.45 seconds.\n",
            "-- Epoch 344\n",
            "Norm: 231.92, NNZs: 38, Bias: 178.606207, T: 15881448, Avg. loss: 72185.746154\n",
            "Total training time: 2.46 seconds.\n",
            "-- Epoch 345\n",
            "Norm: 232.75, NNZs: 38, Bias: 178.591753, T: 15927615, Avg. loss: 71524.734116\n",
            "Total training time: 2.47 seconds.\n",
            "-- Epoch 346\n",
            "Norm: 232.13, NNZs: 38, Bias: 178.567326, T: 15973782, Avg. loss: 71286.371154\n",
            "Total training time: 2.48 seconds.\n",
            "-- Epoch 347\n",
            "Norm: 231.95, NNZs: 38, Bias: 178.589812, T: 16019949, Avg. loss: 70686.181280\n",
            "Total training time: 2.48 seconds.\n",
            "-- Epoch 348\n",
            "Norm: 231.82, NNZs: 38, Bias: 178.593546, T: 16066116, Avg. loss: 70678.323749\n",
            "Total training time: 2.49 seconds.\n",
            "-- Epoch 349\n",
            "Norm: 231.85, NNZs: 38, Bias: 178.552532, T: 16112283, Avg. loss: 71472.373372\n",
            "Total training time: 2.50 seconds.\n",
            "-- Epoch 350\n",
            "Norm: 232.64, NNZs: 38, Bias: 178.551870, T: 16158450, Avg. loss: 70118.953352\n",
            "Total training time: 2.50 seconds.\n",
            "-- Epoch 351\n",
            "Norm: 231.91, NNZs: 38, Bias: 178.553087, T: 16204617, Avg. loss: 70873.811289\n",
            "Total training time: 2.51 seconds.\n",
            "-- Epoch 352\n",
            "Norm: 232.15, NNZs: 38, Bias: 178.535850, T: 16250784, Avg. loss: 69811.151736\n",
            "Total training time: 2.52 seconds.\n",
            "-- Epoch 353\n",
            "Norm: 232.26, NNZs: 38, Bias: 178.530925, T: 16296951, Avg. loss: 70342.223063\n",
            "Total training time: 2.53 seconds.\n",
            "-- Epoch 354\n",
            "Norm: 231.87, NNZs: 38, Bias: 178.550521, T: 16343118, Avg. loss: 70415.587028\n",
            "Total training time: 2.53 seconds.\n",
            "-- Epoch 355\n",
            "Norm: 232.56, NNZs: 38, Bias: 178.540786, T: 16389285, Avg. loss: 70046.329935\n",
            "Total training time: 2.54 seconds.\n",
            "-- Epoch 356\n",
            "Norm: 231.84, NNZs: 38, Bias: 178.542627, T: 16435452, Avg. loss: 69069.929209\n",
            "Total training time: 2.55 seconds.\n",
            "-- Epoch 357\n",
            "Norm: 232.91, NNZs: 38, Bias: 178.552360, T: 16481619, Avg. loss: 68646.224143\n",
            "Total training time: 2.55 seconds.\n",
            "-- Epoch 358\n",
            "Norm: 232.01, NNZs: 38, Bias: 178.573567, T: 16527786, Avg. loss: 70118.177978\n",
            "Total training time: 2.56 seconds.\n",
            "-- Epoch 359\n",
            "Norm: 231.78, NNZs: 38, Bias: 178.566917, T: 16573953, Avg. loss: 69526.198002\n",
            "Total training time: 2.57 seconds.\n",
            "-- Epoch 360\n",
            "Norm: 231.95, NNZs: 38, Bias: 178.559690, T: 16620120, Avg. loss: 67945.939080\n",
            "Total training time: 2.57 seconds.\n",
            "-- Epoch 361\n",
            "Norm: 232.27, NNZs: 38, Bias: 178.573562, T: 16666287, Avg. loss: 68741.542744\n",
            "Total training time: 2.58 seconds.\n",
            "-- Epoch 362\n",
            "Norm: 232.27, NNZs: 38, Bias: 178.620932, T: 16712454, Avg. loss: 67988.153030\n",
            "Total training time: 2.59 seconds.\n",
            "-- Epoch 363\n",
            "Norm: 231.79, NNZs: 38, Bias: 178.662741, T: 16758621, Avg. loss: 68544.316402\n",
            "Total training time: 2.59 seconds.\n",
            "-- Epoch 364\n",
            "Norm: 233.14, NNZs: 38, Bias: 178.671083, T: 16804788, Avg. loss: 67943.669986\n",
            "Total training time: 2.60 seconds.\n",
            "-- Epoch 365\n",
            "Norm: 232.05, NNZs: 38, Bias: 178.690050, T: 16850955, Avg. loss: 67819.630459\n",
            "Total training time: 2.61 seconds.\n",
            "-- Epoch 366\n",
            "Norm: 232.25, NNZs: 38, Bias: 178.679985, T: 16897122, Avg. loss: 67179.385731\n",
            "Total training time: 2.62 seconds.\n",
            "-- Epoch 367\n",
            "Norm: 231.93, NNZs: 38, Bias: 178.685325, T: 16943289, Avg. loss: 67494.796942\n",
            "Total training time: 2.63 seconds.\n",
            "-- Epoch 368\n",
            "Norm: 231.98, NNZs: 38, Bias: 178.694746, T: 16989456, Avg. loss: 66757.270433\n",
            "Total training time: 2.64 seconds.\n",
            "-- Epoch 369\n",
            "Norm: 232.65, NNZs: 38, Bias: 178.695296, T: 17035623, Avg. loss: 66713.257975\n",
            "Total training time: 2.64 seconds.\n",
            "-- Epoch 370\n",
            "Norm: 232.28, NNZs: 38, Bias: 178.715806, T: 17081790, Avg. loss: 67098.700701\n",
            "Total training time: 2.65 seconds.\n",
            "-- Epoch 371\n",
            "Norm: 231.86, NNZs: 38, Bias: 178.713416, T: 17127957, Avg. loss: 67011.685065\n",
            "Total training time: 2.66 seconds.\n",
            "-- Epoch 372\n",
            "Norm: 231.84, NNZs: 38, Bias: 178.723329, T: 17174124, Avg. loss: 65887.226940\n",
            "Total training time: 2.67 seconds.\n",
            "-- Epoch 373\n",
            "Norm: 232.60, NNZs: 38, Bias: 178.743698, T: 17220291, Avg. loss: 66393.074655\n",
            "Total training time: 2.68 seconds.\n",
            "-- Epoch 374\n",
            "Norm: 232.17, NNZs: 38, Bias: 178.753511, T: 17266458, Avg. loss: 66153.588443\n",
            "Total training time: 2.68 seconds.\n",
            "-- Epoch 375\n",
            "Norm: 231.87, NNZs: 38, Bias: 178.745397, T: 17312625, Avg. loss: 65598.896334\n",
            "Total training time: 2.69 seconds.\n",
            "-- Epoch 376\n",
            "Norm: 231.84, NNZs: 38, Bias: 178.741931, T: 17358792, Avg. loss: 65280.448006\n",
            "Total training time: 2.70 seconds.\n",
            "-- Epoch 377\n",
            "Norm: 232.13, NNZs: 38, Bias: 178.726385, T: 17404959, Avg. loss: 64688.111124\n",
            "Total training time: 2.70 seconds.\n",
            "-- Epoch 378\n",
            "Norm: 231.89, NNZs: 38, Bias: 178.717773, T: 17451126, Avg. loss: 65571.425564\n",
            "Total training time: 2.71 seconds.\n",
            "-- Epoch 379\n",
            "Norm: 232.92, NNZs: 38, Bias: 178.701132, T: 17497293, Avg. loss: 65069.391599\n",
            "Total training time: 2.72 seconds.\n",
            "-- Epoch 380\n",
            "Norm: 231.98, NNZs: 38, Bias: 178.700532, T: 17543460, Avg. loss: 65148.104383\n",
            "Total training time: 2.73 seconds.\n",
            "-- Epoch 381\n",
            "Norm: 232.37, NNZs: 38, Bias: 178.746629, T: 17589627, Avg. loss: 65097.675922\n",
            "Total training time: 2.73 seconds.\n",
            "-- Epoch 382\n",
            "Norm: 231.75, NNZs: 38, Bias: 178.769345, T: 17635794, Avg. loss: 64899.719808\n",
            "Total training time: 2.74 seconds.\n",
            "Convergence after 382 epochs took 2.74 seconds\n",
            "-- Epoch 1\n",
            "Norm: 7532.33, NNZs: 3, Bias: -8.773103, T: 46167, Avg. loss: 113946.504276\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1744.94, NNZs: 3, Bias: -8.141412, T: 92334, Avg. loss: 46476.652470\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 855.41, NNZs: 7, Bias: -8.295602, T: 138501, Avg. loss: 26781.113832\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 2670.48, NNZs: 8, Bias: -8.234292, T: 184668, Avg. loss: 19454.507435\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 41.93, NNZs: 10, Bias: -8.101388, T: 230835, Avg. loss: 16497.936911\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 384.72, NNZs: 10, Bias: -8.105972, T: 277002, Avg. loss: 12531.198424\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1106.06, NNZs: 10, Bias: -8.174947, T: 323169, Avg. loss: 9737.191927\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1010.87, NNZs: 10, Bias: -8.118995, T: 369336, Avg. loss: 9325.845503\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 86.41, NNZs: 10, Bias: -8.143106, T: 415503, Avg. loss: 7310.615989\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 684.45, NNZs: 11, Bias: -8.142013, T: 461670, Avg. loss: 7435.684218\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 393.20, NNZs: 11, Bias: -8.083354, T: 507837, Avg. loss: 6729.959047\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 806.74, NNZs: 11, Bias: -8.065254, T: 554004, Avg. loss: 5878.117009\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 102.71, NNZs: 11, Bias: -8.031290, T: 600171, Avg. loss: 5380.760995\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 32.67, NNZs: 12, Bias: -8.047507, T: 646338, Avg. loss: 5014.415210\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 861.06, NNZs: 12, Bias: -8.046908, T: 692505, Avg. loss: 4709.601736\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 211.78, NNZs: 12, Bias: -8.019440, T: 738672, Avg. loss: 4467.622098\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 477.93, NNZs: 12, Bias: -8.019002, T: 784839, Avg. loss: 3761.262245\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 35.74, NNZs: 12, Bias: -8.006587, T: 831006, Avg. loss: 3674.049478\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 361.09, NNZs: 12, Bias: -8.030692, T: 877173, Avg. loss: 3289.953543\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 510.51, NNZs: 12, Bias: -8.064405, T: 923340, Avg. loss: 2989.004292\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 365.67, NNZs: 13, Bias: -8.053946, T: 969507, Avg. loss: 3350.590410\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 372.25, NNZs: 13, Bias: -8.043685, T: 1015674, Avg. loss: 2960.040386\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 312.20, NNZs: 13, Bias: -8.023816, T: 1061841, Avg. loss: 3099.252680\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 81.33, NNZs: 13, Bias: -8.042019, T: 1108008, Avg. loss: 2415.102813\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 167.53, NNZs: 13, Bias: -8.024403, T: 1154175, Avg. loss: 2824.705527\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 12.84, NNZs: 13, Bias: -8.024229, T: 1200342, Avg. loss: 2356.890973\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 17.13, NNZs: 13, Bias: -8.008279, T: 1246509, Avg. loss: 2574.970783\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 95.69, NNZs: 13, Bias: -8.031770, T: 1292676, Avg. loss: 2372.045872\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 22.33, NNZs: 14, Bias: -8.046895, T: 1338843, Avg. loss: 2334.132517\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 199.10, NNZs: 14, Bias: -8.046920, T: 1385010, Avg. loss: 2240.928868\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 339.90, NNZs: 14, Bias: -8.032777, T: 1431177, Avg. loss: 2273.855626\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 24.21, NNZs: 14, Bias: -8.025682, T: 1477344, Avg. loss: 2125.122719\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 11.70, NNZs: 14, Bias: -8.019314, T: 1523511, Avg. loss: 1983.097728\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 121.96, NNZs: 14, Bias: -8.032556, T: 1569678, Avg. loss: 1699.402335\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 101.37, NNZs: 14, Bias: -8.026404, T: 1615845, Avg. loss: 1913.749172\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 39.04, NNZs: 14, Bias: -8.032815, T: 1662012, Avg. loss: 1764.073680\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 70.11, NNZs: 14, Bias: -8.026958, T: 1708179, Avg. loss: 1757.819210\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 100.42, NNZs: 14, Bias: -8.009642, T: 1754346, Avg. loss: 1902.532696\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 101.47, NNZs: 14, Bias: -7.992844, T: 1800513, Avg. loss: 1855.230574\n",
            "Total training time: 0.25 seconds.\n",
            "Convergence after 39 epochs took 0.25 seconds\n",
            "-- Epoch 1\n",
            "Norm: 814.78, NNZs: 6, Bias: -10.626819, T: 46167, Avg. loss: 277881.623790\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 2123.15, NNZs: 7, Bias: -11.073781, T: 92334, Avg. loss: 38243.453236\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 2067.46, NNZs: 8, Bias: -11.276510, T: 138501, Avg. loss: 21744.572678\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 2519.51, NNZs: 8, Bias: -11.269117, T: 184668, Avg. loss: 15533.673516\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 453.99, NNZs: 10, Bias: -11.266605, T: 230835, Avg. loss: 10964.257397\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 953.23, NNZs: 10, Bias: -11.267227, T: 277002, Avg. loss: 9750.959097\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 275.57, NNZs: 10, Bias: -11.270084, T: 323169, Avg. loss: 7860.529824\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 487.77, NNZs: 10, Bias: -11.300356, T: 369336, Avg. loss: 6611.810129\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 225.79, NNZs: 10, Bias: -11.298560, T: 415503, Avg. loss: 5678.648705\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 171.87, NNZs: 10, Bias: -11.321521, T: 461670, Avg. loss: 5088.247465\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 28.34, NNZs: 10, Bias: -11.321636, T: 507837, Avg. loss: 4662.369912\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 1077.09, NNZs: 10, Bias: -11.359627, T: 554004, Avg. loss: 4251.137107\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 339.29, NNZs: 10, Bias: -11.324731, T: 600171, Avg. loss: 4704.327677\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 0.64, NNZs: 10, Bias: -11.324977, T: 646338, Avg. loss: 3684.494858\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 570.42, NNZs: 10, Bias: -11.325058, T: 692505, Avg. loss: 3819.114196\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 553.07, NNZs: 10, Bias: -11.310728, T: 738672, Avg. loss: 3407.151251\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 17.45, NNZs: 10, Bias: -11.297076, T: 784839, Avg. loss: 3219.595352\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 149.69, NNZs: 11, Bias: -11.322528, T: 831006, Avg. loss: 2870.514942\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 509.25, NNZs: 11, Bias: -11.334260, T: 877173, Avg. loss: 2833.527058\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 499.58, NNZs: 11, Bias: -11.334089, T: 923340, Avg. loss: 2722.660219\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 446.58, NNZs: 11, Bias: -11.313181, T: 969507, Avg. loss: 2850.861131\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 378.28, NNZs: 11, Bias: -11.283298, T: 1015674, Avg. loss: 2874.306374\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 101.06, NNZs: 11, Bias: -11.273549, T: 1061841, Avg. loss: 2341.342056\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 460.98, NNZs: 11, Bias: -11.291964, T: 1108008, Avg. loss: 2066.027588\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 162.66, NNZs: 11, Bias: -11.291612, T: 1154175, Avg. loss: 2040.794446\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 213.90, NNZs: 12, Bias: -11.300063, T: 1200342, Avg. loss: 2124.406147\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 86.77, NNZs: 12, Bias: -11.291705, T: 1246509, Avg. loss: 1859.693738\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 69.45, NNZs: 12, Bias: -11.299469, T: 1292676, Avg. loss: 1902.446393\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 79.73, NNZs: 12, Bias: -11.315141, T: 1338843, Avg. loss: 1734.882113\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 345.71, NNZs: 12, Bias: -11.315214, T: 1385010, Avg. loss: 1818.773011\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 168.78, NNZs: 12, Bias: -11.315119, T: 1431177, Avg. loss: 1693.381397\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 356.83, NNZs: 12, Bias: -11.315026, T: 1477344, Avg. loss: 1700.511171\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 369.89, NNZs: 12, Bias: -11.308439, T: 1523511, Avg. loss: 1671.509347\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 350.97, NNZs: 12, Bias: -11.327639, T: 1569678, Avg. loss: 1448.314141\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 339.43, NNZs: 12, Bias: -11.339876, T: 1615845, Avg. loss: 1465.616754\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 107.95, NNZs: 12, Bias: -11.327730, T: 1662012, Avg. loss: 1627.848380\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 318.21, NNZs: 12, Bias: -11.339509, T: 1708179, Avg. loss: 1389.125313\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 323.94, NNZs: 12, Bias: -11.345179, T: 1754346, Avg. loss: 1434.175835\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 66.88, NNZs: 12, Bias: -11.334036, T: 1800513, Avg. loss: 1517.114465\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 250.66, NNZs: 12, Bias: -11.339693, T: 1846680, Avg. loss: 1362.854893\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 36.25, NNZs: 12, Bias: -11.339675, T: 1892847, Avg. loss: 1254.101643\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 265.18, NNZs: 12, Bias: -11.345038, T: 1939014, Avg. loss: 1244.792351\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 36.46, NNZs: 13, Bias: -11.365453, T: 1985181, Avg. loss: 1099.888675\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 47.94, NNZs: 13, Bias: -11.370464, T: 2031348, Avg. loss: 1200.698253\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 70.34, NNZs: 13, Bias: -11.399474, T: 2077515, Avg. loss: 1006.780296\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 31.97, NNZs: 13, Bias: -11.399443, T: 2123682, Avg. loss: 1130.495656\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 236.86, NNZs: 13, Bias: -11.418128, T: 2169849, Avg. loss: 1060.979171\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 128.30, NNZs: 13, Bias: -11.418027, T: 2216016, Avg. loss: 1143.536167\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 92.67, NNZs: 14, Bias: -11.413576, T: 2262183, Avg. loss: 1158.529645\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 121.69, NNZs: 14, Bias: -11.409266, T: 2308350, Avg. loss: 1134.866701\n",
            "Total training time: 0.33 seconds.\n",
            "Convergence after 50 epochs took 0.33 seconds\n",
            "-- Epoch 1\n",
            "Norm: 5073.15, NNZs: 8, Bias: -11.016470, T: 46167, Avg. loss: 439445.165385\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 3499.30, NNZs: 9, Bias: -12.120669, T: 92334, Avg. loss: 83728.625396\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 1610.61, NNZs: 10, Bias: -12.120040, T: 138501, Avg. loss: 53633.218349\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 3020.35, NNZs: 11, Bias: -12.532031, T: 184668, Avg. loss: 34933.008093\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 132.12, NNZs: 11, Bias: -12.400238, T: 230835, Avg. loss: 31850.390138\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 159.71, NNZs: 11, Bias: -12.442016, T: 277002, Avg. loss: 25100.394582\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1802.49, NNZs: 11, Bias: -12.513980, T: 323169, Avg. loss: 20382.612253\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 603.62, NNZs: 11, Bias: -12.624569, T: 369336, Avg. loss: 17429.752029\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 974.57, NNZs: 12, Bias: -12.698683, T: 415503, Avg. loss: 15658.169432\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 608.95, NNZs: 14, Bias: -12.811815, T: 461670, Avg. loss: 13671.511423\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 92.83, NNZs: 14, Bias: -12.909670, T: 507837, Avg. loss: 11605.915701\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 265.80, NNZs: 14, Bias: -12.949260, T: 554004, Avg. loss: 11745.189402\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 5.75, NNZs: 14, Bias: -12.932145, T: 600171, Avg. loss: 11377.955877\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 344.57, NNZs: 14, Bias: -12.980656, T: 646338, Avg. loss: 9935.974474\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 329.89, NNZs: 14, Bias: -13.023939, T: 692505, Avg. loss: 8923.864702\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 241.53, NNZs: 14, Bias: -13.094108, T: 738672, Avg. loss: 8429.431107\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 8.57, NNZs: 14, Bias: -13.094445, T: 784839, Avg. loss: 8048.145163\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 573.01, NNZs: 14, Bias: -13.093808, T: 831006, Avg. loss: 8216.424809\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 237.21, NNZs: 15, Bias: -13.106089, T: 877173, Avg. loss: 7180.468229\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 28.33, NNZs: 15, Bias: -13.106145, T: 923340, Avg. loss: 6894.588592\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 326.99, NNZs: 16, Bias: -13.106948, T: 969507, Avg. loss: 6643.370117\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 98.64, NNZs: 16, Bias: -13.107354, T: 1015674, Avg. loss: 6449.747768\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 27.44, NNZs: 16, Bias: -13.117584, T: 1061841, Avg. loss: 6166.516737\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 116.00, NNZs: 16, Bias: -13.080760, T: 1108008, Avg. loss: 6163.895783\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 89.35, NNZs: 16, Bias: -13.098790, T: 1154175, Avg. loss: 5509.593767\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 271.32, NNZs: 16, Bias: -13.133526, T: 1200342, Avg. loss: 5364.687634\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 68.29, NNZs: 16, Bias: -13.157833, T: 1246509, Avg. loss: 4838.796430\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 271.48, NNZs: 16, Bias: -13.181843, T: 1292676, Avg. loss: 4840.001138\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 84.48, NNZs: 16, Bias: -13.166771, T: 1338843, Avg. loss: 4780.728605\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 401.36, NNZs: 16, Bias: -13.181412, T: 1385010, Avg. loss: 4471.494493\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 31.55, NNZs: 16, Bias: -13.181515, T: 1431177, Avg. loss: 4251.166135\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 175.53, NNZs: 16, Bias: -13.174456, T: 1477344, Avg. loss: 4557.071385\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 52.06, NNZs: 16, Bias: -13.187827, T: 1523511, Avg. loss: 4160.976888\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 321.98, NNZs: 16, Bias: -13.206838, T: 1569678, Avg. loss: 4003.162115\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 71.30, NNZs: 16, Bias: -13.232263, T: 1615845, Avg. loss: 3631.960377\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 191.60, NNZs: 16, Bias: -13.262618, T: 1662012, Avg. loss: 3655.889371\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 16.62, NNZs: 16, Bias: -13.292134, T: 1708179, Avg. loss: 3391.963398\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 47.57, NNZs: 16, Bias: -13.297827, T: 1754346, Avg. loss: 3562.743629\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 11.61, NNZs: 16, Bias: -13.309293, T: 1800513, Avg. loss: 3437.862562\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 1.14, NNZs: 16, Bias: -13.336863, T: 1846680, Avg. loss: 3288.281323\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 244.64, NNZs: 16, Bias: -13.358118, T: 1892847, Avg. loss: 3186.035592\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 5.23, NNZs: 16, Bias: -13.358090, T: 1939014, Avg. loss: 3076.559659\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 112.35, NNZs: 16, Bias: -13.378346, T: 1985181, Avg. loss: 3038.531351\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 51.03, NNZs: 16, Bias: -13.388257, T: 2031348, Avg. loss: 3068.320582\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 251.68, NNZs: 16, Bias: -13.422371, T: 2077515, Avg. loss: 2679.846957\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 230.96, NNZs: 16, Bias: -13.441553, T: 2123682, Avg. loss: 2885.224605\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 26.78, NNZs: 16, Bias: -13.450862, T: 2169849, Avg. loss: 2804.598063\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 120.30, NNZs: 16, Bias: -13.464498, T: 2216016, Avg. loss: 2827.589857\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 37.77, NNZs: 16, Bias: -13.460096, T: 2262183, Avg. loss: 2778.399989\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 62.63, NNZs: 16, Bias: -13.477622, T: 2308350, Avg. loss: 2729.779482\n",
            "Total training time: 0.32 seconds.\n",
            "Convergence after 50 epochs took 0.32 seconds\n",
            "-- Epoch 1\n",
            "Norm: 6171.18, NNZs: 3, Bias: -9.999941, T: 46167, Avg. loss: 32343.015826\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 5638.31, NNZs: 5, Bias: -9.999939, T: 92334, Avg. loss: 5847.990998\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 956.13, NNZs: 5, Bias: -9.911308, T: 138501, Avg. loss: 3968.997711\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1158.09, NNZs: 5, Bias: -9.911307, T: 184668, Avg. loss: 2656.271361\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1491.11, NNZs: 5, Bias: -9.911307, T: 230835, Avg. loss: 1815.428686\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1509.38, NNZs: 5, Bias: -9.911307, T: 277002, Avg. loss: 1318.882373\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 82.52, NNZs: 5, Bias: -9.878011, T: 323169, Avg. loss: 1188.886356\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 23.30, NNZs: 5, Bias: -9.933466, T: 369336, Avg. loss: 767.770662\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 6.79, NNZs: 5, Bias: -9.933466, T: 415503, Avg. loss: 834.374829\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 189.90, NNZs: 6, Bias: -9.980532, T: 461670, Avg. loss: 576.670596\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 89.52, NNZs: 8, Bias: -10.001130, T: 507837, Avg. loss: 582.227043\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 554.52, NNZs: 9, Bias: -10.001130, T: 554004, Avg. loss: 990.988144\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 503.39, NNZs: 9, Bias: -10.001130, T: 600171, Avg. loss: 560.905182\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 388.84, NNZs: 9, Bias: -10.001130, T: 646338, Avg. loss: 515.482823\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 703.60, NNZs: 9, Bias: -10.015621, T: 692505, Avg. loss: 414.455639\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 128.56, NNZs: 9, Bias: -10.001585, T: 738672, Avg. loss: 582.830817\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 283.18, NNZs: 9, Bias: -10.015061, T: 784839, Avg. loss: 364.261607\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 301.24, NNZs: 9, Bias: -10.015061, T: 831006, Avg. loss: 435.682759\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 527.41, NNZs: 10, Bias: -10.015061, T: 877173, Avg. loss: 474.136310\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 76.05, NNZs: 10, Bias: -10.004181, T: 923340, Avg. loss: 428.811253\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 549.15, NNZs: 10, Bias: -10.014752, T: 969507, Avg. loss: 372.053698\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 137.79, NNZs: 10, Bias: -10.004855, T: 1015674, Avg. loss: 448.818470\n",
            "Total training time: 0.15 seconds.\n",
            "Convergence after 22 epochs took 0.15 seconds\n",
            "-- Epoch 1\n",
            "Norm: 9755.97, NNZs: 2, Bias: -8.766194, T: 46167, Avg. loss: 26702.355075\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 3206.58, NNZs: 2, Bias: -8.645578, T: 92334, Avg. loss: 1933.978674\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 992.19, NNZs: 2, Bias: -8.563376, T: 138501, Avg. loss: 857.253159\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 2266.58, NNZs: 2, Bias: -8.563375, T: 184668, Avg. loss: 393.138179\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1121.32, NNZs: 2, Bias: -8.509766, T: 230835, Avg. loss: 786.130304\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 356.44, NNZs: 2, Bias: -8.467497, T: 277002, Avg. loss: 382.891575\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 950.90, NNZs: 2, Bias: -8.467497, T: 323169, Avg. loss: 264.963626\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 397.96, NNZs: 2, Bias: -8.440147, T: 369336, Avg. loss: 293.765917\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 663.48, NNZs: 2, Bias: -8.440147, T: 415503, Avg. loss: 144.897983\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 249.57, NNZs: 2, Bias: -8.416261, T: 461670, Avg. loss: 229.996869\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 91.94, NNZs: 2, Bias: -8.416261, T: 507837, Avg. loss: 104.732778\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 92.22, NNZs: 3, Bias: -8.416261, T: 554004, Avg. loss: 111.454912\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 23.49, NNZs: 3, Bias: -8.416261, T: 600171, Avg. loss: 78.816403\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 588.66, NNZs: 3, Bias: -8.416261, T: 646338, Avg. loss: 276.120799\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 317.50, NNZs: 3, Bias: -8.400885, T: 692505, Avg. loss: 204.166608\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 80.20, NNZs: 3, Bias: -8.386869, T: 738672, Avg. loss: 107.538724\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 209.29, NNZs: 5, Bias: -8.386869, T: 784839, Avg. loss: 102.973318\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 4.33, NNZs: 5, Bias: -8.374199, T: 831006, Avg. loss: 72.612930\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 217.41, NNZs: 5, Bias: -8.374199, T: 877173, Avg. loss: 138.556290\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 32.51, NNZs: 5, Bias: -8.363349, T: 923340, Avg. loss: 72.179912\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 410.70, NNZs: 5, Bias: -8.363349, T: 969507, Avg. loss: 172.581579\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 233.82, NNZs: 5, Bias: -8.353371, T: 1015674, Avg. loss: 138.591024\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 72.31, NNZs: 5, Bias: -8.343960, T: 1061841, Avg. loss: 77.953410\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 129.53, NNZs: 5, Bias: -8.343960, T: 1108008, Avg. loss: 61.589763\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 383.04, NNZs: 5, Bias: -8.343960, T: 1154175, Avg. loss: 60.344164\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 234.41, NNZs: 5, Bias: -8.335418, T: 1200342, Avg. loss: 131.706529\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 96.79, NNZs: 5, Bias: -8.327387, T: 1246509, Avg. loss: 78.805894\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 268.85, NNZs: 5, Bias: -8.327387, T: 1292676, Avg. loss: 59.681141\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 139.52, NNZs: 5, Bias: -8.319684, T: 1338843, Avg. loss: 93.353003\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 18.80, NNZs: 5, Bias: -8.312428, T: 1385010, Avg. loss: 47.265488\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 243.26, NNZs: 7, Bias: -8.319486, T: 1431177, Avg. loss: 102.521737\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 126.84, NNZs: 7, Bias: -8.312553, T: 1477344, Avg. loss: 84.164028\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 17.48, NNZs: 7, Bias: -8.305983, T: 1523511, Avg. loss: 42.927835\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 156.17, NNZs: 7, Bias: -8.305983, T: 1569678, Avg. loss: 76.888142\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 52.21, NNZs: 7, Bias: -8.299652, T: 1615845, Avg. loss: 54.114854\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 190.68, NNZs: 7, Bias: -8.305679, T: 1662012, Avg. loss: 34.041853\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 91.41, NNZs: 7, Bias: -8.299788, T: 1708179, Avg. loss: 65.090543\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 271.90, NNZs: 7, Bias: -8.299788, T: 1754346, Avg. loss: 34.089393\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 175.64, NNZs: 7, Bias: -8.294149, T: 1800513, Avg. loss: 93.785372\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 84.18, NNZs: 7, Bias: -8.288718, T: 1846680, Avg. loss: 59.872622\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 226.34, NNZs: 7, Bias: -8.288718, T: 1892847, Avg. loss: 31.691300\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 138.04, NNZs: 7, Bias: -8.283539, T: 1939014, Avg. loss: 77.358862\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 53.83, NNZs: 7, Bias: -8.278412, T: 1985181, Avg. loss: 47.845052\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 155.55, NNZs: 7, Bias: -8.278412, T: 2031348, Avg. loss: 40.022120\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 74.70, NNZs: 7, Bias: -8.273557, T: 2077515, Avg. loss: 53.481476\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 150.52, NNZs: 7, Bias: -8.273557, T: 2123682, Avg. loss: 27.814751\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 73.21, NNZs: 7, Bias: -8.268907, T: 2169849, Avg. loss: 51.821157\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 142.99, NNZs: 7, Bias: -8.268907, T: 2216016, Avg. loss: 26.059608\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 68.99, NNZs: 7, Bias: -8.264410, T: 2262183, Avg. loss: 49.664983\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 170.51, NNZs: 7, Bias: -8.264410, T: 2308350, Avg. loss: 25.783138\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 98.87, NNZs: 7, Bias: -8.260157, T: 2354517, Avg. loss: 58.367028\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 29.99, NNZs: 7, Bias: -8.255979, T: 2400684, Avg. loss: 33.901071\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 4.28, NNZs: 7, Bias: -8.255979, T: 2446851, Avg. loss: 18.150451\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 150.69, NNZs: 7, Bias: -8.259995, T: 2493018, Avg. loss: 24.929795\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 84.62, NNZs: 7, Bias: -8.256029, T: 2539185, Avg. loss: 51.936381\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 20.91, NNZs: 7, Bias: -8.252147, T: 2585352, Avg. loss: 29.071931\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 119.74, NNZs: 7, Bias: -8.252147, T: 2631519, Avg. loss: 44.770980\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 57.63, NNZs: 7, Bias: -8.248357, T: 2677686, Avg. loss: 41.634525\n",
            "Total training time: 0.38 seconds.\n",
            "Convergence after 58 epochs took 0.38 seconds\n",
            "-- Epoch 1\n",
            "Norm: 6554.72, NNZs: 2, Bias: -9.456876, T: 46167, Avg. loss: 1337.248617\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 2956.72, NNZs: 2, Bias: -9.331267, T: 92334, Avg. loss: 279.265316\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 1740.22, NNZs: 2, Bias: -9.232456, T: 138501, Avg. loss: 196.091907\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1128.69, NNZs: 2, Bias: -9.178274, T: 184668, Avg. loss: 94.590383\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 760.72, NNZs: 2, Bias: -9.126024, T: 230835, Avg. loss: 78.740659\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 514.96, NNZs: 2, Bias: -9.084654, T: 277002, Avg. loss: 52.468797\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 339.21, NNZs: 2, Bias: -9.050876, T: 323169, Avg. loss: 34.775005\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 207.27, NNZs: 2, Bias: -9.023473, T: 369336, Avg. loss: 21.669089\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 104.59, NNZs: 2, Bias: -8.997468, T: 415503, Avg. loss: 14.355141\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 22.39, NNZs: 2, Bias: -8.974220, T: 461670, Avg. loss: 7.282695\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 845.42, NNZs: 2, Bias: -8.974220, T: 507837, Avg. loss: 46.238283\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 715.28, NNZs: 2, Bias: -8.955235, T: 554004, Avg. loss: 58.731708\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 605.12, NNZs: 2, Bias: -8.938475, T: 600171, Avg. loss: 47.846728\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 510.68, NNZs: 2, Bias: -8.922454, T: 646338, Avg. loss: 41.911118\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 428.81, NNZs: 2, Bias: -8.907834, T: 692505, Avg. loss: 34.757200\n",
            "Total training time: 0.10 seconds.\n",
            "Convergence after 15 epochs took 0.10 seconds\n",
            "-- Epoch 1\n",
            "Norm: 3482.17, NNZs: 5, Bias: -8.082044, T: 46167, Avg. loss: 345962.141299\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 4241.59, NNZs: 7, Bias: -8.179362, T: 92334, Avg. loss: 75274.206937\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 3503.73, NNZs: 9, Bias: -8.395054, T: 138501, Avg. loss: 39795.519977\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 148.42, NNZs: 10, Bias: -8.132989, T: 184668, Avg. loss: 33977.586044\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1495.13, NNZs: 11, Bias: -8.042483, T: 230835, Avg. loss: 26587.195702\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 238.15, NNZs: 11, Bias: -7.924518, T: 277002, Avg. loss: 20484.780548\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 901.54, NNZs: 11, Bias: -8.096515, T: 323169, Avg. loss: 15929.138377\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 550.92, NNZs: 11, Bias: -8.180754, T: 369336, Avg. loss: 13034.554447\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 349.90, NNZs: 11, Bias: -8.229269, T: 415503, Avg. loss: 12055.360008\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 955.14, NNZs: 11, Bias: -8.158955, T: 461670, Avg. loss: 12070.367033\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 341.93, NNZs: 13, Bias: -8.136873, T: 507837, Avg. loss: 10493.422661\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 172.80, NNZs: 13, Bias: -8.117972, T: 554004, Avg. loss: 9664.599353\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 223.94, NNZs: 13, Bias: -8.119275, T: 600171, Avg. loss: 8614.144099\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 482.33, NNZs: 13, Bias: -8.134526, T: 646338, Avg. loss: 7802.228263\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 72.68, NNZs: 13, Bias: -8.285742, T: 692505, Avg. loss: 6957.776922\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 264.56, NNZs: 13, Bias: -8.299375, T: 738672, Avg. loss: 7071.279455\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 10.52, NNZs: 13, Bias: -8.312808, T: 784839, Avg. loss: 6101.435272\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 243.95, NNZs: 13, Bias: -8.288548, T: 831006, Avg. loss: 6269.473682\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 627.48, NNZs: 13, Bias: -8.312681, T: 877173, Avg. loss: 5815.708878\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 403.06, NNZs: 13, Bias: -8.290462, T: 923340, Avg. loss: 5977.913644\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 182.98, NNZs: 13, Bias: -8.290405, T: 969507, Avg. loss: 5384.781884\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 8.91, NNZs: 13, Bias: -8.260377, T: 1015674, Avg. loss: 5132.584984\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 201.67, NNZs: 14, Bias: -8.270611, T: 1061841, Avg. loss: 4958.221278\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 481.85, NNZs: 14, Bias: -8.279380, T: 1108008, Avg. loss: 4605.105889\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 153.47, NNZs: 14, Bias: -8.296646, T: 1154175, Avg. loss: 4102.828490\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 22.02, NNZs: 14, Bias: -8.288323, T: 1200342, Avg. loss: 4239.384877\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 274.02, NNZs: 14, Bias: -8.296543, T: 1246509, Avg. loss: 3965.267917\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 328.03, NNZs: 14, Bias: -8.280675, T: 1292676, Avg. loss: 4200.035722\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 48.07, NNZs: 14, Bias: -8.296088, T: 1338843, Avg. loss: 3686.249277\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 336.80, NNZs: 14, Bias: -8.325593, T: 1385010, Avg. loss: 3525.693389\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 3.33, NNZs: 14, Bias: -8.318569, T: 1431177, Avg. loss: 3714.185077\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 336.25, NNZs: 14, Bias: -8.318429, T: 1477344, Avg. loss: 3331.261675\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 252.19, NNZs: 14, Bias: -8.318388, T: 1523511, Avg. loss: 3360.447351\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 50.69, NNZs: 14, Bias: -8.318385, T: 1569678, Avg. loss: 3176.274513\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 251.30, NNZs: 14, Bias: -8.331041, T: 1615845, Avg. loss: 3088.569274\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 21.94, NNZs: 14, Bias: -8.306564, T: 1662012, Avg. loss: 3386.848779\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 90.23, NNZs: 14, Bias: -8.306692, T: 1708179, Avg. loss: 2939.358887\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 228.64, NNZs: 14, Bias: -8.306728, T: 1754346, Avg. loss: 2897.842133\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 121.08, NNZs: 14, Bias: -8.306685, T: 1800513, Avg. loss: 2882.474488\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 109.73, NNZs: 14, Bias: -8.301300, T: 1846680, Avg. loss: 2655.315178\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 241.33, NNZs: 14, Bias: -8.301217, T: 1892847, Avg. loss: 2666.844711\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 158.76, NNZs: 14, Bias: -8.311575, T: 1939014, Avg. loss: 2666.853826\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 102.30, NNZs: 14, Bias: -8.311617, T: 1985181, Avg. loss: 2447.081401\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 9.19, NNZs: 14, Bias: -8.316745, T: 2031348, Avg. loss: 2345.365161\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 135.60, NNZs: 14, Bias: -8.316766, T: 2077515, Avg. loss: 2463.816698\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 136.93, NNZs: 14, Bias: -8.326069, T: 2123682, Avg. loss: 2383.501665\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 3.54, NNZs: 14, Bias: -8.326020, T: 2169849, Avg. loss: 2273.848233\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 59.80, NNZs: 14, Bias: -8.321408, T: 2216016, Avg. loss: 2310.175681\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 7.56, NNZs: 14, Bias: -8.317127, T: 2262183, Avg. loss: 2270.852799\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 13.65, NNZs: 14, Bias: -8.312828, T: 2308350, Avg. loss: 2221.153659\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 77.16, NNZs: 14, Bias: -8.304256, T: 2354517, Avg. loss: 2221.956802\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 29.45, NNZs: 14, Bias: -8.308407, T: 2400684, Avg. loss: 1983.259262\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 93.44, NNZs: 14, Bias: -8.300191, T: 2446851, Avg. loss: 2178.819003\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 204.78, NNZs: 14, Bias: -8.296149, T: 2493018, Avg. loss: 2018.134985\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 22.05, NNZs: 14, Bias: -8.288098, T: 2539185, Avg. loss: 1996.128556\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 62.85, NNZs: 14, Bias: -8.284184, T: 2585352, Avg. loss: 1963.986638\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 4.80, NNZs: 14, Bias: -8.284223, T: 2631519, Avg. loss: 1882.370025\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 133.10, NNZs: 15, Bias: -8.291824, T: 2677686, Avg. loss: 2036.297310\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 170.39, NNZs: 15, Bias: -8.291913, T: 2723853, Avg. loss: 1845.265105\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 13.40, NNZs: 15, Bias: -8.284611, T: 2770020, Avg. loss: 1822.283181\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 55.31, NNZs: 15, Bias: -8.284584, T: 2816187, Avg. loss: 1846.466437\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 82.30, NNZs: 15, Bias: -8.277567, T: 2862354, Avg. loss: 1794.253761\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 148.43, NNZs: 15, Bias: -8.277599, T: 2908521, Avg. loss: 1757.792906\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 7.82, NNZs: 15, Bias: -8.270738, T: 2954688, Avg. loss: 1767.773809\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 111.73, NNZs: 15, Bias: -8.284107, T: 3000855, Avg. loss: 1558.781916\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 175.12, NNZs: 15, Bias: -8.290701, T: 3047022, Avg. loss: 1659.117934\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 21.22, NNZs: 15, Bias: -8.277678, T: 3093189, Avg. loss: 1726.866553\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 43.74, NNZs: 15, Bias: -8.271261, T: 3139356, Avg. loss: 1726.651572\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 23.95, NNZs: 15, Bias: -8.280718, T: 3185523, Avg. loss: 1481.808498\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 48.91, NNZs: 15, Bias: -8.286903, T: 3231690, Avg. loss: 1601.277751\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 27.78, NNZs: 15, Bias: -8.296105, T: 3277857, Avg. loss: 1386.522361\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 57.33, NNZs: 15, Bias: -8.296074, T: 3324024, Avg. loss: 1568.930256\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 99.01, NNZs: 15, Bias: -8.278164, T: 3370191, Avg. loss: 1718.185685\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 7.46, NNZs: 16, Bias: -8.278212, T: 3416358, Avg. loss: 1446.663957\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 50.21, NNZs: 16, Bias: -8.284014, T: 3462525, Avg. loss: 1427.166690\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 94.96, NNZs: 16, Bias: -8.281118, T: 3508692, Avg. loss: 1455.141953\n",
            "Total training time: 0.51 seconds.\n",
            "Convergence after 76 epochs took 0.51 seconds\n",
            "-- Epoch 1\n",
            "Norm: 5604.67, NNZs: 5, Bias: -6.669114, T: 46167, Avg. loss: 212236.820864\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 88.20, NNZs: 7, Bias: -6.541423, T: 92334, Avg. loss: 39048.333430\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 795.99, NNZs: 8, Bias: -6.639705, T: 138501, Avg. loss: 22852.676971\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 660.33, NNZs: 9, Bias: -6.474780, T: 184668, Avg. loss: 17381.315409\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 356.86, NNZs: 9, Bias: -6.277383, T: 230835, Avg. loss: 15011.727645\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1111.44, NNZs: 9, Bias: -6.201346, T: 277002, Avg. loss: 11523.734718\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 392.12, NNZs: 11, Bias: -6.039844, T: 323169, Avg. loss: 10003.714972\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 188.46, NNZs: 11, Bias: -5.985293, T: 369336, Avg. loss: 7671.016936\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 963.72, NNZs: 11, Bias: -6.032929, T: 415503, Avg. loss: 6745.090046\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1110.34, NNZs: 12, Bias: -5.965828, T: 461670, Avg. loss: 6365.128420\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 115.40, NNZs: 12, Bias: -5.924056, T: 507837, Avg. loss: 5754.688730\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 767.35, NNZs: 12, Bias: -5.905512, T: 554004, Avg. loss: 5508.343759\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 313.66, NNZs: 12, Bias: -5.870915, T: 600171, Avg. loss: 4711.121480\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 701.12, NNZs: 13, Bias: -5.838972, T: 646338, Avg. loss: 4304.663991\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 761.89, NNZs: 13, Bias: -5.823578, T: 692505, Avg. loss: 3853.875003\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 504.92, NNZs: 13, Bias: -5.809229, T: 738672, Avg. loss: 3780.062067\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 657.66, NNZs: 13, Bias: -5.769887, T: 784839, Avg. loss: 3665.572946\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 61.70, NNZs: 13, Bias: -5.707367, T: 831006, Avg. loss: 3743.072411\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 73.48, NNZs: 13, Bias: -5.696056, T: 877173, Avg. loss: 2958.678842\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 306.57, NNZs: 13, Bias: -5.696433, T: 923340, Avg. loss: 3037.020627\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 60.47, NNZs: 13, Bias: -5.675050, T: 969507, Avg. loss: 2767.564805\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 32.78, NNZs: 13, Bias: -5.674969, T: 1015674, Avg. loss: 2603.578488\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 193.97, NNZs: 13, Bias: -5.645984, T: 1061841, Avg. loss: 2768.307596\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 44.96, NNZs: 13, Bias: -5.627470, T: 1108008, Avg. loss: 2456.456639\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 340.19, NNZs: 13, Bias: -5.627556, T: 1154175, Avg. loss: 2376.738925\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 397.07, NNZs: 13, Bias: -5.627649, T: 1200342, Avg. loss: 2403.549696\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 231.18, NNZs: 13, Bias: -5.603205, T: 1246509, Avg. loss: 2414.596975\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 84.33, NNZs: 13, Bias: -5.603240, T: 1292676, Avg. loss: 2068.892063\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 29.23, NNZs: 13, Bias: -5.596081, T: 1338843, Avg. loss: 2046.991175\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 55.42, NNZs: 13, Bias: -5.610888, T: 1385010, Avg. loss: 1842.280607\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 201.86, NNZs: 13, Bias: -5.625277, T: 1431177, Avg. loss: 1698.191545\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 355.30, NNZs: 13, Bias: -5.645922, T: 1477344, Avg. loss: 1758.947843\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 402.19, NNZs: 13, Bias: -5.632568, T: 1523511, Avg. loss: 1711.632280\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 153.60, NNZs: 13, Bias: -5.625787, T: 1569678, Avg. loss: 1784.225207\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 14.00, NNZs: 13, Bias: -5.607044, T: 1615845, Avg. loss: 1758.381571\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 158.65, NNZs: 13, Bias: -5.606971, T: 1662012, Avg. loss: 1707.631396\n",
            "Total training time: 0.24 seconds.\n",
            "Convergence after 36 epochs took 0.24 seconds\n",
            "-- Epoch 1\n",
            "Norm: 2368.44, NNZs: 2, Bias: -9.608442, T: 46167, Avg. loss: 28980.022679\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 2550.01, NNZs: 6, Bias: -10.179758, T: 92334, Avg. loss: 7829.499609\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 2075.70, NNZs: 6, Bias: -10.270411, T: 138501, Avg. loss: 5529.271978\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 767.07, NNZs: 6, Bias: -10.270411, T: 184668, Avg. loss: 3238.490535\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 987.78, NNZs: 6, Bias: -10.270410, T: 230835, Avg. loss: 3164.143943\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 394.71, NNZs: 6, Bias: -10.308251, T: 277002, Avg. loss: 1670.809249\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 581.03, NNZs: 6, Bias: -10.308251, T: 323169, Avg. loss: 2331.306450\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 289.63, NNZs: 7, Bias: -10.338578, T: 369336, Avg. loss: 1343.652121\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 351.74, NNZs: 7, Bias: -10.338578, T: 415503, Avg. loss: 1628.482047\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 691.90, NNZs: 7, Bias: -10.382163, T: 461670, Avg. loss: 926.113064\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 122.44, NNZs: 7, Bias: -10.382163, T: 507837, Avg. loss: 1019.112889\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 135.13, NNZs: 8, Bias: -10.400495, T: 554004, Avg. loss: 987.617864\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 67.98, NNZs: 8, Bias: -10.418241, T: 600171, Avg. loss: 835.909152\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 129.45, NNZs: 8, Bias: -10.418241, T: 646338, Avg. loss: 1089.848828\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 253.74, NNZs: 8, Bias: -10.418241, T: 692505, Avg. loss: 1023.270850\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 647.59, NNZs: 9, Bias: -10.473971, T: 738672, Avg. loss: 552.157164\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 657.60, NNZs: 9, Bias: -10.473971, T: 784839, Avg. loss: 816.477059\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 450.63, NNZs: 9, Bias: -10.473971, T: 831006, Avg. loss: 731.453854\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 104.97, NNZs: 9, Bias: -10.473971, T: 877173, Avg. loss: 591.010988\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 537.44, NNZs: 9, Bias: -10.485141, T: 923340, Avg. loss: 595.149639\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 169.31, NNZs: 9, Bias: -10.485141, T: 969507, Avg. loss: 625.004069\n",
            "Total training time: 0.13 seconds.\n",
            "Convergence after 21 epochs took 0.13 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  76 out of  76 | elapsed:   52.7s finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
              "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
              "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
              "              power_t=0.5, random_state=None, shuffle=True, tol=0.01,\n",
              "              validation_fraction=0.1, verbose=True, warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSOEd-J31sww",
        "outputId": "dfdd627a-eee8-409d-ff23-202c019cf9fc"
      },
      "source": [
        "np.set_printoptions(threshold=100)\n",
        "\n",
        "y_pred = sgdc.predict(X_test)\n",
        "\n",
        "confusion_matrix(y_test, y_pred)\n"
      ],
      "id": "XSOEd-J31sww",
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YylvO5B1_y9",
        "outputId": "6f5e16a6-6bdb-4a43-aa54-ee7cf9a7f665"
      },
      "source": [
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ],
      "id": "-YylvO5B1_y9",
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          ar       0.00      0.00      0.00       525\n",
            "     ar_LATN       0.00      0.00      0.00         5\n",
            "          bn       0.00      0.00      0.00         1\n",
            "          bs       0.00      0.00      0.00         2\n",
            "          ca       0.00      0.00      0.00         5\n",
            "          da       0.00      0.00      0.00         2\n",
            "          de       0.00      0.00      0.00        40\n",
            "          el       0.00      0.00      0.00        12\n",
            "          en       1.00      0.00      0.00      4588\n",
            "          es       0.00      0.00      0.00      1517\n",
            "          et       0.00      0.00      0.00         1\n",
            "          eu       0.00      0.00      0.00         2\n",
            "          fa       0.00      0.00      0.00         8\n",
            "          fi       0.00      0.00      0.00         4\n",
            "          fr       0.00      0.00      0.00       228\n",
            "          gl       0.00      0.00      0.00         1\n",
            "          he       0.00      0.00      0.00        11\n",
            "          hi       0.00      0.00      0.00         3\n",
            "     hi-Latn       0.00      0.00      0.00         4\n",
            "          ht       0.00      0.00      0.00         1\n",
            "          hu       0.00      0.00      0.00         6\n",
            "          id       0.00      0.00      0.00       752\n",
            "          it       0.00      0.00      0.00        79\n",
            "          ja       1.00      0.00      0.01      2611\n",
            "          jv       0.00      0.00      0.00         2\n",
            "          ko       0.00      0.00      0.00       127\n",
            "          lv       0.00      0.00      0.00         1\n",
            "          ms       0.00      0.00      0.00        20\n",
            "          ne       0.00      0.00      0.00         1\n",
            "          nl       0.00      0.00      0.00        47\n",
            "          no       0.00      0.00      0.00         3\n",
            "          pl       0.00      0.00      0.00        31\n",
            "          pt       0.00      0.00      0.00       727\n",
            "          ro       0.00      0.00      0.00         3\n",
            "          ru       0.00      0.00      0.00       250\n",
            "          sk       0.00      0.00      0.00         1\n",
            "          sq       0.00      0.00      0.00         3\n",
            "          sr       0.00      0.00      0.00         3\n",
            "          su       0.00      0.00      0.00         2\n",
            "          sv       0.00      0.00      0.00        14\n",
            "          sw       0.00      0.00      0.00         2\n",
            "          ta       0.00      0.00      0.00         3\n",
            "     ta_LATN       0.00      0.00      0.00         1\n",
            "          th       0.00      0.00      0.00       111\n",
            "          tl       0.00      0.00      0.00        84\n",
            "          tr       0.01      0.97      0.02       156\n",
            "          uk       0.00      0.00      0.00         2\n",
            "         und       0.12      0.02      0.03      1172\n",
            "          ur       0.00      0.00      0.00         1\n",
            "     ur_LATN       0.00      0.00      0.00         5\n",
            "          vi       0.00      0.00      0.00         3\n",
            "          xh       0.00      0.00      0.00         1\n",
            "       zh-CN       0.00      0.00      0.00         5\n",
            "       zh-TW       0.00      0.00      0.00         2\n",
            "\n",
            "    accuracy                           0.01     13191\n",
            "   macro avg       0.04      0.02      0.00     13191\n",
            "weighted avg       0.56      0.01      0.00     13191\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCP5CkP_M7eu"
      },
      "source": [
        "#8.Evaluation"
      ],
      "id": "iCP5CkP_M7eu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6YDEr1MNBJr"
      },
      "source": [
        ""
      ],
      "id": "A6YDEr1MNBJr",
      "execution_count": null,
      "outputs": []
    }
  ]
}