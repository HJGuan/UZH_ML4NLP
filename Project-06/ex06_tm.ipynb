{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "ex06_tm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jan-kreischer/UZH_ML4NLP/blob/main/Project-06/ex06_tm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86zA-HlTCBID"
      },
      "source": [
        "##Project 6 - Topic Modeling \n",
        "using Latent Dirichlet Allocation (LDA)  \n",
        "and Combined Topic Models (CTM).  \n",
        "## 1. Setup\n",
        "### 1.1 Dependencies\n",
        "Installing all dependencies needed to run the simulations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hENmFU8FmZbl",
        "outputId": "d203392e-bfdd-4cde-acf9-6c654cc72872"
      },
      "source": [
        "!pip install contextualized-topic-models==2.2.0"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contextualized-topic-models==2.2.0 in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from contextualized-topic-models==2.2.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: ipywidgets==7.5.1 in /usr/local/lib/python3.7/dist-packages (from contextualized-topic-models==2.2.0) (7.5.1)\n",
            "Requirement already satisfied: torchvision>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from contextualized-topic-models==2.2.0) (0.11.1+cu111)\n",
            "Requirement already satisfied: tqdm>=4.56.0 in /usr/local/lib/python3.7/dist-packages (from contextualized-topic-models==2.2.0) (4.62.3)\n",
            "Requirement already satisfied: ipython==7.16.1 in /usr/local/lib/python3.7/dist-packages (from contextualized-topic-models==2.2.0) (7.16.1)\n",
            "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.7/dist-packages (from contextualized-topic-models==2.2.0) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from contextualized-topic-models==2.2.0) (1.4.1)\n",
            "Requirement already satisfied: gensim>=3.8.3 in /usr/local/lib/python3.7/dist-packages (from contextualized-topic-models==2.2.0) (4.1.2)\n",
            "Requirement already satisfied: matplotlib>=3.1.3 in /usr/local/lib/python3.7/dist-packages (from contextualized-topic-models==2.2.0) (3.2.2)\n",
            "Requirement already satisfied: sentence-transformers>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from contextualized-topic-models==2.2.0) (2.1.0)\n",
            "Requirement already satisfied: wordcloud>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from contextualized-topic-models==2.2.0) (1.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython==7.16.1->contextualized-topic-models==2.2.0) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython==7.16.1->contextualized-topic-models==2.2.0) (3.0.23)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython==7.16.1->contextualized-topic-models==2.2.0) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython==7.16.1->contextualized-topic-models==2.2.0) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython==7.16.1->contextualized-topic-models==2.2.0) (57.4.0)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.7/dist-packages (from ipython==7.16.1->contextualized-topic-models==2.2.0) (0.18.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython==7.16.1->contextualized-topic-models==2.2.0) (0.2.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython==7.16.1->contextualized-topic-models==2.2.0) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython==7.16.1->contextualized-topic-models==2.2.0) (5.1.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (5.1.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (3.5.2)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (4.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.8.3->contextualized-topic-models==2.2.0) (5.2.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (5.1.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython==7.16.1->contextualized-topic-models==2.2.0) (0.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->contextualized-topic-models==2.2.0) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->contextualized-topic-models==2.2.0) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->contextualized-topic-models==2.2.0) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->contextualized-topic-models==2.2.0) (0.11.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (4.9.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.16.1->contextualized-topic-models==2.2.0) (0.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.1.3->contextualized-topic-models==2.2.0) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=1.1.1->contextualized-topic-models==2.2.0) (1.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=1.1.1->contextualized-topic-models==2.2.0) (4.12.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=1.1.1->contextualized-topic-models==2.2.0) (0.1.96)\n",
            "Requirement already satisfied: tokenizers>=0.10.3 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=1.1.1->contextualized-topic-models==2.2.0) (0.10.3)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=1.1.1->contextualized-topic-models==2.2.0) (0.2.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=1.1.1->contextualized-topic-models==2.2.0) (3.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->contextualized-topic-models==2.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.7.0->contextualized-topic-models==2.2.0) (7.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=1.1.1->contextualized-topic-models==2.2.0) (4.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=1.1.1->contextualized-topic-models==2.2.0) (3.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=1.1.1->contextualized-topic-models==2.2.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=1.1.1->contextualized-topic-models==2.2.0) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=1.1.1->contextualized-topic-models==2.2.0) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=1.1.1->contextualized-topic-models==2.2.0) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=1.1.1->contextualized-topic-models==2.2.0) (0.0.46)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (0.12.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (1.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (2.11.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (0.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers>=1.1.1->contextualized-topic-models==2.2.0) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (2.0.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (4.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (1.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (0.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (0.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized-topic-models==2.2.0) (0.5.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=1.1.1->contextualized-topic-models==2.2.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=1.1.1->contextualized-topic-models==2.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=1.1.1->contextualized-topic-models==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=1.1.1->contextualized-topic-models==2.2.0) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers>=1.1.1->contextualized-topic-models==2.2.0) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers>=1.1.1->contextualized-topic-models==2.2.0) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers>=1.1.1->contextualized-topic-models==2.2.0) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vLndra3Nbqa"
      },
      "source": [
        "### 1.2 Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci16zWWnlC1n"
      },
      "source": [
        "import re\n",
        "import random\n",
        "import os\n",
        "import urllib\n",
        "import urllib.request\n",
        "import gzip\n",
        "import io\n",
        "import csv\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUHMJB4TR7rf"
      },
      "source": [
        "### 1.3 Google Drive\n",
        "We connect Google Drive in order to access stored data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCpOpcUCsZQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7134f88-aa0c-41f1-c2cc-d1d6f39ba2cb"
      },
      "source": [
        "# Enable access to files stored in Google Drive\n",
        "from google.colab import drive\n",
        "# Leave this like it is\n",
        "mountpoint = '/content/drive/' \n",
        "drive.mount(mountpoint)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP0EqBjAwemg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5cc2f09-80f7-40bc-fcb1-aa4751b33c69"
      },
      "source": [
        "# Adapt this path to the folder where your data is stored in google drive\n",
        "base_path = 'My Drive/UZH_ML4NLP/Projects/Project-06/data' \n",
        "data_path = os.path.join(mountpoint, base_path)\n",
        "# Cd into the directory with the git repo\n",
        "% cd $data_path"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/UZH_ML4NLP/Projects/Project-06/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AiKDCyWSVE3"
      },
      "source": [
        "### 1.4 Constants\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m841xWt8SaiK"
      },
      "source": [
        "NUM_LDA_TOPICS = 8 # The number of different topics to identify\n",
        "NUM_FEATURES = 10000\n",
        "MAX_DF=0.5\n",
        "MIN_DF=0.01"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_mkS_zeWbYW"
      },
      "source": [
        "# Path to the data files\n",
        "path_before_1990 = 'titles_before_1990.txt'\n",
        "path_from_1990_to_2009 = 'titles_from_1990_to_2009.txt'\n",
        "path_from_2010 = 'titles_from_2010.txt'"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdW1URDxY6V4"
      },
      "source": [
        "### 1.5 Data Acquisition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUVy4jyGlVH3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "299306c2-8b41-4476-9aaa-9b8320c6343f"
      },
      "source": [
        "# Execute the following cell only once to download the data and write it as a file to your google drive. Afterwards, skip this cell or comment it out.\n",
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# to download the data manually or get more information, go to: https://dblp.org/faq/How+can+I+download+the+whole+dblp+dataset.html\n",
        "url = 'https://dblp.uni-trier.de/xml/dblp.xml.gz'\n",
        "num_titles = 500000  # the (max)number of titles to load \n",
        "\n",
        "\n",
        "def load_gzip_file(url):\n",
        "    \"\"\"Download Gzip-file.\"\"\"\n",
        "    response = urllib.request.urlopen(url)\n",
        "    compressed_file = io.BytesIO(response.read())\n",
        "    decompressed_file = gzip.GzipFile(fileobj=compressed_file)\n",
        "    return decompressed_file\n",
        "\n",
        "def extract_titles(input_file, max_num=40000):\n",
        "    \"\"\"Extract title and publication year of dblp papers, given as input file.\n",
        "    \n",
        "    Divide the papers into 3 time periods. \n",
        "    \n",
        "    Collect max max_num papers per time period.\n",
        "    \"\"\"\n",
        "    pairs_before_1990 = []\n",
        "    count_before_1990 = 0\n",
        "    pairs_from_1990_to_2009 = []\n",
        "    count_from_1990_to_2009 = 0\n",
        "    pairs_from_2010 = []\n",
        "    count_from_2010 = 0\n",
        "    got_title = False\n",
        "    for line in tqdm(input_file):\n",
        "        line_str = line.decode('utf-8')\n",
        "        if got_title: \n",
        "            # we have a title and check for the corresponding year\n",
        "            year_result = re.search(r'<year>(.*)</year>', line_str)\n",
        "            if year_result:\n",
        "                # we also have the year and thus save the title-year pair\n",
        "                year = int(year_result.group(1))\n",
        "                if year < 1990:\n",
        "                    pairs_before_1990.append((title, year))\n",
        "                    count_before_1990 += 1\n",
        "                elif year < 2010:\n",
        "                    pairs_from_1990_to_2009.append((title, year))\n",
        "                    count_from_1990_to_2009 += 1\n",
        "                else:\n",
        "                    pairs_from_2010.append((title, year))\n",
        "                    count_from_2010 += 1\n",
        "                got_title = False\n",
        "        else:\n",
        "            # we have no title and search for title\n",
        "            result = re.search(r'<title>(.*)</title>', line_str)\n",
        "            if result:\n",
        "                title = result.group(1)\n",
        "                if len(title.split(' ')) < 3:  \n",
        "                    # only include titles with at least four words\n",
        "                    continue\n",
        "                got_title = True\n",
        "        \n",
        "        if count_before_1990 >= max_num and count_from_1990_to_2009 >= max_num and count_from_2010 >= max_num:\n",
        "            return pairs_before_1990, pairs_from_1990_to_2009, pairs_from_2010\n",
        "    \n",
        "    return pairs_before_1990, pairs_from_1990_to_2009, pairs_from_2010\n",
        "\n",
        "def save_data(pairs, file_path):\n",
        "    with open(file_path, 'w') as fout:\n",
        "        writer = csv.writer(fout)\n",
        "        for pair in pairs:\n",
        "            writer.writerow(pair)\n",
        "\n",
        "in_file = load_gzip_file(url)\n",
        "pairs_before_1990, pairs_from_1990_to_2009, pairs_from_2010 = extract_titles(in_file)\n",
        "save_data(pairs_before_1990, path_before_1990)\n",
        "save_data(pairs_from_1990_to_2009, path_from_1990_to_2009)\n",
        "save_data(pairs_from_2010, path_from_2010)\n",
        "'''"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom google.colab import drive\\ndrive.mount(\\'/content/drive\\', force_remount=True)\\n\\n# to download the data manually or get more information, go to: https://dblp.org/faq/How+can+I+download+the+whole+dblp+dataset.html\\nurl = \\'https://dblp.uni-trier.de/xml/dblp.xml.gz\\'\\nnum_titles = 500000  # the (max)number of titles to load \\n\\n\\ndef load_gzip_file(url):\\n    \"\"\"Download Gzip-file.\"\"\"\\n    response = urllib.request.urlopen(url)\\n    compressed_file = io.BytesIO(response.read())\\n    decompressed_file = gzip.GzipFile(fileobj=compressed_file)\\n    return decompressed_file\\n\\ndef extract_titles(input_file, max_num=40000):\\n    \"\"\"Extract title and publication year of dblp papers, given as input file.\\n    \\n    Divide the papers into 3 time periods. \\n    \\n    Collect max max_num papers per time period.\\n    \"\"\"\\n    pairs_before_1990 = []\\n    count_before_1990 = 0\\n    pairs_from_1990_to_2009 = []\\n    count_from_1990_to_2009 = 0\\n    pairs_from_2010 = []\\n    count_from_2010 = 0\\n    got_title = False\\n    for line in tqdm(input_file):\\n        line_str = line.decode(\\'utf-8\\')\\n        if got_title: \\n            # we have a title and check for the corresponding year\\n            year_result = re.search(r\\'<year>(.*)</year>\\', line_str)\\n            if year_result:\\n                # we also have the year and thus save the title-year pair\\n                year = int(year_result.group(1))\\n                if year < 1990:\\n                    pairs_before_1990.append((title, year))\\n                    count_before_1990 += 1\\n                elif year < 2010:\\n                    pairs_from_1990_to_2009.append((title, year))\\n                    count_from_1990_to_2009 += 1\\n                else:\\n                    pairs_from_2010.append((title, year))\\n                    count_from_2010 += 1\\n                got_title = False\\n        else:\\n            # we have no title and search for title\\n            result = re.search(r\\'<title>(.*)</title>\\', line_str)\\n            if result:\\n                title = result.group(1)\\n                if len(title.split(\\' \\')) < 3:  \\n                    # only include titles with at least four words\\n                    continue\\n                got_title = True\\n        \\n        if count_before_1990 >= max_num and count_from_1990_to_2009 >= max_num and count_from_2010 >= max_num:\\n            return pairs_before_1990, pairs_from_1990_to_2009, pairs_from_2010\\n    \\n    return pairs_before_1990, pairs_from_1990_to_2009, pairs_from_2010\\n\\ndef save_data(pairs, file_path):\\n    with open(file_path, \\'w\\') as fout:\\n        writer = csv.writer(fout)\\n        for pair in pairs:\\n            writer.writerow(pair)\\n\\nin_file = load_gzip_file(url)\\npairs_before_1990, pairs_from_1990_to_2009, pairs_from_2010 = extract_titles(in_file)\\nsave_data(pairs_before_1990, path_before_1990)\\nsave_data(pairs_from_1990_to_2009, path_from_1990_to_2009)\\nsave_data(pairs_from_2010, path_from_2010)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDhEoD2Tsh3z"
      },
      "source": [
        "## 2. Topic Modeling\n",
        "### 2.1 Using Latent Dirichlet Allocation (LDA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA-jeLNbJsjt"
      },
      "source": [
        "def load_titles(path):\n",
        "  with open(path) as fin:\n",
        "    reader = csv.reader(fin)\n",
        "    titles = [row[0] for row in reader]\n",
        "  return titles"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vb_BI1xZ4kb"
      },
      "source": [
        "# Simple text preprocessing by removing \n",
        "# all letters which are not in roman alphabet\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z ]', '', text)\n",
        "    #text = re.sub(r'\\b\\w{1,3}\\b', ' ', text)\n",
        "    #text = re.sub(' +', ' ', text)\n",
        "    text = text.lower()\n",
        "    return text"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MSaGhKXepPG"
      },
      "source": [
        "# Now we turn the documents (or titles in this case) into a matrix feature representation.\n",
        "def vectorize_data(titles, max_df=MAX_DF, min_df=MIN_DF, max_features=NUM_FEATURES):\n",
        "  tf_vectorizer = CountVectorizer(max_df=max_df, min_df=min_df, max_features=max_features, stop_words='english')\n",
        "  tf = tf_vectorizer.fit_transform(titles)\n",
        "  tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
        "  return tf, tf_feature_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc09sFDe-5kn"
      },
      "source": [
        "#### 2.1.1 - Before the 1990s:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rwq6kF7vqUj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c12896-2da4-4594-87fc-8315cdd5acca"
      },
      "source": [
        "# Load the titles\n",
        "titles_before_1990 = load_titles(path_before_1990)\n",
        "print(\"{} titles before 1990\".format(len(titles_before_1990)))"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40000 titles before 1990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnXGcMlIKxBS",
        "outputId": "5c87394a-41b8-444d-d61b-b71adb6f4b9c"
      },
      "source": [
        "# Show some random samples\n",
        "random.sample(titles_before_1990, 10)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Computation of e<sup>N</sup> for -&#8734;&lt;N&lt;+&#8734;.',\n",
              " 'On the Omega(n log n) Lower Bound for Convex Hull and Maximal Vector Determination.',\n",
              " 'The future of control.',\n",
              " 'The Average Number of Stable Matchings.',\n",
              " 'The binary derivative test: noise filter, crypto aid, and random-number seed selector.',\n",
              " 'The Arithmetic Cube.',\n",
              " 'Database Machines: An Introduction.',\n",
              " 'Technical Note - An Importance Ranking for System Components Based upon Cuts.',\n",
              " 'Formal and informal communication among scientists in sleep research.',\n",
              " 'Universal Asynchronous Iterative Arrays of Mealy Automata.']"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mulG5Sg2NBZ"
      },
      "source": [
        "preprocessed_titles_before_1990 = [preprocess_text(title) for title in titles_before_1990]"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi6xnCvjd8uq",
        "outputId": "6389b0d8-3eac-4760-e43d-7179e4118edc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Show some preprocessed samples\n",
        "random.sample(preprocessed_titles_before_1990, 10)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['parameteradaptive control with configuration aids and supervision functions',\n",
              " 'an improvement in the iterative data flow analysis algorithm',\n",
              " 'a note on the predicatively definable sets of n n nepeicircvoda',\n",
              " 'review of associative networks  the representation and use of knowledge in computers by n v findler ed academic press',\n",
              " 'algorithm  matrix bandwidth and profile reduction f',\n",
              " 'axioms of symmetry throwing darts at the real number line',\n",
              " 'generalized handle grammars and their relation to petri nets',\n",
              " 'calculation of multicategory minimum distance classifier recognition error for binomial measurement distributions',\n",
              " 'costeffectiveness analysis for strategic decisions',\n",
              " 'meanvariance approximations to expected logarithmic utility']"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhgxgCM_01mx"
      },
      "source": [
        "tf_01, tf_feature_names_01 = vectorize_data(preprocessed_titles_before_1990, max_df=0.95, min_df=0.01)"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_927CQlk4C27"
      },
      "source": [
        "lda_01 = LatentDirichletAllocation(n_components=8, max_iter=10, learning_method='online', random_state=42).fit(tf_01)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-8NinuI4kMG",
        "outputId": "ca1d4b7e-f6c7-4055-d160-318649a2b7f3"
      },
      "source": [
        "for topic_idx, topic in enumerate(lda_01.components_):\n",
        "    print(f'Topic {topic_idx}:', end=' ')\n",
        "    print(' '.join([tf_feature_names_01[i] for i in topic.argsort()[:-12 - 1:-1]]))"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: theory problems algorithms simulation decision parallel application solution applications optimal digital control\n",
            "Topic 1: computer logic model programs digital performance design networks using applications systems simulation\n",
            "Topic 2: problem programming optimal language digital processing software research solution parallel linear control\n",
            "Topic 3: data method network models application languages solution processing problem using analysis programming\n",
            "Topic 4: note information linear functions applications finite technical programming systems time problem decision\n",
            "Topic 5: algorithm design analysis approach sets performance new using implementation parallel linear digital\n",
            "Topic 6: systems using parallel performance implementation decision distributed linear control digital design processing\n",
            "Topic 7: control networks new recognition distributed time pattern optimal systems approach digital linear\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbx0DythfCkP"
      },
      "source": [
        "Topics:\n",
        "0. Graph/networks algorithms (seems to be mostly about algorithms that (maybe) operate on graphs/networks)\n",
        "1. pattern recognition (and maybe robotics)\n",
        "2. ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqioi0rG_Dz9"
      },
      "source": [
        "#### 2.1.2 - From 1990 to 2009:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7cOeXDqXQo5",
        "outputId": "5aa0ff09-71d5-4553-9b53-893dc5f0209a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "titles_from_1990_to_2009 = load_titles(path_from_1990_to_2009)\n",
        "print(\"{} titles from 1990 to 2009\".format(len(titles_from_1990_to_2009)))"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "327307 titles from 1990 to 2009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-67SE1cXXd9",
        "outputId": "df43fef3-82e2-439d-d0ab-6962a33e86ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "random.sample(titles_from_1990_to_2009, 10)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Model Formulation: Generic Data Modeling for Clinical Rrepositories.',\n",
              " 'Linear transformations of Wiener process that born Wiener process, Brownian bridge or Ornstein-Uhlenbeck process.',\n",
              " 'Identifying local spatial association in flow data.',\n",
              " 'Reducing the number of sub-classifiers for pairwise multi-category support vector machines.',\n",
              " 'Designs in a coset geometry: Delsarte theory revisited.',\n",
              " 'Phonemic segmentation using the generalised Gamma distribution and small sample Bayesian information criterion.',\n",
              " 'On call admission control for IP telephony in best effort networks.',\n",
              " 'Highlights of the Fourth Ka Band Utilization Conference.',\n",
              " \"Analyse spatiale et cartes anim&eacute;es : construction d'un prototype d'animation des dynamiques d&eacute;mographiques.\",\n",
              " 'The Control Method for the Robot Hand Based on the Fuzzy Theory.']"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlb3X9hyaYTF"
      },
      "source": [
        "preprocessed_titles_from_1990_to_2009 = [preprocess_text(title) for title in titles]"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4627RUxe1zf",
        "outputId": "a8d42849-ed8b-462b-93f2-95519f856fff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "random.sample(preprocessed_titles_from_1990_to_2009, 10)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a performance analysis of mcbased systems',\n",
              " 'a comparison of order structures for automatic digital computers',\n",
              " 'algorithm  qshepd quadratic shepard method for bivariate interpolation of scattered data',\n",
              " 'the brisbane media centre',\n",
              " 'elimination of cardinality quantifiers',\n",
              " 'convergent deduction for probabilistic logic',\n",
              " 'on insensitivities in urban redistricting and facility location',\n",
              " 'a note on primary and secondary syncategoremata',\n",
              " 'nonlinear programming counterexamples to two global optimization algorithms',\n",
              " 'highspeed indirect cryption']"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oULIkhWnXLGX"
      },
      "source": [
        "tf_02, tf_feature_names_02 = vectorize_data(titles_from_1990_to_2009, max_df=0.95, min_df=0.01)"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh-AKJdlXntq"
      },
      "source": [
        "lda_02 = LatentDirichletAllocation(n_components=8, max_iter=10, learning_method='online', random_state=42).fit(tf_02)"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaDUR7xpXqlV",
        "outputId": "e52cbfe3-e9c5-4cc9-ad4d-bfedaf6fd0f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for topic_idx, topic in enumerate(lda_02.components_):\n",
        "    print(f'Topic {topic_idx}:', end=' ')\n",
        "    print(' '.join([tf_feature_names_02[i] for i in topic.argsort()[:-12 - 1:-1]]))"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: time algorithm linear new network models algorithms efficient high robust equations management\n",
            "Topic 1: method study problems evaluation case space programming equations finite linear new performance\n",
            "Topic 2: design approach nonlinear optimal fuzzy modeling computing robust control equations new time\n",
            "Topic 3: based control model methods computer robust time linear simulation network detection dynamic\n",
            "Topic 4: using analysis networks performance problem multi dynamic neural wireless mobile recognition network\n",
            "Topic 5: systems data information multiple digital linear robust management time control nonlinear analysis\n",
            "Topic 6: adaptive application structure non theory knowledge scheme management robust linear control finite\n",
            "Topic 7: learning estimation applications order image distributed graphs web software power real development\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip_z250Z_Mnz"
      },
      "source": [
        "#### 2.1.3 - From 2010 onwards:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xxhrl06aHKA",
        "outputId": "acd8b2ae-75c4-4f88-8190-7c4c617c1b5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load the titles\n",
        "titles_from_2010 = load_titles(path_from_2010)\n",
        "print(\"{} titles from from 2010\".format(len(titles_from_2010)))"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "715820 titles from from 2010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgi6gbvcX-n6",
        "outputId": "88be1abc-c2f6-4073-c9f5-5252c5576ada",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Show some random samples\n",
        "random.sample(titles_from_2010, 10)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A Modified Earley Parser for Huge Natural Language Grammars.',\n",
              " 'Bidimensional allocation of seats via zero-one matrices with given line sums.',\n",
              " 'Spectral Leakage-Driven Loopback Scheme for Prediction of Mixed-Signal Circuit Specifications.',\n",
              " 'Ontology-Based Mobile Communication in Agriculture.',\n",
              " 'Using machine learning to support healthcare professionals in making preauthorisation decisions.',\n",
              " 'Performance Modeling and Analysis of Heterogeneous Machine Type Communications.',\n",
              " 'An NTF-enhanced incremental &#931;&#916; modulator using a SAR quantizer.',\n",
              " 'Erratum to \"A bubble-stabilized least-squares finite element method for steady MHD duct flow problems at high Hartmann numbers\" [J. Comput. Physics 228 (2009) 8301-8320].',\n",
              " 'Energy-Delay Tradeoff in Ultra-Dense Networks Considering BS Sleeping and Cell Association.',\n",
              " 'Intuitionistic Type-2 Fuzzy Set and Its Properties.']"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erEowFQcabl2"
      },
      "source": [
        "# Preprocess the titles by removing certain characters\n",
        "preprocessed_titles_from_2010 = [preprocess_text(title) for title in titles]"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3BQQX2MakvD"
      },
      "source": [
        "# Vectorize\n",
        "tf_03, tf_feature_names_03 = vectorize_data(preprocessed_titles_from_2010, max_df=0.95, min_df=0.01)"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKRd2B0ManH9"
      },
      "source": [
        "lda_03 = LatentDirichletAllocation(n_components=8, max_iter=10, learning_method='online', random_state=42).fit(tf_03)"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKwtKuvKaofc",
        "outputId": "80941aa8-14a6-4b62-d473-8183f3b3e3f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for topic_idx, topic in enumerate(lda_03.components_):\n",
        "    print(f'Topic {topic_idx}:', end=' ')\n",
        "    print(' '.join([tf_feature_names_03[i] for i in topic.argsort()[:-12 - 1:-1]]))"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: theory problems algorithms simulation decision parallel application solution applications optimal digital control\n",
            "Topic 1: computer logic model programs digital performance design networks using applications systems simulation\n",
            "Topic 2: problem programming optimal language digital processing software research solution parallel linear control\n",
            "Topic 3: data method network models application languages solution processing problem using analysis programming\n",
            "Topic 4: note information linear functions applications finite technical programming systems time problem decision\n",
            "Topic 5: algorithm design analysis approach sets performance new using implementation parallel linear digital\n",
            "Topic 6: systems using parallel performance implementation decision distributed linear control digital design processing\n",
            "Topic 7: control networks new recognition distributed time pattern optimal systems approach digital linear\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pStFBikpsRTz"
      },
      "source": [
        "# Combined Topic Models\n",
        "\n",
        "New method developed by [Bianchi et al. 2021](https://aclanthology.org/2021.acl-short.96/). \n",
        "\n",
        "[A 6min presentation of the paper by one of the authors.](https://underline.io/lecture/25716-pre-training-is-a-hot-topic-contextualized-document-embeddings-improve-topic-coherence)\n",
        "\n",
        "Code: [https://github.com/MilaNLProc/contextualized-topic-models](https://github.com/MilaNLProc/contextualized-topic-models)\n",
        "\n",
        "Tutorial: [https://colab.research.google.com/drive/1fXJjr_rwqvpp1IdNQ4dxqN4Dp88cxO97?usp=sharing](https://colab.research.google.com/drive/1fXJjr_rwqvpp1IdNQ4dxqN4Dp88cxO97?usp=sharing)\n",
        "\n",
        "Again, perform topic modelling for the three time periods - this time using the combined topic models (CTMs). \n",
        "\n",
        "You can use and adapt the code from the tutorial linked above.\n",
        "\n",
        "Use the available GPU for faster running times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNsJrV2DJ8GO"
      },
      "source": [
        "from contextualized_topic_models.models.ctm import CombinedTM\n",
        "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
        "from contextualized_topic_models.utils.preprocessing import WhiteSpacePreprocessing\n",
        "\n",
        "num_ctm_topics = 5  # you can also choose a higher number of topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSv9Ch46LFxp"
      },
      "source": [
        "### Before the 1990s:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNRaJJEMiVQb"
      },
      "source": [
        "### From 1990 to 2009"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfgbLy0KiZ5b"
      },
      "source": [
        "### From 2010 onwards"
      ]
    }
  ]
}