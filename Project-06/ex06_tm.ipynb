{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "ex06_tm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jan-kreischer/UZH_ML4NLP/blob/main/Project-06/ex06_tm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86zA-HlTCBID"
      },
      "source": [
        "##Project 6 - Topic Modeling using Latent Dirichlet Allocation (LDA)\n",
        "## 1. Setup\n",
        "### 1.1 Dependencies\n",
        "Installing all dependencies needed to run the simulations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hENmFU8FmZbl"
      },
      "source": [
        "!pip install contextualized-topic-models==2.2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vLndra3Nbqa"
      },
      "source": [
        "### 1.2 Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci16zWWnlC1n"
      },
      "source": [
        "import re\n",
        "import random\n",
        "import os\n",
        "import urllib\n",
        "import urllib.request\n",
        "import gzip\n",
        "import io\n",
        "import csv\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUHMJB4TR7rf"
      },
      "source": [
        "### 1.3 Google Drive\n",
        "We connect Google Drive in order to access stored data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCpOpcUCsZQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "596419e1-6152-493e-efe6-22d8d81ff1a1"
      },
      "source": [
        "# Enable access to files stored in Google Drive\n",
        "from google.colab import drive\n",
        "# Leave this like it is\n",
        "mountpoint = '/content/drive/' \n",
        "drive.mount(mountpoint)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP0EqBjAwemg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf1c6029-f01d-4c05-9f07-bdf6d4dd4f41"
      },
      "source": [
        "# Adapt this path to the folder where your data is stored in google drive\n",
        "base_path = 'My Drive/UZH_ML4NLP/Projects/Project-06/data' \n",
        "data_path = os.path.join(mountpoint, base_path)\n",
        "# Cd into the directory with the git repo\n",
        "% cd $data_path"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/UZH_ML4NLP/Projects/Project-06/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AiKDCyWSVE3"
      },
      "source": [
        "### 1.4 Constants\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m841xWt8SaiK"
      },
      "source": [
        "NUM_LDA_TOPICS = 5 # The number of different topics to identify\n",
        "NUM_FEATURES = 10000\n",
        "MAX_DF=0.5\n",
        "MIN_DF=0.01"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_mkS_zeWbYW"
      },
      "source": [
        "# Path to the data files\n",
        "path_before_1990 = 'titles_before_1990.txt'\n",
        "path_from_1990_to_2009 = 'titles_from_1990_to_2009.txt'\n",
        "path_from_2010 = 'titles_from_2010.txt'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUVy4jyGlVH3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "b513d85a-6d7e-49d9-a1cd-0a8159982715"
      },
      "source": [
        "# Execute the following cell only once to download the data and write it as a file to your google drive. Afterwards, skip this cell or comment it out.\n",
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# to download the data manually or get more information, go to: https://dblp.org/faq/How+can+I+download+the+whole+dblp+dataset.html\n",
        "url = 'https://dblp.uni-trier.de/xml/dblp.xml.gz'\n",
        "num_titles = 500000  # the (max)number of titles to load \n",
        "\n",
        "\n",
        "def load_gzip_file(url):\n",
        "    \"\"\"Download Gzip-file.\"\"\"\n",
        "    response = urllib.request.urlopen(url)\n",
        "    compressed_file = io.BytesIO(response.read())\n",
        "    decompressed_file = gzip.GzipFile(fileobj=compressed_file)\n",
        "    return decompressed_file\n",
        "\n",
        "def extract_titles(input_file, max_num=40000):\n",
        "    \"\"\"Extract title and publication year of dblp papers, given as input file.\n",
        "    \n",
        "    Divide the papers into 3 time periods. \n",
        "    \n",
        "    Collect max max_num papers per time period.\n",
        "    \"\"\"\n",
        "    pairs_before_1990 = []\n",
        "    count_before_1990 = 0\n",
        "    pairs_from_1990_to_2009 = []\n",
        "    count_from_1990_to_2009 = 0\n",
        "    pairs_from_2010 = []\n",
        "    count_from_2010 = 0\n",
        "    got_title = False\n",
        "    for line in tqdm(input_file):\n",
        "        line_str = line.decode('utf-8')\n",
        "        if got_title: \n",
        "            # we have a title and check for the corresponding year\n",
        "            year_result = re.search(r'<year>(.*)</year>', line_str)\n",
        "            if year_result:\n",
        "                # we also have the year and thus save the title-year pair\n",
        "                year = int(year_result.group(1))\n",
        "                if year < 1990:\n",
        "                    pairs_before_1990.append((title, year))\n",
        "                    count_before_1990 += 1\n",
        "                elif year < 2010:\n",
        "                    pairs_from_1990_to_2009.append((title, year))\n",
        "                    count_from_1990_to_2009 += 1\n",
        "                else:\n",
        "                    pairs_from_2010.append((title, year))\n",
        "                    count_from_2010 += 1\n",
        "                got_title = False\n",
        "        else:\n",
        "            # we have no title and search for title\n",
        "            result = re.search(r'<title>(.*)</title>', line_str)\n",
        "            if result:\n",
        "                title = result.group(1)\n",
        "                if len(title.split(' ')) < 3:  \n",
        "                    # only include titles with at least four words\n",
        "                    continue\n",
        "                got_title = True\n",
        "        \n",
        "        if count_before_1990 >= max_num and count_from_1990_to_2009 >= max_num and count_from_2010 >= max_num:\n",
        "            return pairs_before_1990, pairs_from_1990_to_2009, pairs_from_2010\n",
        "    \n",
        "    return pairs_before_1990, pairs_from_1990_to_2009, pairs_from_2010\n",
        "\n",
        "def save_data(pairs, file_path):\n",
        "    with open(file_path, 'w') as fout:\n",
        "        writer = csv.writer(fout)\n",
        "        for pair in pairs:\n",
        "            writer.writerow(pair)\n",
        "\n",
        "in_file = load_gzip_file(url)\n",
        "pairs_before_1990, pairs_from_1990_to_2009, pairs_from_2010 = extract_titles(in_file)\n",
        "save_data(pairs_before_1990, path_before_1990)\n",
        "save_data(pairs_from_1990_to_2009, path_from_1990_to_2009)\n",
        "save_data(pairs_from_2010, path_from_2010)\n",
        "'''"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom google.colab import drive\\ndrive.mount(\\'/content/drive\\', force_remount=True)\\n\\n# to download the data manually or get more information, go to: https://dblp.org/faq/How+can+I+download+the+whole+dblp+dataset.html\\nurl = \\'https://dblp.uni-trier.de/xml/dblp.xml.gz\\'\\nnum_titles = 500000  # the (max)number of titles to load \\n\\n\\ndef load_gzip_file(url):\\n    \"\"\"Download Gzip-file.\"\"\"\\n    response = urllib.request.urlopen(url)\\n    compressed_file = io.BytesIO(response.read())\\n    decompressed_file = gzip.GzipFile(fileobj=compressed_file)\\n    return decompressed_file\\n\\ndef extract_titles(input_file, max_num=40000):\\n    \"\"\"Extract title and publication year of dblp papers, given as input file.\\n    \\n    Divide the papers into 3 time periods. \\n    \\n    Collect max max_num papers per time period.\\n    \"\"\"\\n    pairs_before_1990 = []\\n    count_before_1990 = 0\\n    pairs_from_1990_to_2009 = []\\n    count_from_1990_to_2009 = 0\\n    pairs_from_2010 = []\\n    count_from_2010 = 0\\n    got_title = False\\n    for line in tqdm(input_file):\\n        line_str = line.decode(\\'utf-8\\')\\n        if got_title: \\n            # we have a title and check for the corresponding year\\n            year_result = re.search(r\\'<year>(.*)</year>\\', line_str)\\n            if year_result:\\n                # we also have the year and thus save the title-year pair\\n                year = int(year_result.group(1))\\n                if year < 1990:\\n                    pairs_before_1990.append((title, year))\\n                    count_before_1990 += 1\\n                elif year < 2010:\\n                    pairs_from_1990_to_2009.append((title, year))\\n                    count_from_1990_to_2009 += 1\\n                else:\\n                    pairs_from_2010.append((title, year))\\n                    count_from_2010 += 1\\n                got_title = False\\n        else:\\n            # we have no title and search for title\\n            result = re.search(r\\'<title>(.*)</title>\\', line_str)\\n            if result:\\n                title = result.group(1)\\n                if len(title.split(\\' \\')) < 3:  \\n                    # only include titles with at least four words\\n                    continue\\n                got_title = True\\n        \\n        if count_before_1990 >= max_num and count_from_1990_to_2009 >= max_num and count_from_2010 >= max_num:\\n            return pairs_before_1990, pairs_from_1990_to_2009, pairs_from_2010\\n    \\n    return pairs_before_1990, pairs_from_1990_to_2009, pairs_from_2010\\n\\ndef save_data(pairs, file_path):\\n    with open(file_path, \\'w\\') as fout:\\n        writer = csv.writer(fout)\\n        for pair in pairs:\\n            writer.writerow(pair)\\n\\nin_file = load_gzip_file(url)\\npairs_before_1990, pairs_from_1990_to_2009, pairs_from_2010 = extract_titles(in_file)\\nsave_data(pairs_before_1990, path_before_1990)\\nsave_data(pairs_from_1990_to_2009, path_from_1990_to_2009)\\nsave_data(pairs_from_2010, path_from_2010)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDhEoD2Tsh3z"
      },
      "source": [
        "# LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ86sQwo124U"
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "num_lda_topics = 8"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA-jeLNbJsjt"
      },
      "source": [
        "def load_titles(path):\n",
        "  with open(path) as fin:\n",
        "    reader = csv.reader(fin)\n",
        "    titles = [row[0] for row in reader]\n",
        "  return titles"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc09sFDe-5kn"
      },
      "source": [
        "### Before the 1990s:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rwq6kF7vqUj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f248f6a-e8fd-4025-ee09-2e66a1bc6938"
      },
      "source": [
        "titles_before_1990 = load_titles(path_before_1990)\n",
        "print(\"{} titles before 1990\".format(len(titles_before_1990)))\n",
        "titles_from_1990_to_2009 = load_titles(path_from_1990_to_2009)\n",
        "print(\"{} titles from 1990 to 2009\".format(len(titles_from_1990_to_2009)))\n",
        "titles_from_2010 = load_titles(path_from_2010)\n",
        "print(\"{} titles from from 2010\".format(len(titles_from_2010)))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40000 titles before 1990\n",
            "327307 titles from 1990 to 2009\n",
            "715820 titles from from 2010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnXGcMlIKxBS",
        "outputId": "7deb73be-cf0c-4dbf-c283-ddf585c4ebf9"
      },
      "source": [
        "random.sample(titles_before_1990, 10)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Equality and Lyndon's Interpolation Theorem.\",\n",
              " 'FSPICE: a tool for fault modelling in MOS circuits.',\n",
              " 'Minimal Degrees and the Jump Operator.',\n",
              " 'Key note address.',\n",
              " \"MMIC's in communications.\",\n",
              " 'A Probabilistic Algorithm for Vertex Connectivity of Graphs.',\n",
              " 'Automorphism Groups of Primitive Distance-Bitransitive Graphs are Almost Simple.',\n",
              " 'On Definitions of a Burst.',\n",
              " 'Residual Life Approximations in General Queueing Networks.',\n",
              " 'Fractal Nature of Software-Cache Interaction.']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgU6qSmdLKfH",
        "outputId": "fa7237f8-920b-4f63-d0b2-618a538329a2"
      },
      "source": [
        "random.sample(titles_from_1990_to_2009, 10)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Global Optimization: Interval Analysis and Balanced Interval Arithmetic.',\n",
              " 'Simulation of two-plane automatic balancing of a rigid rotor.',\n",
              " 'On Enforced Convergence of ACO and its Implementation on the Reconfigurable Mesh Architecture Using Size Reduction Tasks.',\n",
              " 'Approximating the Stable Model Semantics is Hard.',\n",
              " 'Multiresolution representations for surfaces meshes based on the vertex decimation method.',\n",
              " 'Motion analysis and estimation: From ill-posed discrete inverse linear problems to MPEG-2 coding.',\n",
              " '2-D Parachute Simulation by the Immersed Boundary Method.',\n",
              " 'Wireless network directions.',\n",
              " 'Reliability and Risk Models: Setting Reliability Requirements.',\n",
              " 'A Paradigm for Listing (s, t)-Cuts in Graphs.']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZeeV9pALLx5",
        "outputId": "632bd8f5-0ab9-4ed7-fe94-214e2e043d59"
      },
      "source": [
        "random.sample(titles_from_2010, 10)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Vacuum Constraints for Realistic Strongly Coupled Heterotic M-Theories.',\n",
              " 'Error Correction Encoding for Tightly Coupled On-Chip Buses.',\n",
              " 'Cyclic codes over R k.',\n",
              " '3-D Convolution-Recurrent Networks for Spectral-Spatial Classification of Hyperspectral Images.',\n",
              " 'An arts-informed study of information using the draw-and-write technique.',\n",
              " 'Assessments of the Retrieval of Atmospheric Profiles from GNSS Radio Occultation Data in Moist Tropospheric Conditions Using Radiosonde Data.',\n",
              " 'Data-stories about (im)patient customers in tele-queues.',\n",
              " 'Multi-Strategy Sentiment Analysis of Consumer Reviews Based on Semantic Fuzziness.',\n",
              " 'A Strategy to Mitigate the Ionospheric Scintillation Effects on BDS Precise Point Positioning: Cycle-Slip Threshold Model.',\n",
              " 'Ontology-Based Multiple Choice Question Generation.']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmKJDHwI2Dnp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vmu77l62DCp"
      },
      "source": [
        "# Simple text preprocessing by removing \n",
        "# all letters which are not in roman alphabet\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z ]', '', text)\n",
        "    text = text.lower()\n",
        "    return text"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mulG5Sg2NBZ"
      },
      "source": [
        "preprocessed_titles_before_1990 = [preprocess_text(title) for title in titles]\n",
        "preprocessed_titles_from_1990_to_2009 = [preprocess_text(title) for title in titles]\n",
        "preprocessed_titles_from_2010 = [preprocess_text(title) for title in titles]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1hcBI0gToiQ"
      },
      "source": [
        "# Now we turn the documents (or titles in this case) into a matrix feature representation.\n",
        "def vectorize_data(titles, max_df=MAX_DF, min_df=MIN_DF, max_features=NUM_FEATURES):\n",
        "  tf_vectorizer = CountVectorizer(max_df=max_df, min_df=min_df, max_features=max_features, stop_words='english')\n",
        "  tf = tf_vectorizer.fit_transform(titles)\n",
        "  tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
        "  return tf, tf_feature_names"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhgxgCM_01mx"
      },
      "source": [
        "tf, tf_feature_names = vectorize_data(preprocessed_titles_before_1990, max_df=0.95, min_df=0.01)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_927CQlk4C27"
      },
      "source": [
        "lda = LatentDirichletAllocation(n_components=8, max_iter=5, learning_method='online', random_state=42).fit(tf)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-8NinuI4kMG",
        "outputId": "97d0efe6-6ea2-4a56-ad1b-d203d9eadb97"
      },
      "source": [
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    print(f'Topic {topic_idx}:', end=' ')\n",
        "    print(' '.join([tf_feature_names[i] for i in topic.argsort()[:-12 - 1:-1]]))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: theory problems algorithms simulation decision parallel application solution applications optimal digital control\n",
            "Topic 1: computer logic model programs digital performance design networks using simulation applications systems\n",
            "Topic 2: problem programming optimal digital language processing software research parallel solution linear control\n",
            "Topic 3: data method network models application languages solution processing problem using analysis parallel\n",
            "Topic 4: note linear information functions applications finite technical systems programming time sets problem\n",
            "Topic 5: design analysis algorithm approach sets performance new using digital parallel implementation optimal\n",
            "Topic 6: systems using parallel performance implementation decision distributed linear digital control design processing\n",
            "Topic 7: control networks new recognition distributed time pattern optimal systems digital approach linear\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "moOtkgU2JAfn",
        "outputId": "febc5bb6-0fb2-471e-95e1-71eade8e0843"
      },
      "source": [
        "#for i in range(num_lda_topics):\n",
        "#    print(f'Topic {i + 1}:\\t{lda.show_topic(i)}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-b0b4c96e30bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_lda_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Topic {i + 1}:\\t{lda.show_topic(i)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'LatentDirichletAllocation' object has no attribute 'show_topic'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbx0DythfCkP"
      },
      "source": [
        "Topics:\n",
        "0. Graph/networks algorithms (seems to be mostly about algorithms that (maybe) operate on graphs/networks)\n",
        "1. pattern recognition (and maybe robotics)\n",
        "2. ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqioi0rG_Dz9"
      },
      "source": [
        "### From 1990 to 2009:\n",
        "\n",
        "Add your code for topic modelling the period from 1990 to 2009 here..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGa-ItccNRYh"
      },
      "source": [
        "num_features = 10000\n",
        "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=N_FEATURES, stop_words='english')\n",
        "tf = tf_vectorizer.fit_transform(prepro_titles)\n",
        "tf_feature_names = tf_vectorizer.get_feature_names_out()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip_z250Z_Mnz"
      },
      "source": [
        "### From 2010 onwards:\n",
        "\n",
        "Add your code for topic modelling the period from 2010 onwards here..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pStFBikpsRTz"
      },
      "source": [
        "# Combined Topic Models\n",
        "\n",
        "New method developed by [Bianchi et al. 2021](https://aclanthology.org/2021.acl-short.96/). \n",
        "\n",
        "[A 6min presentation of the paper by one of the authors.](https://underline.io/lecture/25716-pre-training-is-a-hot-topic-contextualized-document-embeddings-improve-topic-coherence)\n",
        "\n",
        "Code: [https://github.com/MilaNLProc/contextualized-topic-models](https://github.com/MilaNLProc/contextualized-topic-models)\n",
        "\n",
        "Tutorial: [https://colab.research.google.com/drive/1fXJjr_rwqvpp1IdNQ4dxqN4Dp88cxO97?usp=sharing](https://colab.research.google.com/drive/1fXJjr_rwqvpp1IdNQ4dxqN4Dp88cxO97?usp=sharing)\n",
        "\n",
        "Again, perform topic modelling for the three time periods - this time using the combined topic models (CTMs). \n",
        "\n",
        "You can use and adapt the code from the tutorial linked above.\n",
        "\n",
        "Use the available GPU for faster running times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNsJrV2DJ8GO"
      },
      "source": [
        "from contextualized_topic_models.models.ctm import CombinedTM\n",
        "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
        "from contextualized_topic_models.utils.preprocessing import WhiteSpacePreprocessing\n",
        "\n",
        "num_ctm_topics = 5  # you can also choose a higher number of topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSv9Ch46LFxp"
      },
      "source": [
        "### Before the 1990s:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNRaJJEMiVQb"
      },
      "source": [
        "### From 1990 to 2009"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfgbLy0KiZ5b"
      },
      "source": [
        "### From 2010 onwards"
      ]
    }
  ]
}